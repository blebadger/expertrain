## Three body problem II: Simulating Divergence with Parallelized Computation

This page is a continuation from [Part I](https://blbadger.github.io/3-body-problem.html), where simulations of the three body problem are explored.  Here we explore the computation of simulated trajectories using a parallelized computer achitecture, ending with am optimized CUDA kernel that is around five times as fast as the Pytorch code introduced in Part 1. In [Part III](https://blbadger.github.io/3-body-problem-3.html) we will explore how to parallelize this kernel to be used with many GPUs simultaneously.

### Introduction

Most nonlinear dynamical systems are fundamentally irreducible: one cannot come up with a computational procedure to determine the parameters of some object at any given time in the future using a fixed amount of computation.  This means that these systems are inherently sequential to some extent. This being the case, there are still many problems that benefit from computations that do not have to proceed in sequence.  One particular example is the problem of finding which positions in a given plane are stable for the trajectory of three bodies in space.  This problem can be approached by determining the stability of various starting locations in sequence, but it is much faster to accomplish this goal by determining the stabilities at various starting locations in parallel.  In [Part 1](https://blbadger.github.io/3-body-problem.html) this parallel computation was performed behind the scenes using the Python `torch` library, which abstracts away the direct computation of tensors on parallelized computational devices like graphics processing units (GPUs) or tensor processing units.

Even with the use of the [optimized](https://pytorch.org/tutorials/advanced/cpp_extension.html) torch library, however, computation of stable and unstable locations takes a substantial amount of time.  Most images displayed in [part 1](https://blbadger.github.io/3-body-problem.html) require around 18 minutes to compute on an entry level gaming GPU: this is mostly due to the large number of iteration required (50,000 or more), the large size of each array (18 arrays with more than a million components each) and even the data type used (double precision 64-bit floating point, which gaming GPUs are flow with in general).

### A CUDA kernel for divergence

Here we will explore speeding up the three body computation by writing our own GPU code, rather than relying on torch to supply this when given higher-level instructions.  The author has an Nvidia GPU and code on this page will therefore be written in C/C++ CUDA (Compute Unified Device Architecure).  The code contains a standard C++ -style library inclusion and function initialization (C or C++ execution always begins with `main()`), all of which is performed on the CPU.  Here we first initialize some constants for the three body similation.

```c++
#include <stdio.h>
#include <iostream>
#include <chrono>

int main(void)
{
  int N = 90000;
  int steps = 50000;
  double delta_t = 0.001;
  double critical_distance = 0.5;
  double m1 = 10;
  double m2 = 20;
  double m3 = 30;
```

And then we continue by assigning pointer variables with the proper data type for each of our planets.  This is an efficient form of an array in C++, allowing us to allocate memory and initialize each element directly.  

In the above code snippet `N` is the number of pixels; for example a 300x300 divergence plot contains N=90,000 pixels. We will perform all the required computations in 1D arrays for now, such that separate arrays are initialized for each x, y, z component of each attribute of each planet. Note however that this is purely for the purpose of clarity: we could just as easily maintain one single triply large array for all three x, y, z components and instead offset each component by a factor of 1/3 times the size of the array.

We also have to initialize position, acceleration (dv), velocity, and a temporary buffer array for new velocities as shown below.

```c++
int main(void)
{
  ...
  double *p1_x, *p1_y, *p1_z;
  ...
  double *dv_1_x, *dv_1_y, *dv_1_z;
  ...
  double *nv1_x, *nv1_y, *nv1_z;
  ...
  double *v1_x, *v1_y, *v1_z;
```

We also want to initialize boolean arrays for trajectories that have not diverged yet for any given iteration, a boolean array for trajectories that are not diverging right now, and an array to keep track of the iteration in which divergence occurred.

```c++
bool *still_together,*not_diverged;
int *times
```

For each array, we must allocate the proper amount of memory depending on the type of data stored. We have $N$ total elements, so we need to allocate the size of each element times $N$ for each array.

```c++
  ...
  p1_x = (double*)malloc(N*sizeof(double));
  ...
  still_together = (bool*)malloc(N*sizeof(bool));
  times = (int*)malloc(N*sizeof(int));
  not_diverged = (bool*)malloc(N*sizeof(bool));  
```

Next, space is allocated for each array in the GPU. One way to do this is to declare a new pointer variable corresponding to each array (see below for p1_x) before allocating the proper amount of memory for that variable (which must be referenced as it was defined as a pointer.  Each must be different than the name for each corresponding CPU array, and here `d_` is prefixed to designate that this is a 'device' array.

```c++
double *d_p1_x;
cudaMalloc(&d_p1_x, N*sizeof(double)); 
```

Now we need to initialize each array with our starting condition.  As we are working with 1D arrays rather than the 2D arrays, we need to initialize each array to capture 2D information in 1D array.  Our approach will be similar to how 2D CUDA arrays are represented in GPU memory, and is sometimes termed a 'row-major' layout (as each row is contiguous).  This is the most commonly used format today, with the column-major layout (contiguous columns) being used in the past by Fortran.

![1d illustration]({{https://blbadger.github.io}}/3_body_problem/1d_cuda.png)

Row-major layout may be implemented using modulo division in which the x parameter is equivalent to the remainder of the division of the total number of elements by the square root of elements, and the y parameter is equal to the integer (floor) division of the number of elements by the square root of elements. Here we also scale each element by the appropriate constants (here to make a linear interpolation between -20 and 20),

```c++
  for (int i = 0; i < N; i++) {
    int remainder = i % resolution;
    int step = i / resolution;
    p1_x[i] = -20. + 40*(double(remainder)/double(resolution));
    p1_y[i] = -20. + 40*(double(step)/double(resolution));
    ...
```

Now we copy each array from the CPU to the GPU

```c++
  cudaMemcpy(d_p1_x, p1_x, N*sizeof(double), cudaMemcpyHostToDevice);
```

and now we can run the CUDA kernel, keeping track of the time spent by initializing a start time clock.

```c++
std::chrono::time_point<std::chrono::system_clock> start, end;
start = std::chrono::system_clock::now();
divergence<<<(N+255)/256, 256>>>(
      N, 
      steps, 
      delta_t,
      d_still_together,
      d_not_diverged,
      d_times,
      m1, m2, m3,
      critical_distance,
      d_p1_x,
      ...
      );

```

CUDA functions are termed 'kernels', and are called by the kernel name followed by the number of grid blocks and threads per block for execution. We call our CUDA function by `divergence<<<blocks, threads_per_block>>>(args)`. The denominator of the `blocks` must equal the `threads_per_block` for this experiment for reasons detailed below.

We have to synchronize the GPU before measuring the time of completion, as otherwise the code will continue executing in the CPU after the kernel instructions have been sent to the GPU.

```c++
  // don't proceed until kernel run is complete
  cudaDeviceSynchronize();
  // measure elapsed kernel runtime
  end = std::chrono::system_clock::now();
  std::chrono::duration<double> elapsed_seconds = end - start;
  std::time_t end_time = std::chrono::system_clock::to_time_t(end);
  std::cout << "Elapsed Time: " << elapsed_seconds.count() << "s\n";
```

A typical CUDA kernel declaration is `__global__ void funcname(args)` although `__device__` may also be used.  For brevity, only the first planet's arrays are included below but note that the full kernel requires all three planet arrays.

```c++
// kernel declaration
__global__
void divergence(int n, 
              int steps,
              double delta_t,
              bool *still_together,
              bool *not_diverged,
              int *times,
              double m_1, double m_2, double m_3,
              double critical_distance,
              double *p1_x, double *p1_y, double *p1_z, 
              double *p1_prime_x, double *p1_prime_y, double *p1_prime_z, 
              double *dv_1_x, double *dv_1_y, double *dv_1_z,
              double *dv_1pr_x, double *dv_1pr_y, double *dv_1pr_z,
              double *v1_x, double *v1_y, double *v1_z,
              double *v1_prime_x, double *v1_prime_y, double *v1_prime_z,
              double *nv1_x, double *nv1_y, double *nv1_z,
              double *nv1_prime_x, double *nv1_prime_y, double *nv1_prime_z,
)
```
Parallelized computation must now be specified: in the following code, we define index `i` to be a certain block's thread, in one dimension as this is how the arrays were defined as well. Note that as the array is 1D, `blockDim.x` will always evaluate to 1.  The arrangement of blocks and threads in our kernel call is now clearer, as each thread is responsible for one index.

```c++
{
  int i = blockIdx.x*blockDim.x + threadIdx.x;
```

Now the trajectory simulation computations are performed. In the spirit of refraining from as much data transfer from the CPU to the GPU and back, we will perform the simulation calculations entirely inside the GPU by moving the trajectory loop to the CUDA kernel.  This is notably different than the pytorch approach, where the loop existed on the CPU side (in python) and the GPU was instructed to perform one array computation at a time. It can be shown that moving the loop to the GPU saves a small amount of time, although less than what the author would expect (typically ~5% of runtime with no other optimizations performed).

Moving the loop to the GPU does have another substantial benefit, however: it reduced the electrical power required by the GPU and CPU during the simulation.  For the GPU alone, the internal loop typically requires around 100 watts whereas the external loop usually takes around 125 watts on an Nvidia RTX 3060.

For each index `i` corresponding to one CUDA thread, $steps$ iterations of the three body trajectory are performed and at each iteration the $times$ array is incremented if the trajectory of planet one has not diverged from its slightly shifted counter part (planet one prime). 

```c++
  for (int j=0; j < steps; j++) {
    if (i < n){
      // compute accelerations
      dv_1_x[i] = -9.8 * m_2 * (p1_x[i] - p2_x[i]) / pow(sqrt(pow(p1_x[i] - p2_x[i], 2) + pow(p1_y[i] - p2_y[i], 2) + pow(p1_z[i] - p2_z[i], 2)), 3) \
                  -9.8 * m_3 * (p1_x[i] - p3_x[i]) / pow(sqrt(pow(p1_x[i] - p3_x[i], 2) + pow(p1_y[i] - p3_y[i], 2) + pow(p1_z[i] - p3_z[i], 2)), 3);
      dv_1pr_x[i] = -9.8 * m_2 * (p1_prime_x[i] - p2_prime_x[i]) / pow(sqrt(pow(p1_prime_x[i] - p2_prime_x[i], 2) + pow(p1_prime_y[i] - p2_prime_y[i], 2) + pow(p1_prime_z[i] - p2_prime_z[i], 2)), 3) \
                    -9.8 * m_3 * (p1_prime_x[i] - p3_prime_x[i]) / pow(sqrt(pow(p1_prime_x[i] - p3_prime_x[i], 2) + pow(p1_prime_y[i] - p3_prime_y[i], 2) + pow(p1_prime_z[i] - p3_prime_z[i], 2)), 3);

      // find which trajectories have diverged and increment *times
      not_diverged[i] = sqrt(pow(p1_x[i] - p1_prime_x[i], 2) + pow(p1_y[i] - p1_prime_y[i], 2) + pow(p1_z[i] - p1_prime_z[i], 2)) <= critical_distance;
      still_together[i] &= not_diverged[i];
      if (still_together[i] == true){
        times[i]++;
      };

      // compute new velocities
      nv1_x[i] = v1_x[i] + delta_t * dv_1_x[i];
      nv1_prime_x[i] = v1_prime_x[i] + delta_t * dv_1pr_x[i];

      // compute positions with current velocities
      p1_x[i] = p1_x[i] + delta_t * v1_x[i];
      p1_prime_x[i] = p1_prime_x[i] + delta_t * v1_prime_x[i];

      // assign new velocities to current velocities
      v1_x[i] = nv1_x[i];

      v1_prime_x[i] = nv1_prime_x[i];
      }
    }

```

The same needs to be done for all `x, y, z` vectors of `p1, p2, p3` in order to track all the necessary trajectories.  In total we have 63 vectors to keep track of, which makes the cuda code somewhat unpleasant to write even with the help of developer tools.

The cuda kernel with driver code can be compiled via `nvcc`, which is available through the nvidia cuda toolkit.  Ubuntu Desktop users be warned that the drivers necessary for full Nvidia toolkit use with an Ampere architecture GPU (such as the author's RTX 3060) may not be compatible with the latest linux kernel version available, so downgrading to an older kernel version may be necessary. The author has found that kernel version `5.19.0-45-generic` is compatible with recent versions of `nvcc`, and this can be selected in the 'advanced options' of the linux boot menu for Ubuntu 22.04. This is not necessary for Ubuntu Server 22.04, which by default runs a `5.15.0-112-generic` Linux kernel and seems to have no issues with nvidia compiler and toolkit compatibility.

Those wishing to compile CUDA code via `nvcc` should note that there are two CUDA versions in each distribution: the runtime API version and the driver version as used by the compiler (and another, the driver version such as `535.171.04` that will not be discussed here).  The driver version should meet or exceed the runtime API software version, which can be checked by ensuring that the CUDA version displayed in the upper right hand corner of the readout called by entering `nvidia-smi` in bash meets or exceeds that shown when calling `nvcc --version`. This is usually the case if you install using automatic package managers, but if for some reason the runtime API version is exceeded by the driver version then the compiler will be riddled with problems and will be unusable and thus should be re-installed (looking at you, Databricks ML runtime 13.x). For Ubuntu server 22.04 with V100s, for example, you typically default to a runtime API `nvcc` version 11.5 with a 12.2 CUDA driver.

Here we compile with the flag `-o` followed by the desired file name where the compiled binary program will be stored.

```bash
(base) bbadger@pupu:~/Desktop/threebody$ nvcc -o divergence divergence.cu
```

For a 300x300 x,y resolution after the full 50,000 timesteps, we have a somewhat disappointing runtime of 

```bash
(base) bbadger@pupu:~/Desktop/threebody$ ./divergence
Elapsed Time: 144.3s
```

Compare this to the Pytorch library version of the same problem (see [part 1](https://blbadger.github.io/3-body-problem.html)), which 

```python
[Finished in 107.1s]
```

Pytorch employs some optimizations in its CUDA code, so this difference is not particularly surprising and only indicates that the movement of the loop into the cuda kernel does not offer the same performance benefit as other optimizations that are possible.  In future sections we explore methods to optimize the cuda kernel further to achieve faster runtimes for the three body problem than are available using torch.

### Getting data from CUDA to Numpy

We can compare the planetary array elements from our CUDA kernel to those found for the torch version to be sure that our kernel's computational procedure is correct. But what if we want to plot the divergence map generated by our CUDA kernel?  For the sake of comparison, let's say that we want to plot our array using the same color map as was presented in [part 1](https://blbadger.github.io/3-body-problem.html).  There are no C++ CUDA libraries (to the author's knowledge) that implement the Matlab-style mathematical plots that `matplotlib` does, but that is a python library and cannot be directly applied to C-style arrays.

Thus we want a way to send our C++ CUDA-computed arrays (specifically the `*int times` array) from CUDA C++ to Python for manipulation and visualization there.  Perhaps the most straighforward way of accomplishing this task is to use the versatile `ctypes` library, which 

The use of `ctypes` can get quite complex, but in this case we can apply this library with only a few lines of code. First we need the proper headers in our `.cu` file containing the CUDA kernel and driver C++ code.

```c++
#! C++ CUDA
#include <cuda.h>
#include <cuda_runtime_api.h>
```

It should be noted that some standard C++ CUDA libraries (ie `<equation.h>`) are incompatible with the CUDA runtime API, so some care must be taken in choosing the header files for a project that involves `ctypes`.  Next we leave the kernel the same as before, but change the main function declaration, which was

```c++
int main(void)
{...
```

to a function linker that enforces an identical datatype between C++ and C (as we are using **c**types but are programming in C++ Cuda) with whichever arguments we desire.

```c++
extern "C" {
  int* divergence(int arg1, int arg2,...)
  {
    ...
    return times;
  }
```

where we provide the proper return type (here a pointer to an integer array). This code can be compiled by `nvcc` and sent to a `.so` (dynamic library) file with the proper flags:

```bash
(base) bbadger@pupu:~/Desktop/threebody$ nvcc -Xcompiler -fPIC -shared -o divergence.so divergence_kernel.cu
```

where our CUDA kernel is located in `divergence_kernel.cu` and we are sending the compiled library to `divergence.so`. Now we can call the `divergence` function in our `.so` file using `ctypes as follows:

```python
#! python
import numpy as np
import ctypes
import matplotlib.pyplot as plt 

f = ctypes.CDLL('./divergence.so').divergence
```

Next we need a pointer of the proper type in order to read the function `divergence` return, which here is `ctypes.c_int` as well as any argument types (here integers as well). Integers may be passed as arguments without supplying a type definition but other types (double, float, etc) must be supplied via `argtypes` definition.  We need the array size as well, so for a 300x300 array we specify a pointer to a block 300*300 ints large.  Then the file contents can be read into an `ctypes` object `<class '__main__.c_int_Array_90000'>`.

```python
dim = 300
f.argtypes = [ctypes.c_int, ctypes.c_int]
f.restype = ctypes.POINTER(ctypes.c_int * dim**2)
arr = f(arg1, arg2).contents
```

At this point we have `arr` which is a `ctypes` object that can be converted to a Numpy array very quickly by a direct cast, and the result may be plotted via Matplotlib as follows:

```python
time_array = np.array(arr)
time_steps = 50000
time_array = time_array.reshape(dim, dim)
time_array = time_steps - time_array
plt.style.use('dark_background')
plt.imshow(time_array, cmap='inferno')
plt.axis('off')
plt.savefig('Threebody_divergence_cuda.png', bbox_inches='tight', pad_inches=0, dpi=410)
plt.close()
```

when we compare a 1000x1000 array after 50,000 time steps using our CUDA kernel to the Torch-based calculation, we find that the output is identical (although with optimization the computation time is reduced by a factor of 2.4).

![profile]({{https://blbadger.github.io}}/3_body_problem/cuda_vs_torch.png)

For a write-up on the use of `ctypes` for cuda-defined functions where Python array arguments are defined and passed to CUDA code, see Bikulov's [blog post](https://bikulov.org/blog/2013/10/01/using-cuda-c-functions-in-python-via-.so-and-ctypes/). 

### Optimizing the Three Body Trajectory Computations

Many data-parallelized programs implemented on GPUs spend more clock cycles (and therefore typically total runtime) on memory management than actual computation.  This is nearly always true for deep learning mode inference and also holds for many more traditional graphics applications as well.  Memory management occurs both within the GPU and in transfers of data to and from the CPU.  For the three body simulation, a quick look at the code suggests that this program should spend very little time sending data to and from the GPU: we allocated memory for each array, initialized each one before sending to the GPU once, and then copied each array back to the CPU once the loop completes.  This can be confirmed by using a memory profiler such as Nsight-systems, which tells us that the memory copy from GPU (device) to CPU (host) for the 300x300 example requires only ~20ms.  From the screenshot below, it is clear that nearly all the GPU time is spent simply performing the necessary computations (blue boxes on top row).

![profile]({{https://blbadger.github.io}}/3_body_problem/nvidia-nsight.png)

It may therefore be wondered how much time our kernel spends reading and writing memory within the GPU itself, which for most consumer hardware is one form of vRAM or another.  GPUs have a few types of memory, and modern nvidia GPUs typically have what is termed global memory (which is the vRAM storage and is by far the largest form of memory available), shared local memory, and register memory (which was the smallest capacity and is usually reserved for variables). Speed is inversely proportional to the amount of storage the memory element contains, as is the case for different types of CPU memory.  In particular, global memory is not actually very fast at all and the idea is that the GPU hides this slow memory access via massive data parallelization.

When arrays (such as `double *p1_x`) are read or written to the GPU they are by default stored in global memory, which means that we may suspect our kernel as written above to be slower than overwise it perhaps might be because each thread reads and writes many array elements many times over, typically dozens of elements during each iteration. If global memory is indeed accessed at each iteration, it would be expected that changing the memory access pattern to use shared or register elements would drastically speed up the kernel.

Fortunately it is not too hard to change the memory access to register from global memory in the main loop of our CUDA kernel: all we have to do is to assign variables to the proper array elements before the loop commences, and then use only those variables until the loop ends.  Variables are by default stored in register memory, so we should not have any global memory access operations if we stick to using only variables in the loop.

```cuda
// kernel declaration
__global__
void divergence(int n, 
              int steps,
              double delta_t,
              int *times,
	...
	)
{
  int i = blockIdx.x*blockDim.x + threadIdx.x;
  int still_together = 1;
  int not_diverged = 1;
  int times_ind = times[i];
  double p1x = p1_x[i];
  double p1y = p1_y[i];
  double p1z = p1_z[i];
  int times_ind = times[i];
..
```

This approach has the added benefit of reducing the number of arguments to the `void divergence()` kernel, as intermediate variable arrays `nv1, dv_1_x` etc. can be initialized inside the kernel,

```cuda
double nv1x, nv1y, nv1z;
```

and in the loop we use these variables to avoid any array read/write operations

```cuda
  if (i < n){
    for (int j=0; j < steps; j++) {
        // compute accelerations
        dv_1_x = -9.8 * m_2 * (p1x - p2x) / pow(sqrt(pow(p1x - p2x, 2) + pow(p1y - p2y, 2) + pow(p1z - p2z, 2)), 3) \
                 -9.8 * m_3 * (p1x - p3x) / pow(sqrt(pow(p1x - p3x, 2) + pow(p1y - p3y, 2) + pow(p1z - p3z, 2)), 3);
```

and we can also avoid any `if` statements in the kernel (branches are typically slow for massively parallelized programming frameworks) as follows:

```
	// find which trajectories have diverged and increment times_ind
        not_diverged = sqrt(pow(p1x - p1_primex, 2) + pow(p1y - p1_primey, 2) + pow(p1z - p1_primez, 2)) <= critical_distance;
        still_together &= not_diverged; // still_together is initialized as an int with value 0
        times_ind = times_ind + still_together; 
```

The intermediate variables are updated at each iteration, and at the end of the loop the `times_ind` value is written to the `*times` array at the proper location.

```cuda
 	// compute new velocities
        nv1x = v1x + delta_t * dv_1_x;
	...
        nv1_primex = v1_primex + delta_t * dv_1pr_x;
	...
        // compute positions with current velocities
        p1x = p1x + delta_t * v1x;
	...
        p1_primex = p1_primex + delta_t * v1_primex;
	...
        // assign new velocities to current velocities
        v1x = nv1x;
	...
        v1_primex = nv1_primex;
	...
        }

    times[i] = times_ind;
```

Running this kernel, however, shows us that there is practically no difference in time saved when we avoid all inner loop array calls. This is because modern CUDA compiler versions optimize memory management for operations like this by default, such that the previously-read array elements are cached in registers without requiring explicit register allocation.  This indicates that our kernel is not memory-limitted but instead is compute-limited.

Now that we have convinced ourselves that the three body problem simulation is not memory bandwidth-limited, some experimentation can convince us that by far the most effective single change is to forego use of the `pow()` cuda kernel operator for simply multiplying together the necessary operands.  The reason for this is that the cuda `pow(base, exponent)` is designed to handle non-integer `exponent` values which make the evaluatation a [transcendental function](https://forums.developer.nvidia.com/t/register-usage-of-pow/23104), which on the hardware level naturally requires many more registers than one or two multiplication operations.

Thus we can forego the use of the `pow()` operator for direct multiplication in order to optimize the three body trajectory computation.  This change makes a somewhat-tedious CUDA code block become extremely tedious to write, so we can instead have a Python program write out the code for us.  

```python
def generate_string(a: str, b: str, c: str, d: str, t: str, , m: str, prime: bool) -> str:
    if prime:
        e = '_prime'
    else:
        e = ''
    first_denom =  f'sqrt(({a}{e}_x[i] - {b}{e}_x[i])*({a}{e}_x[i] - {b}{e}_x[i]) + ({a}{e}_y[i] - {b}{e}_y[i])*({a}{e}_y[i] - {b}{e}_y[i]) + ({a}{e}_z[i] - {b}{e}_z[i])*({a}{e}_z[i] - {b}{e}_z[i]))' 
    second_denom = f'sqrt(({c}{e}_x[i] - {d}{e}_x[i])*({c}{e}_x[i] - {d}{e}_x[i]) + ({c}{e}_y[i] - {d}{e}_y[i])*({c}{e}_y[i] - {d}{e}_y[i]) + ({c}{e}_z[i] - {d}{e}_z[i])*({c}{e}_z[i] - {d}{e}_z[i]))'
    template = f'''-9.8 * m_{m} * ({a}{e}_{t}[i] - {b}{e}_{t}[i]) / ({first_denom}*{first_denom}*{first_denom}) -9.8 * m_{m} * ({c}{e}_{t}[i] - {d}{e}_{t}[i]) / ({second_denom}*{second_denom}*{second_denom});'''
    return template

a, b = 'p3', 'p1'
c, d = 'p3', 'p2'
m = '3'
t = 'z'
print (generate_string(a, b, c, d, t, prime=True))
```

For the acceleration of planet 1, this gives us

```c++
  dv_1_x[i] = -9.8 * m_2 * (p1_x[i] - p2_x[i]) / (sqrt((p1_x[i] - p2_x[i])*(p1_x[i] - p2_x[i]) + (p1_y[i] - p2_y[i])*(p1_y[i] - p2_y[i]) + (p1_z[i] - p2_z[i])*(p1_z[i] - p2_z[i]))*sqrt((p1_x[i] - p2_x[i])*(p1_x[i] - p2_x[i]) + (p1_y[i] - p2_y[i])*(p1_y[i] - p2_y[i]) + (p1_z[i] - p2_z[i])*(p1_z[i] - p2_z[i]))*sqrt((p1_x[i] - p2_x[i])*(p1_x[i] - p2_x[i]) + (p1_y[i] - p2_y[i])*(p1_y[i] - p2_y[i]) + (p1_z[i] - p2_z[i])*(p1_z[i] - p2_z[i]))) -9.8 * m_3 * (p1_x[i] - p3_x[i]) / (sqrt((p1_x[i] - p3_x[i])*(p1_x[i] - p3_x[i]) + (p1_y[i] - p3_y[i])*(p1_y[i] - p3_y[i]) + (p1_z[i] - p3_z[i])*(p1_z[i] - p3_z[i]))*sqrt((p1_x[i] - p3_x[i])*(p1_x[i] - p3_x[i]) + (p1_y[i] - p3_y[i])*(p1_y[i] - p3_y[i]) + (p1_z[i] - p3_z[i])*(p1_z[i] - p3_z[i]))*sqrt((p1_x[i] - p3_x[i])*(p1_x[i] - p3_x[i]) + (p1_y[i] - p3_y[i])*(p1_y[i] - p3_y[i]) + (p1_z[i] - p3_z[i])*(p1_z[i] - p3_z[i])));
```

Likewise, we can remove the `pow()` operator from our divergence check by squaring both sides of the $L^2$ norm inequality given critical distance $c$,

$$
N = \sqrt{x^2_1 + x^2_2 + ... + x^2_n} < c \\
N^2 = {x^2_1 + x^2_2 + ... + x^2_n} < c * c
$$

which is implemented as

```c++
not_diverged[i] = (p1_x[i]-p1_prime_x[i])*(p1_x[i]-p1_prime_x[i]) + (p1_y[i]-p1_prime_y[i])*(p1_y[i]-p1_prime_y[i]) + (p1_z[i]-p1_prime_z[i])*(p1_z[i]-p1_prime_z[i]) < critical_distance*critical_distance;
```

other small optimizations we can perform are to change the evaluation of `still_together[i]` to a binary bit check

```c++
if (still_together[i] == 1){
        times[i]++;
      };
```

and the like. Finally, we can halt the CUDA kernel if a trajectory has diverged. This allows us to prevent the GPU from continuing to compute the trajectories of starting values that
have already diverged, which don't yield any useful information.

```c++
  int i = blockIdx.x*blockDim.x + threadIdx.x;
  for (int j=0; j < steps; j++) {
    if (i < n and still_together[i]){
    ...
```

In the case of block and thread size of 1, the following depicts the difference between our early stopping CUDA code and the torch-based method employed in [part 1](https://blbadger.github.io/3-body-problem.html).  

![early stopping]({{https://blbadger.github.io}}/3_body_problem/cuda_abbreviated.png)

With these optimizations in place, we have for a resolution of $300^2$ a runtime of

```bash
(base) bbadger@pupu:~/Desktop/threebody$ ./divergence
Elapsed Time: 44.9377s
```

which is a ~2.4x speedup compared to the `torch` code, a substantial improvement.  These optimizations become more effective as the number of iterations increases (and thus the area of the input that has already diverged increases): for example, for $i=90,000$ iterations at a resolution of $1000^2$ we have a runtime of 771s for the optimized CUDA kernel but 1951s for the `torch` version (a 2.53x speedup) and for $i=200,000$ we have 1181s for our CUDA kernel but 4390s for the torch version (3.7x speedup).  As the CUDA kernel is executed block-wise such that the computation only halts if all $i$ indicies for that block evaluate to `false`, decreasing the block size (and concomitantly the number of threads per block) in the kernel execution configuration can lead to modest speedups as well.


### Data precision optimization

Calculations performed in 64-bit double precision floating point format are in the case of the three body problem not optimally efficient.  This is because double precision floating point numbers (according to the IEEE 754 standard) reserve 11 bits for denoting the exponent, but the three body trajectories for the cases observed in [Part 1](https://blbadger.github.io/3-body-problem.html) rarely fall outside the range $[-1000, 1000]$.  This means that we are effectively wasting 9 bits of information with each calculation, as the bits encode information that is unused for our simulations.

Memory optimizations for GPUs go far beyond the goal of fitting our calculation into a device's vRAM (virtual random access memory, ie global GPU memory). To give an example, suppose we wanted to use 32-bit single precision floating point data for the three body problem computations. For a $1000^2$ resolution three body divergence computation, this decreases the memory requirements to 400MB vRAM from 685MB for double precision floating point. But single precision computation is also much faster: 50k iterations require only 215s with our optimized kernel (see above), which is less than half the time (472s) required for the same number of iterations using double precision.  

Thus we could make a substantial time and memory optimization by simply converting to single precision floating point data, but this comes with a problem: single precision leads to noticeable artefacts in the resulting divergence array, which are not present when performing computation using double precision.  Observe in the following plot the grainy appearance of the boundary of diverging regions near the center-right (compare this to the plots using double precision above).

![single precision artefacts]({{https://blbadger.github.io}}/3_body_problem/Threebody_divergence_cuda.png)

In effect, what we are observing is discrete behavior between adjacent pixels, which is a typical indicator of insufficient computational precision. What we want therefore is to use the unused bits in the `float` exponent for the mantissa. One way to do this is to define a new C++ data type using `struct` and convert to and from integers at each operation, but this is not 

We can attempt to increase the precision of our computations while maintaining the speedups that `float` offers by instead converting initial values to integers before performing computations using these integers.  This is known as fixed-point arithmetic, and we will use it to increase the precision of our datatype compared to the 6 or 7 decimal places of precision offered by `float`.

Perhaps the fastest way of performing fixed point precision calculations using CUDA is to bit shift integer type data. Bit shifting is the process of moving bits right or left in a bitwise representation of a number: for example, the integer 7 can be represented in 8-bit unsigned integer format as 00000101, and if we bit-shift thie number to the left by two then we have 00010100, which may be accomplished in C/C++ CUDA using double less-than symbols (assuming 8-bit unsigned integer format has been implemented),

```cuda
int8 number = 7 << 2 // 0010100
```

Similarly we can bit-shift numbers to the right as well, 

```cuda
int8 number = 7 >> 1 // 0000001
```

and in both cases the bits shifted out of the 8-bit field are lost.

To use bit shifting to perform fixed point decimal arithmetic, we simply shift all numbers by the number of bits we want to use as the decimal expansion of our approximations of real numbers, in this case 26 bits (corresponding to around 8 decimal digits).  The rest of the bits correspond to the whole number portions of the real numbers we are representing.  The rules for performing fixed point decimal arithmetic are simple: for addition and subtraction the shift must be made to both operands, but for multplication and division the shift must be made to only the operand that we are interested in.  For example, do define fractions like `critical_distance = 0.5` we shift the chosen number of bits and then use integer division by and unshifted value.

```cuda
#define SHIFT_AMOUNT 26
int critical_distance = (5 << SHIFT_AMOUNT) / 10; // 0.5
int m1 = 10 << SHIFT_AMOUNT; // 10
```

Things are more tricky when we perform multiplication or division when we care about both operands, where if we use two's complement (see the next section) we simply multiply operands and bit shift after doing so.

Representing negative numbers is also somewhat difficult and platform-dependent, as a signed integer may be offset by some amount (such that 00000001 signifies not 1 but $-2^{8-1}+1$) or else the first bit may signify the sign, where $10000101$ might be $-7$ and $00000101$ is $7$ and is much rarer for integer representations) or may involve two's complement, where all bits are flipped before adding 1 (-7 is now $00000101 \to 11111010 \to 11111011$ and is much more common as it does not waste any bit space).  

Unfortunately, fixed point arithmetic does not particularly effective here because the process of bit shifting itself requires much more time than a normal float computation.  Performing bit-shifted fixed point arithmetic requires around 70% more time for the three body problem than float arithmetic, meaning that the performance gains from switching to floats compared to double types are nearly eliminated.

Instead of changing the precision of all array elements in our simulation, we instead consider the idea that the precision of certain computations may be less important than the precision of others.  If this is the case, then we can change the precision of those precision-insensitive computations to decrease the program run-time without affecting the precision of the divergence plot itself.  

Some quick experimentation convinces us that most of the CUDA kernel compute time in the three body divergence simulation is taken up by the planet acceleration computations rather than the array element updates or the divergence checks themselves.  When considering one term of the $a_n$ acceleration computation,

$$
a_1 = \cdots -Gm_3\frac{p_1 - p_3}{\left( \sqrt{(p_{1, x} - p_{3, x})^2 + (p_{1, y} - p_{3, y})^2 + (p_{1, z} - p_{3, z})^2} \right) ^3}
$$

it may be wondered whether some of the computations in the denominator need to be quite as precise as those of the numerator.  This is because for each $x, y, z$ difference terms in the denominator are raised to a power of three (which necessarily reduces the accuracy after the decimal point for floating point arithmetic) and because the denominator simply scales the numerator and does not change the vector direction.

After some more experimentation we find that this guess is accurate (at least for the $x, y$ scale used previously, note that at a smaller spatial scale there is a noticeable difference in noise).  Replacing each `sqrt()` with a single-precision (rounded down) square root function `__fsqrt_rd()` for $i=90,000$ iterations at a resolution of $1000^2$ we have a total runtime of 525s, which is 3.72x faster than the `torch` version (and 1.5x faster than the `sqrt` kernel).  For $i=200,000$ iterations with the same resolution the fully optimized kernel requires only 851s, which is a speedup of 5.2x from the torch version.  Accuracy in the divergence estimation itself is not affected, however, as the divergence plot remains identical to the double-precision square root kernel version as seen below at $i=90,000$. Thus we find that reducing the precision of the square root functions in the denominator did not change the trajectory simulations with respect to divergence (at least not at this scale), which was what we wanted.

![sqrt compare]({{https://blbadger.github.io}}/3_body_problem/sqrt_compare.png)

With this much faster generation time, we can obtain larger views of the diverged regions.  For a wider view of the above region, we have

![sqrt compare]({{https://blbadger.github.io}}/3_body_problem/Threebody_divergence_wide.png)

And extending the x-axis to $x \in [-40, 470]$ by stitching together multiple plots we find that the region to the right is repeated and stretched at each repetition.

![compilation]({{https://blbadger.github.io}}/3_body_problem/threebody_compilation.png)

### Divergence fractal zoom

Thus far we have been focusing on the use of parallelized computation to map the stable (dark regions) and unstable (light regions) initial values of a planet at a familiar scale: around forty or sixty or (in the last plot) a few hundred meters.  Once the divergence map is generated, can we simply find the point of interest and have an answer to whether or not that point is stable? In this section we will see that no, the question of whether a point is stable for all time is much more difficult to answer than this.

To address this question, we might want more resolution.  If planet 1 at (x, y) coordinates $(-4.1, 2.3)$ diverges, what about $(-4.1001, 2.3)$? This is not possible to determine using our plot scaled at $x \in [-40, 40]$, but it may be when we compute thousands of points $x \in [-4.11, -4.10]$.

If the reader attempts to calculate these plots, they will find that there is not that much more detail present at these small scales than is found in our larger map.  But it should be remembered that we seek the eventual stability or instability of these points rather than the limited approximation currently observed.  In particular, for more accuracy at these smaller scales we must both increase the number of maximum iterations of our divergence plot (where each iteration is of Euler's approximation to the solution of an ordinary differential equation, $x_{n+1} = x_n + \delta t x'_n$) as well as decrease the size of the shift performed at the start of our computation such that $x_0 - x'_0 \to 0$.

Such a zoom video becomes computationally feasible for a single GPU using the optimizations detailed in the last two sections.  In the following video, the x-scale and y-scale decrease from 1.28km to 518nm, which to put in perspective means that each pixel is separated from its neighbors by the width of an atom.

{% include youtube.html id='a7Hu6kEzTfQ' %}

This video demonstrates a manifestation of why the three body problem is unsolvable: extreme sensitivity to initial conditions.  Here we see that at scales down to the width of an atom, stable and unstable trajectories may be arbitrarily close to one another.  It is not hard to see that this phenomenon is by no means limited to the scales we have investigated here but extends towards the infinitely small. Indeed, sensitivity to initial conditions stipulates that for any point in our x, y grid any other point arbitrarily close will eventually diverge, but difference in relative stability will also tend to infinity as $t \to \infty$ and $x_0 - x'_0 \to 0$.

This method is straightforward to implement but results in an increase in the amount of computation required for each successively smaller field of view. Practically speaking this means that even with the optimized CUDA kernel, it takes days to finish the computations required for a zoom video from the tens of meters range (in $x, y$) down to the micrometer range. A little experimentation shows us that subsituting the single-precision square root operation for the double-precision is no longer sufficient around the millimeter scale, as noise begins to overwhelm the divergence plot patterns if this substitution is made. 

How can we reduce the amount of computation per frame in a video on divergence? In other words, given a sequence of divergence maps of decreasing domain sizes, centered on the same region, can we reduce the amount of computation required for some or all of these maps? One way to reduce the total computation amount is to attempt to re-use previously computed values: if the divergence values for $(x, y)$ coordinates calcuated at one scale are again needed at a smaller scale, we can simply save that divergence value and look it up rather than re-calculating.

To implement this cached zoom approach, we can either save values in a cache in C++ such that the cache memory is not freed each time the kernel is called or else we can have the cache in the Python code that uses `ctypes` to interface with our .so CUDA kernel.  Here we will explore the latter, although there is no real expected performance difference between these approaches. 

Therefore we start by initializing our `already_computed` cache as a Python dictionary

```python
# python
already_computed = {}
```

and then we initialize the `time_steps` and `shift_distance` variables such that the number of timesteps increases linearly while the shifted distance decreases logarithmically as the zoom video frame number increases. Likewise we can specify the rate at which we are zooming in on the point `x_center, y_center` by dividing our initial range by the power of two appropriate. In the code below, dividing the video iteration i by 30 serves to have the video halve in range for every 30 frames.

Another decision to be made is how we want to look up coordinates that are sufficiently close to previously computed coordinates. For simplicity we will just round to a decimal place denoted by the variable `decimal`, which needs to increase as the scale decreases to maintain resolution.

```python
last_time_steps = 0
video_frames = 500
for i in range(video_frames):
	x_res, y_res = 1000, 1000
	...
	x_range = 40 / (2**(i/30))
	decimal = int(-np.log(x_range / (x_res)))

```
Before looking up the values, we need to increment the number of expected iterations until divergence for each stored input (because the number of iterations is continually increasing as the range decreases).

```python
	for pair in already_computed:
		already_computed[pair] += time_steps - last_time_steps
	last_time_steps = time_steps
```

Now we assemble an array of all locations that we do not have a stored value for,

```python
	start_x = x_center - x_range/2
	start_y = y_center - y_range/2
	for j in range(int(x_res*y_res)):
		remainder = j % y_res
		step = j // x_res
		x_i = start_x + x_range*(remainder/x_res)
		y_i = start_y + y_range*(step/y_res)

		if (round(x_i, decimal), round(y_i, decimal)) not in already_computed:
			x.append(x_i) 
			y.append(y_i)
			return_template.append(-1)
		else:
			return_template.append(already_computed[(round(x_i, decimal), round(y_i, decimal))])
	length_x, length_y = len(x), len(y)
```

and next we need to modifying the CUDA kernel driver C++ function to accept array objects (pointers) `x, y`.

```python
	f = ctypes.CDLL('./divergence_zoom.so').divergence
	x_array_type = ctypes.c_float * len(x)
	y_array_type = ctypes.c_float * len(y)
	x = x_array_type(*x)
	y = y_array_type(*y)

	f.argtypes = [ctypes.c_int, 
		ctypes.c_int, 
		ctypes.c_int, 
		ctypes.c_double, 
		ctypes.c_double, 
		ctypes.c_double, 
		ctypes.c_double, 
		ctypes.c_double, 
		ctypes.POINTER(ctypes.c_float*len(x)), 
		ctypes.POINTER(ctypes.c_float*len(y)),
		ctypes.c_int
		] 

	f.restype = ctypes.POINTER(ctypes.c_int * length_x) # kernel return type
	arr = f(x_res, y_res, time_steps, x_center, x_range, y_center, y_range, shift_distance, x, y, length_x).contents
	time_array = np.array(arr)
	flattened_arr = time_array.flatten()
	return_arr = []
	inc = 0
	for k in range(len(return_template)):
		if return_template[k] == -1:
			return_arr.append(flattened_arr[inc])
			already_computed[(round(x[inc], decimal), round(y[inc], decimal))] = flattened_arr[inc]
			inc += 1
		else:
			return_arr.append(return_template[k])
	output_array = np.array(return_arr).reshape(x_res, y_res)
	output_array = time_steps - output_array
	plot(output_array, i)
```
Deleting cache keys that are out-of-range or with too small precision allows us to prevent the cache from growing too large as the zoom video is computed.

Unfortunately, when implemented we see that this method has a significant flaw: we don't really want to re-use precomputed divergence computations from larger scales at smaller even if they are incremented to match an expected value for more total iteration. Because of sensitivity to initial conditions, such incremented estimates are bound to become more inaccurate the larger the difference in scale (and thus the number of maximum iterations). Our method of saving the values of certain elements is useful only if we can recall the values of elements at much smaller elements.

For example, observe the following figure where the decimal precision is fixed (and so the time to compute each successive iteration heads to zero as the zoom iteration increases).

![zoom errors]({{https://blbadger.github.io}}/3_body_problem/cached_zoom_errors.png)


### Multistep Linear Methods

It is a not generally well-known fact that although the advances in computational speed due to hardware and software have led to speed decreases on the order ot $10^7$ over the last seventy years, the advances in terms of better algorithms for many problems in the field of numerical analysis (which this work may be thought to fall under) during this same time have led to speed decreases that exceed even that of rapidly advancing hardware.

When we consider the three body problem divergence plot computation from an algorithmic optimization perspective, one apparent place for such optimization is in the number of iterations required for the plot which is generally on the order of $50,000$ but increases to more than $500,000$ at small scales.  So far we have optimized the computations per time step and removed superfluous steps for diverged trajectories but we have not 


The special case of a multistep linear method with only one step is just Euler's method.

$$
x_{n+1} = x_n + \Delta t * f(x_n)
$$

For the three body problem we must adapt this method somewhat as we can only compute acceleration using three variables (ie for the three body problem, positions of $x, y z$), but we cannot apply acceleration directly to position.  Instead we calculate the velocity $v$ from acceleration $v'$ before finding the next position $p_{n+1}$ from $p_{n}$ as follows:

$$
v(x, y, z)_{n+1} = v(x, y, z)_n + \Delta t * v' ((x, y, z)_n) \\
p(x, y, z)_{n+1} = p(x, y, z)_n + \Delta t * v((x, y, z)_n)
$$

To increase the order of the convergence of this dynamical system, we may use a higher-order linear multistep method, also known as Adams-Bashforth methods. The order of convergence is determined by the number of terms the method has, for example the two-step method 

$$
x_{n+2} = x_n + \Delta t *  \frac{1}{2}(3 f(x_{n+1}) - 1f(x_n))
$$

converges quadratically, and the four-step method

$$
x_{n+4} = x_n + \Delta t * \frac{1}{24}(55f(x_{n+3}) - 59f(x_{n+2}) + 37f(x_{n+1}) - 9f(x_n))
$$

converges with order 4. There are some choices available to us for how these methods are adapted to the three body problem trajectories, and below is an example of using a second-order multistep method for both velocity and position computation.

$$
v(x, y, z)_{n+2} = v(x, y, z)_n + \Delta t * \frac{1}{2}(3v' ((x, y, z)_{n+1})  - 1v'((x, y, z)_n)\\
p(x, y, z)_{n+2} = p(x, y, z)_n + \Delta t * \frac{1}{2}(3v ((x, y, z)_{n+1})  - 1v((x, y, z)_n)
$$

It may be unclear how one is supposed to first find $x_{n+1}, x_{n+2}...$ given only $x_n$ while using an order >1 multistep method. Here we use a one-step (Euler's) method for computing the first $x_{n+1}$.

Because multistep methods converge faster than the one-step Euler's method, we can in principle take fewer steps at a larger step size while maintaining accuracy.  The computational requirements for the three body problem scale somewhat less than linearly with the number of steps required, so reducing a step number by a factor of 10 leads to a substantial speedup. 

The order 4 linear multistep method may be implemented using a queue as follows:

```python
def adams_bashforth(self, current, fn_arr):
	assert len(fn_arr) >= order

	# note that array is newest to the right, oldest left
	fn, fn_1, fn_2, fn_3 = fn_arr[-1], fn_arr[-2], fn_arr[-3], fn_arr[-4]
	v = current + (1/24) * self.delta_t * (55*fn - 59*fn_1 + 37*fn_2 - 9*fn_3)
	return v
```
where for each array we compute and pop the oldest computed `fn` value, for example the velocity computation is as follows:

```python
dv1_arr = deque([])
...
nv1 = self.adams_bashforth(self.v1, dv1_arr)
dv1_arr.popleft()
```

When we apply linear multistep method to three body problem trajectories, we see that indeed the use of second or fourth-order updates allows for the use of a larger step size 

![adam-bashford]({{https://blbadger.github.io}}/3_body_problem/linear_multistep.png)

There is a problem with this approach, however: at very small scales, the use of a larger step size results in numerical artefacts regardless of the method used.  For example, we can see vertical lines of quickly-diverging regions appear for Euler's method as well as order 2 or order 4 Adams-Bashforth methods, which are absent from the order 1 method (Euler's).  

![adam-bashford]({{https://blbadger.github.io}}/3_body_problem/linear_multistep_artefacts.png)

These are evidently numerical instabilities in the calculation of the three body divergence, which leads to discrete shifts in stability for adjacent pixels (which is not expected to occur for our simulation unless the number of time steps heads towards infinity and the initial shift heads towards zero).  These instabilities lead to repetitive shifts in stability, where alternating patterns of stable and unstable regions exist.

## Parallelizing the Three Body Problem across many GPUs

### Single CPU threaded parallelization

In [Part II](https://blbadger.github.io/3-body-problem-3.html) we explored a number of different optimization strategies to make integrating a three body problem trajectory faster. The most effective of these (linear multistep methods) are unfortunately not sufficiently numerically stable for very high-resolution divergence plots at small scale, although compute optimizations when stacked together yielded a significant 4x decrease in runtime to Newton's method.

Perhaps the easiest way to further decrease the runtime of our three body kernel is to simply choose a device that is best suited for the type of computation required. In particular, the Nvidia RTX 3060 (like virtually all gaming GPUs) has poor support for the double precision computation that is necessary for high-resolution three body problem integration, so switching to a datacenter GPU such as the P100, V100, A100 etc. with more 64-bit cores will yield substantial speedups. Indeed, this is exactly what we find when a [V100 GPU](https://blbadger.github.io/gpu-server.html) is used instead of our 3060: a 9x decrease in runtime compared to the 3060. This is effectively stackable on top of the 

Another way to speed up the computational process is to use more than one GPU. This is a common approach in the field of deep learning, where large models are trained on thousands of GPUs simultaneously. Sophisticated algorithms are required for efficient use of resources during deep learning training, but the three body problem simulation is happily much simpler with respect to GPU memory movement: we only need to move memory onto the GPU at the start of the computation, and move it back to the CPU at the end. 

This is somewhat easier said than done, however. A naive approach would be to split each original array into however many parts as we have GPUs and then run our kernel on each GPU, and then combine the parts together. This approach has a few problems, however: firstly it would require a substantial re-write of our codebase, secondly copying memory from CUDA device to host must require pre-allocated space which is difficult in this scenario, and more importantly because the GPUs will not execute their kernels in parallel but in sequence. Because of this, we need to modify our approach. The goal will be to do so without modifying the kernel itself, as it is highly optimized already.

Perhaps the most straightforward way to do this is to work in the single thread, multiple data paradigm. The essentials of this approach applied to one array are shown below:

![threebody distributed]({{https://blbadger.github.io}}/3_body_problem/distributed_threebody.png)

Briefly, we first allocate CPU memory for each array in question (shown is the divergence iteration number array but also required are all positions, velocities etc.), find which index corresponds to an even split of this flattened array and make pointers to those positions, allocate GPU memory for each section and copy from CPU, asynchronously run the computations on the GPUs (such that the slowest device determines the speed), and asynchronously copy back to CPU memory.  This is all performed by one thread, and happily this approach requires no change to the `divergence()` kernel itself.

The first step (allocating CPU memory) requires a change in our driver code, however: asynchronous copy to and most importantly from the GPU requires paged-locked memory, rather than the pageable memory get when calling `malloc()`. Happily we can allocate and page-lock memory using the `cudaHostAlloc` call as follows: for our `times` divergence array of `int`s, we allocate using the address of our `times` pointer with the correct size and allocation properties.

```cuda
int *times,
cudaHostAlloc((void**)&times, N*sizeof(int), cudaHostAllocWriteCombined | cudaHostAllocMapped);
```

This replaces our `malloc()` used in the previous section. We repeat this for all arrays (x, y, z, velocity etc.) and can then initialize the arrays with values exactly as before, ie 

```cuda
int resolution = sqrt(N);
double range = 40;
double step_size = range / resolution;
for (int i = 0; i < N; i++) {
	times[i] = 0;
}
```
After allocation and initialization, we can find the number of GPUs we have to work with automatically. This can be done using the `cudaGetDeviceCount` function which expects a pointer to an integer, and assigns that integer the proper value via the pointer.

```cuda
int n_gpus;
cudaGetDeviceCount(&n_gpus);
```

Now that our arrays are allocated and initialized and we know the number of gpus in our system, we can proceed with distributing the arrays among the GPUs present. This is done by supplying `cudaSetDevice()` with an integer corresponding to the GPU number (0, 1, 2, etc.). As we are splitting each array into $n_gpus$ parts, it is helpful to assign variables to the number of array elements per GPU (`block_n`) as well as the starting and ending pointer positions for each block.

```cuda
// launch GPUs using one thread
for (int i=0; i<n_gpus; i++){
	std::cout << "GPU number " << i << " initialized" << "\n";
	// assumes that n_gpus divides N with no remainder, which is safe as N is a large square.
	int start_idx = (N/n_gpus)*i;
	int end_idx = start_idx + N/n_gpus;
	int block_n = N/n_gpus;
	cudaSetDevice(i);
```

Now we can proceed as before, using memory allocations such as `cudaMalloc(&d_p1_x, block_n*sizeof(double));` for each necessary sub-array of size `block_n`. After doing this we can copy just the GPU's block of memory to each allocated space (we don't actually need to specify an asynchronous memory copy)

```cuda
	cudaMemcpy(d_p1_x, p1_x+start_idx, block_n*sizeof(double), cudaMemcpyHostToDevice);
```
and call the kernel `divergence<<<(block_n+127)/128, 128>>>`. After the kernel is called we need to asynchronously copy memory back to the CPU as follows:

```cuda
	cudaMemcpyAsync(p1_x+start_idx, d_p1_x, block_n*sizeof(double), cudaMemcpyDeviceToHost);
```
which allows the loop to continue without waiting for each GPU to finish its computation and send data back to the CPU memory.

There are two final ingredients that we need to finish adapting this code for use with multiple GPUs. Firstly we need a synchronization step to prevent the process from completing prematurely. This can be done by adding `cudaDeviceSynchronize();` after the loop over `n_gpus`, which prevents further code from executing until the cuda devices on the current thread (all GPUs in this case) have completed their computation. Lastly we need to de-allocate our pinned memory: simply calling `free()` on the `cudaHostAlloc()` arrays leads to a segfault, one must instead use the proper cuda host deallocation. Instead we need to explicitly free the pinned memory via the cuda function `cudaFreeHost(var)`.

With that, we have a multi-gpu kernel that scales to any number of GPUs. The exact amount of time this would save any given computational procedure depends on a number of variables, but one can expect to find near-linear speedups for clusters with identical GPUs (meaning that for $n$ GPUs the expected completion time is $t/n$ where $t$ is the time to completion for a single GPU).  This can be shown in practice, where a cluster with one V100 GPU completes 50k iterations of a 1000x1000 starting grid of the three body problem in 73.9 seconds, whereas four V100s complete the same in 18.8s which corresponds to a speedup of 3.93x, which is very close to the expected value of 4x.

We can see this approach in action by observing the CPU and GPU utilizations while the simulation is underway. For a 56-core server CPU, we see that only one core is highly utilized: this is the thread that is running our four GPUs.

![threebody distributed]({{https://blbadger.github.io}}/3_body_problem/single_threaded_cpus.png)

And the GPUs may be checked via `nvidia-smi`, which shows us that indeed all four GPUs for this server are occupied with the simulation

![threebody distributed]({{https://blbadger.github.io}}/3_body_problem/single_threaded_gpus.png)

This is not quite the end of our efforts, however: we have thus far avoided performing cuda memory de-allocation, instead relying on automatic deallocation to occur after the kernel processes are completed (and the CPU process is terminated). But if we want to call this kernel repeatedly, say in a loop in order to obtain a zoom video, this approach is not quite complete and gives memory segmentation faults in a hardware implementation-specific manner. Why this is the case and how one can change the approach will be detailed in the last section on this page, as it is closely related to a problem we will find for multi-threading in the next section.

### Multithreaded Parallelization

Suppose that one wanted to squeeze a little more performance out of a compute node containing more than one GPU. How would one go about making all GPUs perform as closely as possible to what can be achieved with only one device?

One way to do this is conceptually straightforward: each GPU is assigned a single CPU thread. This should prevent a situation from occuring where one device waits for instructions from the CPU while it is sending other instructions to a different device. All we need is for the number of CPU threads to meet or exceed the number of GPU devices per node, which is practically guaranteed on modern hardware (typically one has over 10 CPU threads per GPU). 

It should be noted that this approach would only be expected to realize the smallest increases in performance for the three body simulation because this kernel is already highly optimized to remove as much communication from the CPU to GPU. One should not see any instances of a device waiting for a thread during kernel execution because no data and very few instructions are sent from CPU to GPU during kernel execution. This is apparent in the near-linear speedup observed in the last section. Nonetheless, as something of an exercise it is interesting to implement a multi-threaded multi-GPU kernel.

In practice multithreading a CUDA is more difficult because threading is usually implementation-specific, and combining multithreading with GPU acceleration is somewhat finnicky for a heave kernel like the one we have. The C libraries standard for multithreading (OpenMP, HPX, etc) were developed long before GPUs were capable of general purpose compute, and are by no means always compatible with CUDA. We will develop a multithreading approach with OpenMP that will illustrate some of the difficulties in doing so here.

Multithreading is in some sense a simple task: we have one thread (the one that begins program execution) initialize other threads to complete sub-tasks of a certain problem, and then either combine the threads or else combine the output in some way. The difficulty of doing this is that CPU threads are heavy (initializing a new thread is costly) and memory access must be made in a way to prevent race conditions where two threads read and write to the same data in memory in an unordered fashion. Happily much of this difficulty is abstracted away when one uses a library like OpenMP, although these are slightly leaky abstractions when it comes to CUDA.

We begin as before: arrays are allocated in pinned memory,

```cuda
cudaHostAlloc((void**)&x, N*sizeof(float), cudaHostAllocWriteCombined | cudaHostAllocMapped);
```

and then initialized as before, and likewise the number of GPUs is found. To make things more clear, it is best practice to use explicit data streams when sending data to multiple devices, rather than relying on default streams. Here we make an array of `n_gpus` streams.

```cuda
cudaStream_t streams[n_gpus];
```

and now we can use OpenMP to initialize more threads! OpenMP is one of the most widely-used multithreading libraries, and is used by Pytorch under the hood for distributed deep learning training algorithms such as Distributed Data Parallelism. It is typical to use one thread per GPU for those applications as well.

This can be done a few different ways: one would be to wrap the `for` loop in the last section (looping over devices) with the preprocessor directive `#pragma omp parallel for`, but this turns out to lead to difficult-to-debug problems with cuda memory access when more than two devices are used. It turns out to be more robust to proceed as follows: first we initialize one thread per device, we get the thread's integer number and assign the thread to the corresponding GPU device, and then we create a stream between that thread and the device.

```cuda
#pragma omp parallel num_threads(n_gpus)
{
	int d=omp_get_thread_num();
	cudaSetDevice(omp_get_thread_num());
	cudaStreamCreate(&streams[d]);
```

After doing so, we can allocate memory on the device for the portion of each array that device will compute. One must use `cudaMemcpyAsync` for both host->device as well as device-> host communication, and for clarity we also specify the stream associated with that device and thread in both memory copies and kernel call. Finally we synchronize each thread rather than the first driver thread.

```cuda
#pragma omp parallel num_threads(n_gpus)
{
	...
	// H->D
	cudaMalloc(&d_x, (N/n_gpus)*sizeof(float));
	cudaMemcpyAsync(d_x, x+start_idx, (N/n_gpus)*sizeof(float), cudaMemcpyHostToDevice, streams[device]);
	// kernel call
	divergence<<<(block_n+255)/256, 256, 0, streams[d]>>>(...)
	// D->H
	cudaMemcpyAsync(x+start_idx, d_x, (N/n_gpus)*sizeof(float), cudaMemcpyDeviceToHost, streams[device]);
	cudaDeviceSynchronize();
}
```

The following figure gives a simplified view of this process:

![multithreaded threebody]({{https://blbadger.github.io}}/3_body_problem/multithreaded_threebody.png)

The cuda kernel and driver code can be compiled using the `-fopenmp` flag for linux machines as follows:

```bash
badger@servbadge:~/Desktop/threebody$ nvcc -Xcompiler -fopenmp -o mdiv multi_divergence.cu
```

After doing so we find something interesting: parallelizing via this kernel is successful in a hardware implementation-specific manner. For a desktop with two GPUs there is no problem, but on a [4x V100 node](https://blbadger.github.io/gpu-server.html) with three or four GPUs available we find memory access errors that result from the CPU threads attempting to load and modify identical memory blocks. Close inspection reveals that this is due to the CPU (actually two CPUs for that node) attempting to access one identical memory location for each thread, which may be found by observing the string associated with the thrown `cudaError_t`,

```cuda
cudaError_t err = cudaGetLastError(); 
if (err != cudaSuccess) std::cout << "CUDA error: " << cudaGetErrorString(err) << std::endl;
```

Why does this happen? Consider what occurs when we call the following

```cuda
cudaMalloc(&d_x, (N/n_gpus)*sizeof(float));
cudaMemcpyAsync(d_x, x+start_idx, (N/n_gpus)*sizeof(float), cudaMemcpyHostToDevice, streams[device]);
```

with four different threads. Each thread is attempting to allocate memory on its 'own' device, but the address must be fetched and is identical as the variable `double *d_x` was only initialized once. Thus if we print the address of `d_x` for each thread we have one identical integer, which for me was the following address:

```
std::cout << &d_x; // 0x7ffedb04f2e8
```

Now for newer hardware implementations this does not lead to immediate problems because CPU threads are scheduled such that they do not experience read conflicts with `d_x`. But older hardware is not so lucky, and so instead we must use a separate memory location for each device's address. This can be done as follows: first we move 

```cuda
int n_gpus;
cudaGetDeviceCount(&n_gpus);
```

to the start of our C++ driver code, and then we initialize device array pointers as arrays with length equal to the number of GPUs,

```cuda
double * d_x[n_gpus]; // not double *d_x;
```

For an example of what we are accomplishing by initializing an array of integer (pointers), for four GPUs on the V100 cluster this gives us pointers with addresses `&d_x = [0x7fffdebe1a70, 0x7fffdebe1a78, 0x7fffdebe1a80, 0x7fffdebe1a88]` which are unique and indeed these addresses are contiguous in memory. Then as we launch threads, each GPU's thread gets a unique address for each array as we reference the pointer address corresponding to the thread number during device memory allocation

```cuda
#pragma omp parallel num_threads(n_gpus)
{
	int d=omp_get_thread_num();
	cudaMalloc(&d_x[d], block_n*sizeof(double));
	...
```

For example, if `omp_get_thread_num()` returns thread number 0 then we allocate that thread's cuda array block to address `&d_x[0] = 0x7fffdebe1a70`. But why do we need different addresses if each device is different, such that there is no memory conflicts on the devices? This is becase we are accessing the address *on the CPU* in order to allocate on the device, and this access will lead to segfaults if enough threads attempt this at one time (depending on the hardware implementation).

Now that the kernel is complete(ish), we can profile it! Recall that the single-threaded multi-GPU kernel for 50k steps with 1000x1000 resolution completes in around 18.8s on a 4x V100 cluster. If we run our multithreaded version we find that completion occurs in around 19.1s, a little worse. This is because CPU threads are relatively slow to initialize, and in this case slower to initialize than the extra time necessary for one thread to allocate and copy memory to each GPU.

### Multiple GPU memory deallocation

Thus far we have avoided the issue of memory deallocation for multiple GPUs. This is a more difficult issue than it otherwise might seem due to the under-the-hood implementation of cuda. First we will tackle the single-threaded multi-GPU kernel to illustrate the problem, and then use the knowledge gained to finish the multi-threaded multi-GPU kernel.

How would one deallocate memory on multiple GPUs and a single CPU? A naive approach to memory deallocation would be to simply free the host memory and then proceed to free each allocated memory block in each GPU, iterating through the devices one by one. As we are using pinned memory on the host, we have to free array `x` via `cudaFreeHost(x);` and doing so for all the arrays in the three body problem results in successful CPU memory deallocation. But if we implement this idea as follows

```cuda
for (int i=0; i<n_gpus; i++){
      std::cout << "Deallocating memory from GPU number " << i << "\n";
      cudaSetDevice(i);
      cudaFree(d_x[i]);
```
we run into an interesting problem: only the last GPU to have memory allocated (this happens sequentially remember when using one host thread) experiences successful deallocation. The other GPUs retain their allocated arrays until the host process is terminated. 

This might not seem like a huge problem, but if this kernel is driven by a loop (say via a `ctypes` interface with python to make a zoom video) then the GPUs will eventually overflow.  As each 64-bit 1000x1000 three body problem computation requires 452 MB in total per GPU, this occurs rather quickly. Moreover, depening on the hardware implementation a memory segmentation fault will be observed after a mere 2-4 iterations.

What these problems tell us is that the cuda interface forms a link between a pointer in CPU memory and its address for GPU memory allocation, and that critically only one link may be formed per CPU memory address. When one particular address in CPU memory (say `&d_x = 0x7fffdebe1a70` for example) is assigned to one particular GPU via `cudaSetDevice(d); cudaMalloc(&d_x, (N/n_gpus)*sizeof(float));` then that address may be re-assigned to another GPU's memory allocation but the CPU will not be able to free the block it first assigned to the other GPU. This implies that cuda is changing the memory allocation procedure in machine code, as an array at one address in a GPU should be able to be de-allocated regardless of the sequence of CPU thread operations.

To remedy this, we can take a similar approach as to what was implemented for multithreading: instead of a single pointer we allocate `d_x` as an array of unique pointers, of length equal to the number of GPUs. Then we can proceed with de-allocation

```cuda
for (int i=0; i<n_gpus; i++){
      cudaSetDevice(i);
      cudaFree(d_p1_x[i])
```

with this approach, memory is correctly de-allocated!

For the multithreaded kernel, we don't have to re-iterate over GPUs as we can instead have each CPU thread free its allocated GPU arrays. As long as the memory of any arrays of interest is copied back to the host before freeing, we will get the result we want. `cudaFree()` is a synchronous operation such that we don't really need to add `cudaDeviceSynchronize()` here, but it is added for clarity.

```cuda
	divergence<<<(block_n+127)/128, 128>>>(
	cudaMemcpyAsync(times+start_idx, d_times[d], block_n*sizeof(int), cudaMemcpyDeviceToHost, streams[d]);
	cudaDeviceSynchronize();
	cudaFree(d_x[d]);
```

And with that we have working kernels for as many GPUs as we have in one node, using either one CPU thread for all GPUs or one thread per GPU. Practically speaking, a three body zoom video that takes around three days to complete on an RTX 3060 requires only two hours with a 4x V100 node.

We can check that indeed each GPU is run by one CPU thread by using `htop`. For the same four-GPU server as above, we find that four CPU cores are 100% utilized and each core's process is running one GPU (which can be checked via `nvidia-smi`) which was what we wanted.

![threebody distributed]({{https://blbadger.github.io}}/3_body_problem/multithreaded_cpus.png)

Parallelizing across multiple nodes is also possible with MPI (Message Passing Interface), but this will not be explored here.













## The three body problem

### Newtonian mechanics

Newtonian mechanics are often thought to render the motion of planets and stars a solved problem: as the laws of motion have been found, the only thing left to do is to apply them to the heavenly bodies to know where they will go.  Consulting the section on Newtonian mechanics in Feynman's lectures on physics (vol. 1 section 9-9), we find this confident statement:

"Now armed with the tremendous power of Newton's laws, we can not only calculate such simple motions but also, given only a machine to handle the arithmetic, even the tremendously complex motions of the planets, to as high a degree of precision as we wish!"

This statement seems logical at first glance, and was the hope of Laplace and others immediately following Newton (but not, as we shall see, of Feynman).  The most accurate equations of force, momentum, and gravitational acceleration are all fairly simple and for most examples that are taught in school, there are solutions that do not involve time at all.  We can define the differential equations for which there exists a closed (non-infinite, or in other words practical) solution that does not contain any reference to the variable time as 'solved'.  The mechanics most of us learned were problems that were solvable with some calculus, usually by integrating over time to remove that variable.  The solution furthermore must be of finite length, and cannot itself grow as time increases.

If one peruses the curriculum generally taught to people just learning mechanics, a keen eye might spot something curious: the systems considered in the curriculum are all systems of two objects: a planet and a moon, or else the sun and earth.  This is the problem Newton inherited from Kepler, and the solution he found is the one we learn about today. 

But what about 3 bodies, or more?  Newton attempted to find a similar solution to this problem but failed.  This did not deter others, and it seems that some investigators (Laplace in particular) were confident of a solution being just around the corner, even if they could not find one themselves.

The three body problem may be formulated as follows:

$$
a_1 = -Gm_2\frac{p_1 - p_2}{\lvert p_1 - p_2 \rvert ^3} - Gm_3\frac{p_1 - p_3}{\lvert p_1 - p_3 \rvert^3} \\
\; \\
\; \\
a_2 = -Gm_3\frac{p_2 - p_3}{\lvert p_2 - p_3 \rvert ^3} - Gm_1\frac{p_2 - p_1}{\lvert p_2 - p_1 \rvert^3} \\
\; \\
\; \\
a_3 = -Gm_1\frac{p_3 - p_1}{\lvert p_3 - p_1 \rvert ^3} - Gm_2\frac{p_3 - p_2}{\lvert p_3 - p_2 \rvert^3} 
$$
 
where $p_i = (x_i, y_i, z_i)$ and $a_i$ refers to the acceleration of $p_i$, ie 

$$
a_i = (\ddot x_i, \ddot y_i, \ddot z_i)
$$

Note that $\lvert p_1 \rvert$ signifies a vector norm, not absolute value or cardinality. The vector norm is the distance to the origin, calculated in three dimensions as

$$
\lvert p_1 \rvert = \sqrt {x_1^2 + y_1^2 + z_1^2}
$$

The norm of the difference of two vectors may be understood as a distance between those vectors, if our distance function is an arbitrary dimension -extension of the function above.


### Modeling the three body problem

After a long succession of fruitless attempts, Bruns and Poincare showed that the three body problem does not contain a solution approachable with the method of integration used by Newton to solve the two body problem. No one could solve the three body problem, that is, make it into a self-contained algebraic expression, because it is impossible to do so!  

Is any solution possible?  Sundman found that there is an infinite power series that describes the three body problem, and in that sense there is.  But in the sense of actually predicting an orbit, the power series is no help because it cannot directly infer the position of any of the three bodies owing to round-off error propegation from extremely slow convergence ([ref](https://arxiv.org/pdf/1508.02312.pdf)).  From the point of a solution being one in which time is removed from the equation, the power series is more of a problem reformulation than a solution: the time variable is the power series base.  Rather than numerically integrate the differential equations of motion, one can instead add up the power series terms, but the latter option takes an extremely long time to do.  To give an appreciation for exactly how long, it has been estimated that more than $10^{8000000}$ terms are required for calculating the series for one short time step ([ref](http://articles.adsabs.harvard.edu/pdf/1930BuAst...6..417B)). 

For more information, see [Wolfram's notes](https://www.wolframscience.com/reference/notes/972d). 

Unfortunately for us, there is no general solution to the three body problem: we cannot actually tell where three bodies will be at an arbitrary time point in the future, let alone four or five bodies.  This is an inversion with respect to what is stated in the quotation above: armed with the power of Newton's laws, we cannot calculate, with arbitrary precision in finite time, the paths of any system of more than two objects.  

### Bounded trajectories of 3 objects are almost always aperiodic

Why is there a solution to the two body problem but not three body problem?  One can imagine that a problem with thousands of objects would be much harder to deal with than two objects, but why does adding only one more object create such a difficult problem?

One way to gain an appreciation for why is to simply plot some trajectories. Let's do this using python. To start with, a docstring is added and the relevant libraries are imported.

```python
#! python3
# A program that produces trajectories of three bodies
# according to Netwon's laws of gravitation

# import third-party libraries
import numpy as np 
import matplotlib.pyplot as plt 
from mpl_toolkits.mplot3d import Axes3D
plt.style.use('dark_background')
```

Next we can specify the initial conditions for our three bodies.  Here we have bodies with masses of 10, 20, and 30 kilograms, which could perhaps be a few small asteroids orbiting each other.  We then specify their initial positions and velocities, and for simplicity we assume that the bodies are very small such that collisions do not occur.

```python
# masses of planets
m_1 = 10
m_2 = 20
m_3 = 30

# starting coordinates for planets
# p1_start = x_1, y_1, z_1
p1_start = np.array([-10, 10, -11])
v1_start = np.array([-3, 0, 0])

# p2_start = x_2, y_2, z_2
p2_start = np.array([0, 0, 0])
v2_start = np.array([0, 0, 0])

# p3_start = x_3, y_3, z_3
p3_start = np.array([10, 10, 12])
v3_start = np.array([3, 0, 0])
```

Now for a function that calculates the change in velocity (acceleration) for each body, referred to as `planet_1_dv` etc. that uses the three body formulas above.

```python
def accelerations(p1, p2, p3):
	"""
	A function to calculate the derivatives of x, y, and z
	given 3 object and their locations according to Newton's laws
	
	"""

	m_1, m_2, m_3 = self.m1, self.m2, self.m3
	planet_1_dv = -9.8 * m_2 * (p1 - p2)/(np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2 + (p1[2] - p2[2])**2)**3) - \
		       9.8 * m_3 * (p1 - p3)/(np.sqrt((p1[0] - p3[0])**2 + (p1[1] - p3[1])**2 + (p1[2] - p3[2])**2)**3)

	planet_2_dv = -9.8 * m_3 * (p2 - p3)/(np.sqrt((p2[0] - p3[0])**2 + (p2[1] - p3[1])**2 + (p2[2] - p3[2])**2)**3) - \
		       9.8 * m_1 * (p2 - p1)/(np.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2 + (p2[2] - p1[2])**2)**3)

	planet_3_dv = -9.8 * m_1 * (p3 - p1)/(np.sqrt((p3[0] - p1[0])**2 + (p3[1] - p1[1])**2 + (p3[2] - p1[2])**2)**3) - \
		       9.8 * m_2 * (p3 - p2)/(np.sqrt((p3[0] - p2[0])**2 + (p3[1] - p2[1])**2 + (p3[2] - p2[2])**2)**3)

	return planet_1_dv, planet_2_dv, planet_3_dv
	
```

A time step size `delta_t` is chosen, which should be small for accuracy, and the number of steps are specified and an array corresponding to the trajectory of each point is initialized.  Varying initial velocites are allowed, so both position and velocity require array initialization.

```python
# parameters
delta_t = 0.001
steps = 200000

# initialize trajectory array
p1 = np.array([[0.,0.,0.] for i in range(steps)])
v1 = np.array([[0.,0.,0.] for i in range(steps)])

p2 = np.array([[0.,0.,0.] for j in range(steps)])
v2 = np.array([[0.,0.,0.] for j in range(steps)])

p3 = np.array([[0.,0.,0.] for k in range(steps)])
v3 = np.array([[0.,0.,0.] for k in range(steps)])

```

The first element of each position and velocity array is assigned to the variables denoting starting coordinates,

```python
# starting point and velocity
p1[0], p2[0], p3[0] = p1_start, p2_start, p3_start

v1[0], v2[0], v3[0] = v1_start, v2_start, v3_start
```

and the velocity and position at each point is calculated for each time step.  For clarity, a two-step approach is chosen here such that the three body eqution is used to calculate the accelerations for each body given its position.  Then the acceleration is applied using Euler's method such that a new velocity `v1[i + 1]` is calculated. Using the current velocity, the new position `p1[i + 1]` is calculated using Euler's formula.

```python
# evolution of the system
for i in range(steps-1):
	# calculate derivatives
	dv1, dv2, dv3 = accelerations(p1[i], p2[i], p3[i])

	v1[i + 1] = v1[i] + dv1 * delta_t
	v2[i + 1] = v2[i] + dv2 * delta_t
	v3[i + 1] = v3[i] + dv3 * delta_t

	p1[i + 1] = p1[i] + v1[i] * delta_t
	p2[i + 1] = p2[i] + v2[i] * delta_t
	p3[i + 1] = p3[i] + v3[i] * delta_t
```

After the loop above runs, we are left with six arrays: three position arrays and three velocity arrays containing information on each body at each time step.  For plotting trajectories, we are only interested in the position at each time.  Some quick list comprehension can separate x, y, and z from each array for the `plt.plot()` method.  p1, p2, and p3 are colored red, white, and blue, respectively.

```python
fig = plt.figure(figsize=(8, 8))
ax = fig.gca(projection='3d')
plt.gca().patch.set_facecolor('black')

plt.plot([i[0] for i in p1], [j[1] for j in p1], [k[2] for k in p1] , '^', color='red', lw = 0.05, markersize = 0.01, alpha=0.5)
plt.plot([i[0] for i in p2], [j[1] for j in p2], [k[2] for k in p2] , '^', color='white', lw = 0.05, markersize = 0.01, alpha=0.5)
plt.plot([i[0] for i in p3], [j[1] for j in p3], [k[2] for k in p3] , '^', color='blue', lw = 0.05, markersize = 0.01, alpha=0.5)

plt.axis('on')

# optional: use if reference axes skeleton is desired,
# ie plt.axis is set to 'on'
ax.set_xticks([]), ax.set_yticks([]), ax.set_zticks([])

# make panes have the same color as the background
ax.w_xaxis.set_pane_color((0.0, 0.0, 0.0, 1.0)), ax.w_yaxis.set_pane_color((0.0, 0.0, 0.0, 1.0)), ax.w_zaxis.set_pane_color((0.0, 0.0, 0.0, 1.0))
plt.show()
plt.close()

```
And we are done!  This yields the following map of the trajectories in 3D space. 

![3 body image]({{https://blbadger.github.io}}/3_body_problem/3_body_3_axes.png)

With grid lines removed for clarity (`ax.set_xticks([]), ax.set_yticks([]), ax.set_zticks([])` removed),

![3 body image]({{https://blbadger.github.io}}/3_body_problem/3_body_3.png)

The orientation of the view can be specified using the interactive matplotlib interface if `plt.show()` is called, and if images are saved directly then `plt.view_init()` should be used as follows:

```python
...
ax.view_init(elev = 10, azim = 40)
plt.savefig('{}'.format(3_body_image), dpi=300)

```

Over time (sped up for brevity) and from a slightly different perspective, the trajectory is

![3 body vid]({{https://blbadger.github.io}}/3_body_problem/three_body_full.gif)


### Poincar and sensitivity to initial conditions

What happens when we shift the starting position of one of the bodies by a miniscule amount? Changing the z position of the third body from $12$ to $12.000001$ yields

![3 body image]({{https://blbadger.github.io}}/3_body_problem/3_body_3_shifted_all.png)

The trajectories are different to what was observed before, but how so?  The new image has different x, y, and z-scales than before and individual trajectories are somewhat difficult to make out.  

If we compare the trajectory of the first body to it's trajectory (here red) in the slightly shifted scenario (blue),

![3 body image]({{https://blbadger.github.io}}/3_body_problem/3_body_shifted_1.png)

from the side and over time, we can see that they are precisely aligned for a time but then diverge, first slowly then quickly.

![3 body vid]({{https://blbadger.github.io}}/3_body_problem/three_body_shifted.gif)

The same is true of the second body (red for original trajectory, blue for the trajectory when the tiny shift to the third body's initial position has been made)

![3 body image]({{https://blbadger.github.io}}/3_body_problem/3_body_shifted_2.png)

And of the third as well.

![3 body image]({{https://blbadger.github.io}}/3_body_problem/3_body_shifted_3.png)

Plotting the distance between each point and its counterpart as follows

```python
distance_2 = []
for i in range(steps):
	distance_2.append(np.sqrt(np.sum([j**2 for j in p2[i] - p2_prime[i]])))

distance_1 = []
for i in range(steps):
	distance_1.append(np.sqrt(np.sum([j**2 for j in p1[i] - p1_prime[i]])))

distance_3 = []
for i in range(steps):
	distance_3.append(np.sqrt(np.sum([j**2 for j in p3[i] - p3_prime[i]])))

fig, ax = plt.subplots()
ax.plot(time, distance_1, alpha=0.5, color='red')
ax.plot(time, distance_2, alpha=0.5, color='grey')
ax.plot(time, distance_3, alpha=0.5, color='blue')
plt.ylim(0, 120)
ax.set(xlabel='time', ylabel='Distance')
plt.show()
plt.close()
```

yields

![3 body image]({{https://blbadger.github.io}}/3_body_problem/three_body_distance.png)


In 1914, H. Poincar observed that "small differences in initial conditions produce very great ones in the final phenomena" (as quoted [here](https://books.google.com/books?id=vGuYDwAAQBAJ&pg=PA271&lpg=PA271&dq=Poincare+3+body+problem+impossibility+1880&source=bl&ots=yteTecRsK8&sig=ACfU3U2ngm5xUXygi-JdLzpU0bwORuOq7Q&hl=en&sa=X&ved=2ahUKEwiO4JT_86zqAhUlZjUKHYn5Dk8Q6AEwDHoECAwQAQ#v=onepage&q=Poincare%203%20body%20problem%20impossibility%201880&f=false)). 

If we make slightly worse and worse initial measurements, does the inaccuracy of our prediction get worse too?  Surprisingly the answer to this question is often but not always.  To see how, here we iterate the distance measurement above but change the distance between planet 3 and its 'true' value to increase from $\Delta z = 0.00000001 \to \Delta z = 0.00001$ 

![3 body image]({{https://blbadger.github.io}}/3_body_problem/three_body_distance.gif)

Just as for the [logistic map](logistic-map.md), the benefit we achieve for getting better initial accuracies is unpredictable. Sometimes a better initial measurement will lead to larger errors in prediction!

### Divergence and Smale Horseshoes with Homoclinic Tangles

We have so far observed the behavior of only one initial set of points in three-dimensional space for each planet.  But it is especially interesting to consider the behavior of many initial points.  One could consider the behavior of all points in $\Bbb R^3$, but this is often difficult to visualize as the trajectories may form something like a solid object.  To avoid this difficulty, we can restrict our initial points to some subspace.

In $\Bbb R^3$ we can choose vectors in two basis vectors, perhaps $x, y$, to vary while the third $z$ stays constant.  Allowing two basis vectors to change freely in $\Bbb R^3$ forms a two-dimensional sheet in that three-dimensional space, or in other words forms a plane. What happens to points embedded in this plane as they observe Newton's laws of gravitation, and in particular do different points in this plane have different sensitivities to initial conditions?  Another way of saying this is that we will ask the question of which starting positions diverge earlier, and which diverge later.

As we have three bodies, we can choose one of these (say `p1`) to allow to start in different locations in a two-dimensional plane while holding the other two bodies fixed in their starting points before observing the trajectory resulting from each of these different starting locations.  As we are interested in divergence, we can once again compare the trajectory of the planet `p1` at each of these starting points to a slightly shifted planet `p1_prime`.  As we are now dealing with an multidimensional array of initial points rather than only three per planet, a slightly different approach is required.  Code snippets and the overall approach will be described here, and the complete program may be found in this [source code](https://github.com/blbadger/threebody/blob/master/divergence.py) file.

Our computational structure to calculate sensitivity is initialized by changing the 3x1x1 initial point for planet 1 to an array of size 3 byy_res by x_res for that planet.  For example, if we want to plot 100 points along both the x and y-axis we start with a 3x1000x1000 size array for `p1`.  

```python
def sensitivity(self, y_res, x_res, steps, double_type=True):
	"""
	Plots the sensitivity to initial values per starting point of planet 1, as
	measured by the time until divergence.
	"""

	delta_t = self.delta_t
	y, x = np.arange(-20, 20, 40/y_res), np.arange(-20, 20, 40/x_res)
	grid = np.meshgrid(x, y)
	grid2 = np.meshgrid(x, y)

	# grid of all -11, identical starting z-values
	z = np.zeros(grid[0].shape) - 11

	# shift the grid by a small amount
	grid2 = grid2[0] + 1e-3, grid2[1] + 1e-3
	# grid of all -11, identical starting z-values
	z_prime = np.zeros(grid[0].shape) - 11 + 1e-3

	# p1_start = x_1, y_1, z_1
	p1 = np.array([grid[0], grid[1], z])
	p1_prime = np.array([grid2[0], grid2[1], z_prime])
		
```
But now we also have to have arrays of identical dimensions for all the other planets, as their trajectories each depend on the initial points of planet 1.  This means that we must make arrays that track the motion of each planet in three-dimensional space for all points in the initial plane of possible planet 1 values.

For example, planet two can be initialized to the same value as in our approach above, or $(0, 0, 0)$ for both position and velocity, with the following:

```python
	p2 = np.array([np.zeros(grid[0].shape), np.zeros(grid[0].shape), np.zeros(grid[0].shape)])
	v2 = np.array([np.zeros(grid[0].shape), np.zeros(grid[0].shape), np.zeros(grid[0].shape)])
	...
```

Numpy is an indespensible library for mathematical computations, but there are others that may be faster for certain computations.  To speed up the three body divergence calculations we can employ pytorch to convert these numerical arrays from `numpy.ndarray()` objects to `torch.Tensor()` as follows:

```python
	# convert numpy arrays to torch.Tensor() objects (2x speedup with cpu)
	p1, p2, p3 = torch.Tensor(p1), torch.Tensor(p2), torch.Tensor(p3)
	...
```
before doing the same for `v1`, `v2`, `v3`, `p1_prime`, etc.  If the device performing these computations contains a graphics processing unit, these arrays can be computed using that by specifying `p1 = torch.Tensor(p1, device=torch.device('cuda'))` for each `torch.Tensor()` object to speed up the computations even further.

Sensitivity to initial values can be tested by observing if any of the `p1` trajectories have separated significantly from the shifted `p1_prime` values.  This is known as divergence, and the following function creates a boolean array to see which points have separated (diverged):

```python
def not_diverged(self, p1, p1_prime):
	"""
	Find which trajectories have diverged from their shifted values

	Args:
		p1: np.ndarray[np.meshgrid[bool]]
		p1_prime: np.ndarray[np.meshgrid[bool]]

	Return:
		bool_arr: np.ndarray[bool]

	"""
	separation_arr = np.sqrt((p1[0] - p1_prime[0])**2 + (p1[1] - p1_prime[1])**2 + (p1[2] - p1_prime[2])**2)
	bool_arr = separation_arr <= self.distance
	return bool_arr
		
```

Using this function to test whether or not planet 1 has diverged from its shifted planet 1 prime,

```python
def sensitivity(self, y_res, x_res, steps, double_type=True):
	...
	time_array = np.zeros(grid[0].shape)
	# bool array of all True
	still_together = grid[0] < 1e10
	t = time.time()

	# evolution of the system
	for i in range(self.time_steps):
		if i % 100 == 0:
			print (i)
			print (f'Elapsed time: {time.time() - t} seconds')

		not_diverged = self.not_diverged(p1, p1_prime)
		# convert Tensor[bool] object to numpy array
		not_diverged = not_diverged.numpy()
		
		# points still together are not diverging now and have not previously
		still_together &= not_diverged

		# apply boolean mask to ndarray time_array
		time_array[still_together] += 1
```

now we can follow the trajectories of each planet at each initial point.  Note that these trajectories are no longer tracked, as doing so requires an enormous amount of memory.  Instead the points and velocities are simply updated at each time step.

```python
		# calculate derivatives
		dv1, dv2, dv3 = self.accelerations(p1, p2, p3)
		dv1_prime, dv2_prime, dv3_prime = self.accelerations(p1_prime, p2_prime, p3_prime)

		nv1 = v1 + dv1 * delta_t
		nv2 = v2 + dv2 * delta_t
		nv3 = v3 + dv3 * delta_t

		p1 = p1 + v1 * delta_t
		p2 = p2 + v2 * delta_t
		p3 = p3 + v3 * delta_t
		v1, v2, v3 = nv1, nv2, nv3
```

After $50,000$ time steps, initial points on the $x, y$ plane of planet 1 (y-axis running top to bottom and x-axis running left to right) on both x- and y-axes such that the top left is $(y, x) = (-20, -20)$ and the bottom right is $(y, x) = (20, 20)$) we have

![homoclinic tangle]({{https://blbadger.github.io}}/3_body_problem/Threebody_divergence_xy.png)

where lighter values indicate earlier divergence. The folded and stretched topology here is known as Smale's horseshoe, also known as the baker's dough topology.  This map results when a section of space is repeatedly stretched and folded in upon itself, mirroring the process of dough kneading during the bread preparation process.  As time goes by, more and more 'folds' are apparent, for example as $t_i = 0 \to t_i =  100,000$ we have

{% include youtube.html id='Vp4r8SfWoEA' %}

If we observe this same plane but starting the planet 1's z-value at 10.9 and moving to 11.15, we have

{% include youtube.html id='dco0Xg9TOCk' %}

It is worth considering what these maps tells us.  In a certain region of 2-dimensional space, a planet's starting point may be shifted only slightly to result in a large difference in the earliest time of divergence.  This is equivalent to saying that a planet's starting point, within a certain region of space, may yield an unpredicable (if it is a point of fast divergence) or relatively predictable (if divergence is slower) trajectory, but even knowing which one of these two possibilities will occur is extremely difficult.  

Moreover, knowing which starting points diverge for one set of planets does not allow us to understand which starting points diverge for slightly different planetary masses. If we change the mass of planet 1 from 30 to 5.9 (kg) linearly we have

{% include youtube.html id='Y_4NSytASmI' %}

and changing the force of gravity from $g = 0 \to g = 30.7\; m/s^2$ we have

{% include youtube.html id='mp69jdbRm1c' %}

This topology is not special to points on the $x, y$ plane: on the $y, z$ plane (holding $x=-10$) with $z$ on the vertical axis and $y$ on the horizontal such that the bottom left is $(y, z) = (-20, -20)$ and the top right is $(y, z) = (20, 20)$ after $50,000$ time steps,

![homoclinic tangle]({{https://blbadger.github.io}}/3_body_problem/Threebody_divergence_yz.png)

<!--
which, going from $t_i = 0 \to t_i =  150,000$ we have

{% include youtube.html id='aahfR5Lpqps' %}
-->

Notice something interesting in the $y, z$ divergence pattern that does not exist in the $x, y$ map: a line of mirror symmetry, running from bottom left to top right with a slope of just more than 1.  Why does this symmetry exist, and why does it not exist for the $x, y$ map?  Most importantly, why are both maps of divergence filled with such intricate detail?

We can address the first question as follows: something must exist in the initial positions of the second and third planets such that symmetry results for the $y, z$ plane but not $z, y$ plane.  Thinking about this question in general terms, what would cause our first planet to behave identically with regards to divergence rate at two different locations in space?  A readily available answer is that two starting points that result in identical (up to a translation or mirror symmetry) trajectories in space should experience the same divergence rate.  After all, if the trajectories are indistinguishable then it would be very strange if one were to diverge faster than the other.

Now consider the placement of planets 2 and 3 in three-dimensional space.  Which points on the $y, z$ plane are of equal distance to planets 2 and 3?  The answer is the points that are equidistant from any certain point on the line connecting planets 2 and 3, projected onto the $y, z$ plane. Each pair of points in the $y, z$ plane that are equidistant from any point on this line will thus have identical trajectories, and should therefore have equal divergence which follows from our earlier argument.

Is this the case?  Recalling that the initial points for planet 2 and planet 3 are

$$
(p2_x, p2_y, p2_z) = (0, 0, 0) \\
(p3_x, p3_y, p3_z) = (10, 10, 12)
$$ 

the line connecting these points projected onto the $y, z$ plane has the equation $z=(12/10)y$.  Plotting this line together with the map formed by observing divergence in the $y, z$ plane for $x=-10$, we see that indeed this is our line of mirror symmetry:

![threebody projection]({{https://blbadger.github.io}}/3_body_problem/Threebody_ogproj500.png)

Why does our $x, y$ plane not exhibit such symmetry? After all, projecting the line connecting p2 and p3 onto the $x, y$ plane we have $y=x$ so why is there no line of symmetry about the diagonal?  This is because the initial velocites for both p1 as well as p3 contain non-zero components of $x$.  Now imagine any two initial points that are equidistant from a point on the line $y-x$.  Are the trajectories of these two points still identical given that they have different identical but non-zero starting velocities in the $x, y$ plane? They are, because for one point of the pair the initial velocity vector will cause the approach to the line of symmetry to come sooner, whereas for the other point it will be longer or may not occur at all.

To begin to answer the second question of why such detailed shapes form when we plot divergence time, one can ask the following: which initial points of the $y, z$ plane land close to the line of symmetry as the planets move over time?  Because the trajectory of all three bodies are completely determined by their initial positions (and velocities), for any initial value of $y_0, z_0$ that approaches the line of symmetry such that $(12/10)y_i - z_i < \delta$ then the initial value's mirror point $y'_0, z'_0$ also approaches the line, as the trajectories and distances to that line are identical for these initial points.

Thus although could very well pick any other region to investigate the question of which initial points (for p1) end up there at any time, the region of initial mirror symmetry has one important simplifying aspect: the resulting map will stay symmetric about the initial line of symmetry, making it easier to see where the points are located.

The code to plot this is as follows:

```python
def plot_projection(self, divergence_array, i):
	...
	divergence_array[(self.p1[1] * 12 - self.p1[2] * 10 < 1).numpy() & (self.p1[1] * 12 - self.p1[2] * 10 > -1).numpy()] = i
	plt.imshow(divergence_array, cmap='inferno', extent=[-20, 20, -20, 20])
	plt.axis('on')
	plt.xlabel('y axis', fontsize=7)
	plt.ylabel('z axis', fontsize=7)
	plt.savefig('Threebody_divergence{0:04d}.png'.format(i//100), bbox_inches='tight', dpi=420)
	plt.close()
```

such that each initial point in the $y, z$ plane that is close to (specifically within 2 units using the Manhattan distance metric) of the line $z=(12/10)y$ appear as white spots.  At $t=50,000$, the map is as follows

![threebody projection]({{https://blbadger.github.io}}/3_body_problem/Threebody_projection_yz.png)

Now we can attempt to understand how the divergence map attains the horseshoe topology by observing the points that are mapped to our line of symmetry over time, as the horseshoe map itself forms over time.  The points which map to the line $z=(12/10)y$ exist on a relatively stable manifold.  Why is the line of symmetry relatively stable? 

We can see that indeed the initial points which map to the line of symmetry also tend to be stable by simply observing that in our figure, the white points mostly occupy the dark regions in the divergence map.  But why is this the case?  All points starting exactly on the line of symmetry (meaning that p1 starts on any point where $10z = 12y$) will remain on this line because in that case all three planets exist in a plane, and with no initial velocity in the $y, z$ plane they will stay in that plane and thus are accurately modeled in two dimensions.  This means that their trajectories will be periodic (see the next section for more details) and therefore divergence will not occur, making these starting points stable.

What about the case for trajectories of p1 that reach the line of symmetry after some time, why do they tend to be more stable?  We now have a far more difficult question to address, but for an investigation into one- and two-dimensional analogues of that question [this page](https://blbadger.github.io/aperiodic-inverted.html).

Observing as $t_i=0 \to t_i=87,800$ we have:

{% include youtube.html id='dl198kBuKTI' %}

Notice how these points on a stable manifold continually intersect, or more accurately meet and become repelled by unstable regions such that they elongate and gradually form a web-like mesh.  This dynamic structure was termed a 'homoclinic tangle' by Poincar, and was later shown by Smale to imply and be implied by the horseshoe map.

To see what happens when we observe a different region, here is the map of which initial points are located near the line $z=(12/10)y + 3$ at any given time from $t_i=0 \to t_i=150,000$

{% include youtube.html id='YX76cAmFbkg' %}

So in one sense, the regions of quickly- and slowly-diverging points are arranged in such a complicated and detailed fashion because they result from the continual mixing of stable (slowly diverging) and unstable (quickly diverging) regions of space.

We can also ask which points in the $y, z$ plane end up in an arbitrary region, and not limit ourselves to lines near our line of symmetry. Observing which points map to the interior of a circle of radius $5$ centered on the point $y, z = 20, 10$ we have

{% include youtube.html id='mA2ca62Puus' %}

This clearly shows us how for the three body system, space is stretched and folded.  Stretched here is where approximate circles are flatten out and elongated, and folded is where new regions mapped to our circle seem to appear out of nothing.

Is there a specific region in which initial positions that have diverged are more likely to be found compared to positions of planet 1 that have not diverged?  One guess is that trajectories in which planets that are 'ejected', ie sent far away from each other, are those for which the divergence is more likely to occur.  We can plot the initial points where planet 1 is at least 150 units from the origin in white as follows:

{% include youtube.html id='Ig3e0wW4Eyo' %}

Which suggests that some but not all ejected trajectories are also diverged, and on the other hand some non-diverged trajectories are also those that experience ejection.


### Two body versus three body problems
How does this help us understand the difference between the two body and three body problems?  Let's examine the two body problem as a restricted case of the three body problem: the same differential equations used above can be used to describe the two body problem simply by setting on the of masses to 0.  Lets remove the first object's mass, and also change the second object's initial velocity for a trajectory that is easier to see.

```python
# masses of planets
m_1 = 0
m_2 = 20
m_3 = 30

...

# p2_start = x_2, y_2, z_2
p2_start = np.array([0, 0, 0])
v2_start = np.array([-3, 0, 0])
```

Plotting the trajectories of p2 and p3,

![3 body image]({{https://blbadger.github.io}}/3_body_problem/two_body_1.png)

This plot looks much more regular!  As we will later see, these trajectories form periodic orbits that, like other two body trajectories, lie along a plane.  We can do some fancy rotation in three dimensional space by changing using a second loop after our array-filling loop to show this.

```python
...

for t in range(360):
	fig = plt.figure(figsize=(10, 10))
	ax = fig.gca(projection='3d')
	plt.gca().patch.set_facecolor('black')
	
	plt.plot([i[0] for i in p2], [j[1] for j in p2], [k[2] for k in p2] , '^', color='white', lw = 0.05, markersize = 0.01, alpha=0.5)
	plt.plot([i[0] for i in p3], [j[1] for j in p3], [k[2] for k in p3] , '^', color='blue', lw = 0.05, markersize = 0.01, alpha=0.5)

	plt.axis('on')
	# optional: use if reference axes skeleton is desired,
	# ie plt.axis is set to 'on'
	ax.set_xticks([]), ax.set_yticks([]), ax.set_zticks([])

	# make panes have the same color as the background
	ax.w_xaxis.set_pane_color((0.0, 0.0, 0.0, 1.0))
	ax.w_yaxis.set_pane_color((0.0, 0.0, 0.0, 1.0))
	ax.w_zaxis.set_pane_color((0.0, 0.0, 0.0, 1.0))
	
	ax.view_init(elev = 20, azim = t)
	plt.savefig('{}'.format(t), dpi=300, bbox_inches='tight')
	plt.close()
	
```

![3 body image]({{https://blbadger.github.io}}/3_body_problem/two_body_rotated_2.gif)

An aside: aren't trajectory crossings impossible for ordinary differential equations?  For the case of a single object moving in space, this is correct, because any trajectory crossing would imply that some point heads toward two different points next, an impossibility for any function.  But as we have two objects, each with velocity as well as position vectors, crossings can occur if the other object is in a different place than before.  On the other hand, it would be impossible for the two planets to occupy the same position they held previously, with the same velocity vectors, without re-visiting future points.

Now let's see what happens when we shift the starting value of one of the points by the same amount as before ($z_3 = 12 \to z_3 = 12.000001$).

![3 body image]({{https://blbadger.github.io}}/3_body_problem/two_body_1_shifted.png)

The trajectories looks the same!  When both original and shifted trajectories of the $p_2$ are plotted, it is clear to see that there is no separation
($z_3 = 12$ in white and $z_3 = 12.000001$ in blue)

![3 body image]({{https://blbadger.github.io}}/3_body_problem/two_body_shifted_2.png)

This means that this trajectory of a two body problem is not sensitive to initial conditions: it is not chaotic.  Is this always the case regardless of the initial positions and velocities?  Indeed it is, as by the Poincar-Bendixson theorem all continuous trajectories in two dimensions [are periodic](https://blbadger.github.io/continuity-poincare.html).  As was later shown by Lorenz and others, periodicity implies insensitivity to initial values and thus no two-dimensional continuous map can be chaotic.

Thus it turns out that periodicity (and asensitivity to initial values) is the rule for all two body problems: all are periodic or quasi-periodic, meaning that future trajectories are identical to past trajectories.  This means that we can remove (all but a negligable amount) of the time variable when we integrate these differential equations.  

On the other hand, most (all but a miniscule number of) three body trajectories are aperiodic.  And this in turn means that their future trajectories are never exactly like previous ones such that we cannot remove time from the differential equations.  This makes them unsolvable, with respect to a solution that does not consist of adding up time steps from start to finish.

### The three body problem is general 

One can hope that the three (or more) body problem is restricted to celestial mechanics, and that it does not find its way into other fields of study.  Great effort has been expended to learn about the orbitals an electron will make around the nucleus of a proton, so hopefully this knowledge is transferrable to an atom with more than one proton. Unfortunately it does not: any three-dimensional system with three or more objects that operates according to nonlinear forces (gravity, electromagnetism etc.) reaches the same difficulties outlined above for planets. 

This was appreciated by Feynman, who states in his lectures (2-9):

"[Classical mechanics] is deterministic.  Suppose, however, that we have a finite accuracy and do not know exactly where just one atom is, say to one part in a billion.  Then as it goes along it hits another atom...if we start with only a tiny error it rapidly magnifies to a very great uncertainty.... Speaking more precisely, given an arbitrary accuracy, no matter how precise, one can find a time long enough that we cannot make predictions valid for that long a time"

The idea that error magnifies over time is only true of nonlinear systems, and in particular aperiodic nonlinear systems. On this page, we have seen that in a nonlinear system of two planets, error does not magnify whereas it does for the cases observed with three planets.  Therefore measurement error does not necessarily become magnified, but only for aperiodic dynamical systems. This was the salient recognition of Lorenz, who found that aperiodicity and sensitivity to initial conditions (which is equivalent to magnification of error) implied one another.

### Does it matter that we cannot solve the three body problem, given that we can just simulate the problem on a computer?

When one hears about solutions to the three body problem, they are either restricted to a (miniscule) subset of initial conditions or else are references to the process of numerical integration by a computer.  The latter idea gives rise to the sometimes-held opinion that the three body problem is in fact solveable now that high speed computers are present, because one can simply use extremely precise numeric methods to provide a solution.  

To gain an appreciation for why computers cannot solve our problem, let's first pretend that perfect observations were able to be made.  Would we then be able to use a program to calculate the future trajectory of a planetary system exactly?  We have seen that we cannot when small imperfections exist in observation, but what about if these imperfections do not exist?  Even then we cannot, because it appears that Newton's gravitational constant, like practically all other constants, is an irrational number.  This means that even a perfect measurement of G would not help because it would take infinite time to enter into a computer exactly.

That said, computational methods are very good for determining short-term trajectories.  Furthermore, when certain bodies are much larger in mass than others (as is the case in the solar system where the sun is much more massive than all the planets combined), the ability to determine trajectories is substantially enhanced. But like any aperiodic equations system, the ability to determine trajectories for all time is not possible.

## About The Author

My name is Benjamin Badger and I hope you have enjoyed perusing these pages as much as I did making them.  I have recently completed a doctoral degree, and here I am writing my dissertation.

![writing in the orchard]({{https://blbadger.github.io}}/misc_images/orchard.JPG)

In my free time I enjoy playing music, spending time with friends, playing sports, snowboarding, reading, and occasionally fixing my car. 

![skiing]({{https://blbadger.github.io}}/assets/images/skiing.jpg)

A playing a balanced game of smash bros. is a pleasure as well.

![smash bros]({{https://blbadger.github.io}}/assets/images/smash_bros.png)

If you would like to get in touch, email me at blbadger10 at  gmail dot com.








## Additivity and order

### Probability 

Imagine tossing a fair coin and trying to predict the outcome. There exists a 50% chance of success, but there is no way to know with any certainty what will happen.  Now imagine tossing the coin thousands of times, and consider what you will be able to predict: each toss continues to be random and completely uncertain, so one's ability to predict the outcome of each individual toss does not change.  But now add the results of the tosses together, assigning an arbitrary value to heads and a different value to tails.  As the number of individual coin tosses increases, one is better able to predict what value the sum will take.  Moreover, if the same coin toss experiment is repeated many times (say repeating 1000 tosses 1000 times), we can predict what the sum of the tosses will be with even more accuracy.  

Specifically, there are two possible outcomes for each toss (heads or tails) and so a binomial distribution is appropriate to model the expected value $E(X)$ after $n$ tosses as follows:

$$
E(X) = n * p \\
E(X) = n * \frac{1}{2}
$$

where $p$ is probability of the coin landing in a specific orientation, say heads. 

The variance $V(X)$ approximates a binomial distribution centered around the expected value of each toss times the number of tosses.

$$  
V(X) = n * p (1-p) \\
V(X) = n \left( \frac{1}{2} \right) ^2
$$

And the standard deviation is the square root of the variance, 

$$
\sigma = \sqrt{n \left( 1/2 \right)^2}
$$

As n increases, $\sigma$ shrinks with respect to $E(V)$: after 100 tosses the standard deviation is 10% of the expected value, and after 1000 tosses the standard deviation is ~3% of the expected value, whereas after 10000 tosses the standard deviation is only 1% of the expected value. This is because $\sigma$ increases in proportion to the square root of $n$, whereas $E(V)$ increases linearly with $n$.

The second observation is perhaps more striking: the additive transformation on a coin toss leads to the gaussian distribution with arbitrarily close precision.  Let's simulate this so that we don't have to actually toss a coin millions of times, assigning the value of $1$ to heads and $0$ to tails.  We can compare the resulting distribution of heads to the expected normal distribution using `scipy.stats.norm` as follows:

```python
import numpy
import matplotlib.pyplot as plt
from scipy.stats import norm

flips = 10
trials = 10
sum_array = []
for i in range(trials):
	s = 0
	for j in range(flips):
		s += numpy.random.randint(0, 2)
	sum_array.append(s)

variance = flips * (0.5) ** 2
y = numpy.linspace(0, flips, 100)
z = norm.pdf(y, loc=(flips/2), scale=(variance**0.5))

final_array = [0 for i in range(flips)]
for i in sum_array:
	final_array[i] += 1

fig, ax = plt.subplots()
ax.plot(final_array, alpha=0.5)
ax.plot(y, z*trials, alpha=0.5)
plt.show()
plt.close()
```

Let's assign $n$ as the number of tosses for each experiment and $t$ as the number of experiments plotted.  For 10 experiments ($t=10$) of 10 flips ($n=10$) each, the normal distribution (orange) is an inaccurate estimation of the the actual distribution (blue).

![gaussian]({{https://blbadger.github.io}}/assets/images/coing_10_10.png)

As we go from 100 tries of 100 tosses each to 1000000 tries of 100 tosses each, the normal distribution is fit more and more perfectly.  For $n=100, t=100$,the distribution more closely 
approximates the normal curve.

![gaussian]({{https://blbadger.github.io}}/assets/images/coin_100_100.png)

For $n = 100, t = 1000$, 

![gaussian]({{https://blbadger.github.io}}/assets/images/coin_100_1k.png)

for $n=100, t=1 * 10^4$,

![gaussian]({{https://blbadger.github.io}}/assets/images/coin_100_10k.png)

for $n=100, t=1 * 10^5$,

![gaussian]({{https://blbadger.github.io}}/assets/images/coin_100_100k.png)

and for $n=100, t=1 * 10^6$, there is almost no discernable difference between the Gaussian curve centered on the expectation value of $50$ and the actual distribution.

![gaussian]({{https://blbadger.github.io}}/assets/images/coin_100_1mil.png)


Let's take a moment to appreciate what has happened here: a random input can be mapped to a curve with arbitrary precision simply by adding outputs together.  (Well, not completely random: digital computers actually produce pseudo-random outputs that are periodic with an exceedingly large interval. But we do not need to worry about that here, as our output number is far less than what would be necessary for repetition.)

This observation is general: individual random events such as a die roll or card shuffle cut are quite unpredictable, but adding together many random events yields a precise mapping to a gaussian curve centered at the expectation value.  Each individual event remains just as unpredictable as the last, but the sum is predictable to an arbitrary degree given enough events.  One way to look at these observations is to see that addition orders random events into a very non-random map, and if we were to find the sum of sums (ie integrate under the gaussian curve) then a number would be reached with arbitrary precision given unlimited coin flips and experiments.

### Brownian motion

There is an interesting physical manifestation of the abstract statistical property of additive ordering: Brownian motion, the irregular motion of small (pollen grain size or smaller) particles in fluid.  Thermal motions are often thought of as random or stochastic, meaning that they are described by probability distributions.  Sush motions of fluid molecules add together on the surface of a larger particle to result in a non-differentiable path of that particle.  Movement along this path is termed Brownian motion, after the naturalist Brown who showed that this motion is not the result of a biological process (although it very much resembles the paths taken by small protists living in pond water). 

In three dimensions, Browniam motion leads to a movement away from the initial particle position.  The direction of this movement is unpredictable, but over many experiments (or with many particles underging brownian motion at once), the distances of the particles away from the initial point together form a Gaussian distribution (see [here](https://en.wikipedia.org/wiki/Brownian_motion) for a good summary of this).  Regardless of the speed or trajectory of each individual particle undergoing Brownian motion, an ensemble that start at the same location form a Gaussian distribution with arbitrary accuracy, given enough particles. 

### White and fractional noise

Noise may signify the presence of sound, or it may mean any observation that is not classified as a signal. Both meanings are applicable here. White noise is defined as being frequency-independent: over time, neither low nor high frequencies are more likely to be observed than the other.  Fractional noise is defined here as any noise that is inversely frequency-dependent: lower frequency signal occurs more often than higher frequency signal.  Inverse frequency noise is also called pink or $1/f$ noise.

White noise is unpredictable, and 'purely' random: one cannot predict the frequency or intensity of a future signal any better than by chance.  But because fractional noise decays in total intensity with an increase in frequency, this type of noise is not completely random (meaning that it is somewhat predictable).  Consider one specific type of fractional noise, Brown noise, which for our purpose is characterized by a $\frac{1}{f^n}, \; n > 0$ frequency vs. intensity spectrum.  This is the noise that results from Brownian motion (hence the name).  It may come as no surprise that another way to generate this noise is to integrate (sum) white noise, as the section above argues that Brownian motion itself acts to integrate random thermal motion.  

As noted by [Mandelbrot](https://books.google.com/books/about/The_Fractal_Geometry_of_Nature.html?id=0R2LkE3N7-oC) and [Bak](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.59.381), fractional noise results from fractal objects as they change over time.  Brownian noise coming from Brownian motion is simply a special case of this more general phenomenon: Brownian motion traces out fractal paths.  These paths are self-similar in that very small sections of the path resemble much larger sections, not that smaller portions exactly match the whole as is the case for certain geometric fractals.  This type of self-similarity is sometmies called statistical self-similarity, and is intimately linked to the property this motion exhibits of being nowhere-differentiable.  For more about fractals, see [here](/fractal-geometry.md).

In general terms, Brownian motion is a fractal because it appears to trace paths that are rough and jagged at every scale until the atomic, meaning that at large scales the paths somewhat resemble the Weierstrauss function above.  Now imaging trying to measure the length of such a shape: the more you zoom in, the longer the measurement is!  Truly nowhere-differentiable paths are of infinite length and instead can be characterized by how many objects (a box or sphere or any other regular shape) it takes to cover the curve at smaller and smaller scales (either by making the shapes themselves smaller or by making the curve larger).  The ratio of the logarithm of the change in the number of objects necessary for covering the curve (a fraction) divided by the logarithm of the change in scale is the counting dimension.  Equivalently,

$$
D = \frac{log(N)}{log(1/r)} \\
N =  \left( \frac{1}{r} \right) ^D \\
$$


where $D$ is the fractal dimension, $N$ is the ratio of the number of objects required to cover the curve, and $1/r$ is the change in scale expressed as a fraction.  Note that this is equivalent to the equation for fractional (Brownian) noise,

$$
I = \left( \frac{1}{f} \right) ^n
$$

where the number of objects required to cover the curve N is eqivalent to the signal intensity I, the radius of the objects r is equivalent to the frequency f, and the dimension D is equivalent to the noise scaling factor n.  

If the fractal dimension is larger than the object's topological dimension, it is termed a fractal.  For very rough paths, an increase in scale by, say, twofold leads to a larger increase in the number of objects necessary to cover the path because the path length has increased relative to the scale. Brownian trails in two dimensions have a fractal dimension of approximately $2$, meaning that they cover area even though they are topologically one-dimensional. 


###  Gambler's fallacy

Statistical distributions of the natural world rarely conform to Gaussian distributions.  Instead, rare events are often overrepresented: the more data that is obtained, the larger the standard deviation describing those observations.  This is equivalent to the fractal noise mentioned above, where uncommon events (low frequency signal) are over-represented compared to the expected value for these events if modeled with white noise.  This implies that truly independent samples are rare, a conclusion that is often in agreement with experimentation.  For example, the weather a place receives from one day to the next is not completely independent, as a downpour one day will lead to greater humidity in the hours following, which necessarily affects the weather of the hours following.

The gambler's fallacy may be defined as a tendancy to assume that events are not independent. In light of the ubiquity of fractal noise and scarcity of white noise in the natural world, it may be that the gambler's fallacy is actually a more accurate interpretation of most natural processes than one assuming independent events.


### Nonlinear statistics

Additivity is usually assumed in the calculation of expectated values:

$$
E(A + B) = E(A) + E(B) \\
if \; E(A) \; \bot \; E(B)
$$

or the expected value of event A or B is the same as the expected value of A plus the expected value of B.  Additivity is a given for independent events.  Additivity also holds for pseudoindependent events, meaning those that are dependent but not non-randomly dependent.  When events are non-randomly dependent, additivity is no longer valid because a change in the expectation value of one event could change the expectation value of the other:

$$
E(A + B) \neq E(A) + E(B) \\
if \; \lnot E(A) \; \bot \; E(B)
$$

Markov processes attempt to model non-random dependencies by providing each state with a separate probability rule, and then by applying standard linear statistical rules to each separate state.  This is similar to the decomposition of a curve into many small lines and is a practical way of addressing simpler probability states such as those that exist while playing cards, but is extremely difficult to use to model states where each decision leads to exponentially more states in the future (as is the case for chess, where one starting configuration leads to $10^{46}$ possibilities for future ones). 

It may instead be more effective to consider a nonlinear model of statistics in which additivity is not assumed (meaning that the expected value of one variable may affect the expected value of another in a nonrandom (and noncomplementary) way.  One could imagine a Markov chain-like graph in which the state probabilites continually updated depending on the path to each state.


### Randomized fractals 

One example of organization is found in randomized fractals.  These are shapes that can be obtained by from random (pseudorandom, as no digital computer is capable of truly random number generation) inputs that are restricted in some way.  Take the Sierpinski triangle:

![sierpinski]({{https://blbadger.github.io}}misc_images/sierpinski_triangle.png)

This fractal may be constructed in a deterministic fashion by instructing a computer to draw exactly where we want it to using recursion on a base template consisting of seven line segments with exact angles between them (for more on exactly how to draw this fractal, see [here](https://blbadger.github.io/fractal-geometry.html)).  

The Sierpinski triangle can also be constructed using random inputs in a surprising way: take three points that are the edges of an equilateral triangle and add an initial point somewhere in this triangle.  Assign two numbers on a dice to each vertex, roll the dice, and move half way towards the vertex indicated by the dice and repeat.  This can be done in python as follows: 

```python
a = (-400,-300)
b = (0, 300)
c = (400, -300)

def randomized_sierpinski(steps, a, b, c):
	'''
	A function that draws the sierpinski triangle using random.randint, accepts arguments 'steps' (int)
	for the number of points to plot and 2D vertex points a, b, and c (tuples) as the corners of an 
	equilateral triangle.
	'''
	for i in range(steps):
		pos = [j for j in turtle.pos()] # find current position

		n = random.randint(0, 6) # dice roll
		turtle.pu()
		# move half way towards the vertex indicated by the dice roll
		if n < 2:
			turtle.goto((pos[0]+a[0])/2, (pos[1]+a[1])/2)

		elif n >= 4:
			turtle.goto((pos[0]+b[0])/2, (pos[1]+b[1])/2)

		else:
			turtle.goto((pos[0]+c[0])/2, (pos[1]+c[1])/2)

		turtle.pd()
		# skip the first 100 rolls 
		if i > 100:
			turtle.forward(0.5)
```

One might think that this procedure would result in a haze of random points in the triangle.  Indeed, if the verticies of a square are used instead of a triangle then the plotted points occupy random positions in this square and do not make a pattern.  But in a triangle, the following shape is made:

![randomized sierpinski]({{https://blbadger.github.io}}misc_images/randomized_sierpinksi_2.gif)

Guessing where the next point will land is not possible, but as more and more iterations are made each iteration comes arbitrarily close to the Sierpinski triangle.  This occurs regardless of where the initial point is located!  The points, when added togather, approximate the intricate structure of a fractal.

For a more dramatic example of the organizing effect of a decrease in distance travelled towards the chosen vertex on each iteration, observe what happens when we go from $d = 1 \to d = 1/4$, or in other words when we go from $(x, y) + (v_x, v_y)$ to $((x, y) + (v_x, v_y))/4)$ on each iteration:

![randomized sierpinski]({{https://blbadger.github.io}}misc_images/random_sierpinski_distance.gif)

A similar process can be used to make a Sierpinski carpet.  The idea is to specify the location of four verticies of a rectangle as $a, b, c, d$.  Each iteration proceeds as above, except that the pointer only moves $1/3$ the distance to the vertex chosen by the random number generator. 

```python
from turtle import *
import turtle
import random

a = (-500,-500)
b = (-500, 500)
c = (500, -500)
d = (500, 500)
div = 3
def randomized_sierpinski(steps, a, b, c, d, div):
	for i in range(steps):
		pos = [j for j in turtle.pos()]

		n = random.randint(0, 8)
		turtle.pu()
		if n < 2:
			turtle.goto((pos[0]+a[0])/div, (pos[1]+a[1])/div)

		elif n >= 6:
			turtle.goto((pos[0]+b[0])/div, (pos[1]+b[1])/div)

		elif n < 6 and n >= 4:
			turtle.goto((pos[0]+c[0])/div, (pos[1]+c[1])/div)

		elif n < 4 and n >= 2:
			turtle.goto((pos[0]+d[0])/div, (pos[1]+d[1])/div)

		turtle.pd()
		if i > 100:
			turtle.forward(0.5)
```

And the setup for viewing the resulting carpet from $d=1 \to d=1/4$ is

```python
...
div = 1
...
for i in range(300):
	turtle.hideturtle()
	turtle.speed(0)
	turtle.delay(0)

	turtle.pensize(2)
	width = 1900
	height = 1080
	turtle.setup (width, height, startx=0, starty=0)

	print (i)
	randomized_sierpinski(5000, a, b, c, d, div + i/100)
	turtle_screen = turtle.getscreen()
	turtle_screen.getcanvas().postscript(file="randomized_sierpinski{0:03d}.eps".format(i), colormode = 'color')
	turtle.reset()
```
which results in a Sierpinski carpet. 

![randomized sierpinski]({{https://blbadger.github.io}}misc_images/randomized_carpet.gif)

### Irrationals are not closed under addition

[Here](https://blbadger.github.io/aperiodic-irrationals.html) it is established for a certain class of dynamical equation that periodic maps exist in a bijective correspondence with the rational numbers, and that aperiodic maps correspond to irrational numbers $\Bbb I$.  What happens if we add together multiple aperiodic maps: can a periodic map ever result?  Defining addition on maps here could be simply a composition of one map with another.

$\Bbb I$ is not closed under addition (or subtraction).  For example, $x_1 = 1 + \pi$ is irrational and $x_2 = 2 - \pi$ is irrational but $y = x_1 + x_2 = 3$ which is rational.  By equivalence two aperiodic maps may be added together to yield a periodic one, according to the method of digits (see the link in the last paragraph).  

But note that $\Bbb Q$ is closed under addition (and subtraction), meaning that two rationals will not add to yield an irrational.  This means that, given a set $S$ of numbers in $\Bbb R$, addition may convert elements of $S$ from irrational to rational.  For a set of trajectory maps $M$, addition may lead to the transformation of aperiodic trajectories to periodic.

### Aside: quantum mechanics and non-differentiable motion
 
The wave-like behavior of small particles such as photons or electrons is one of the fundamnetal aspects of the physics of small objects.  Accordingly, the equations of quantum mechanics have their root in equations describing macroscopic waves of water and sound.  Now macroscopic waves are the result of motion of particles much smaller than the waves themselves.  As quantum particles are well-described by wave equations, it seems logical to ask whether or not these particles are actually composed of many smaller particles in the same way macroscopic waves are.  

As a specific example of what this would mean, imagine a photon as a collection of particles undergoing browninan motion for a time that is manifested by the photon's wavelength: longer wavelengths mean more time has elapsed whereas smaller wavelengths signify less time. When a photon is detected, it forms a Gaussian distribution and this is exactly the pattern formed by a large collection of particles undergoing Brownian motion for a specified time.  The same is true for any small particle: the wavefunctions ascribed to these objects may be represented as Brownian motion of many particles.  

If this seems strange, consider that this line of reasoning predicts that the paths of small particles, as far as they can be determined, are non-differentiable everywhere, just as the path of a particle undergoing Brownian motion is.  In the path integral approach to quantum events, this is exactly what is assumed and the mathematics used to make sense of quantum particle interactions (uncertainty relations etc.) is derived from the mathematics used to understand Brownian motion of macroscopic particles.  This math is tricky because nowhere-differentiable motion is not only poorly understood in a simple form by calculus, but the paths themselves of such particles are infinitely long, leading to a number of paradoxes. 

The similarities that exist between how quantum particles behave and how macroscopic particles behave when undergoing Brownian motion suggests that we consider the possibility of small particles existing in a similar environment to macroscopic ones in fluid.  








 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
## Periodicity and reversibility

Suppose one were to observe movement over time, and wanted to describe the movement mathematically in order to be able to predict what happens in the future.  This is the goal of dynamical theory, and other pages on this site should convince one that precise knowledge of the future is much more difficult than it would seem even when a precise dynamical equation is known.  

What about if someone were curious about the past?  Given a system of dynamical equations describing how an object's movement occurs, can we find out where the object came from?  This question is addressed here for two relatively simple iterative systems, the logistic and Henon maps.  

### The logistic map is non-invertible

The logistic equation, which has been explored [here](https://blbadger.github.io/logistic-map.html) and [here](https://blbadger.github.io/logistic-boundary.html) is as follows:

$$
x_{n+1} = rx_n(1-x_n)
\tag{1} \label{eq1}
$$

The logistic equation is a one-dimensional discrete dynamical map of very interesting behavior: periodicity for some values of $r$, aperiodicity for others, and for $0 < r < 4$, the interval $(0, 1)$ is mapped into itself. 

What does \eqref{eq1} look like in reverse, or in other words given any value $x_{n+1}$ which are the values of $x_n$ which when evaluated with \eqref{eq1} yield $x_{n+1}$?  Upon substituting $x_n$ for $x_{n+1}$, 

$$
x_n = rx_{n+1}(1-x_{n+1}) \\
0 = -rx_{n+1}^2 + rx_{n+1} - x_n \\
$$

The term $x_n$ can be treated as a constant (as it is constant for any given input value $x_n$), and therefore we can solve this expression for $x_{n+1}$ with the quadratic formula $ax^2 + bx + c$ where $a = -r$, $b = r$, and $c = -x_n$, to give

$$
x_{n+1} = \frac{r \pm \sqrt{r^2-4rx_n}}{2r}
\tag{2} \label{eq2}
$$

Now the first thing to note is that this dynamical equation is not strictly a function: it maps a single input $x_n$ to two outputs $x_{n+1}$ (one value for $+$ or $-$ taken in the numerator) for many values of $x_n \in (0, 1)$ whereas a function by definition has one output for any input that exists in the pre-image set.  In other words the logistic map is non-invertible.  

### The aperiodic logistic map in reverse is unstable

What does it mean for a dynamical equation to be non-invertible?  It means that, given a point in a trajectory, we cannot determine what its previous point was with certainty.  In the case of the reverse logistic equation \eqref{eq2}, one point $x_n$ could have two possible previous points $x_{n-1}, x_{n-1}'$ and each of these could have two possible previous points $x_{n-2}, x_{n-2}', x_{n-2}'', x_{n-2}'''$ and so on (note that some points have only one previous point, because either $x_{n-1}, x_{n-1}' \not \in (0, 1)$).  

![reverse tree]({{https://blbadger.github.io}}misc_images/logistic_inverted.png)

Suppose one wanted to calculate the all the possible values that could have preceded $x_n$ after a certain number of steps.  The set `s` of points a point could have come from after a given number of `steps` may be found using recursion on \eqref{eq2} as shown below:

```python
def reverse_logistic_map(r, array, steps, s):
	# returns a set s of all possible values of
	# the reverse logistic starting from array[0]
	# after a given number of steps
	if steps == 0:
		for i in array:
			s.add(i)
		return 

	array_2 = []
	for y in array:
		y_next = (r + (r**2 - 4*r*y)**0.5)/ (2*r)
		if not np.iscomplex(y_next):
			if 0 < y_next < 1:
				array_2.append(y_next)

		y_next = (r - (r**2 - 4*r*y)**0.5)/ (2*r)
		if not np.iscomplex(y_next):
			if 0 < y_next < 1:
				array_2.append(y_next)
	
	reverse_logistic_map(r, array_2, steps-1, s)

```

A set is used as our return type because it is immutable.  A tupe or even a mutable element like a list (array) would also work well as python functions usually act as pass-by-reference, but for certain object oriented functions mutable datatypes such as lists would not suffice. Note that the computed previous value `y_next` is only added to `array_2` if it is not complex.

As the above program uses tail recursion, it can be converted into one that returns from the bottom of the recursion stack by returning `reverse_logistic` rather than calling it.  For some variety, here this is in C++

```cpp
// C++
#include <iostream>
#include <vector>
#include <cmath>
#include <iomanip>
using namespace std;

vector<double> reverse_logistic(vector<double> values, double r, int steps){
	if (steps == 0){
		return values;
	}
	vector<double> new_values {};
	for (int i=0; i < values.size(); i++){
		double current_value = values[i];
		
		if ((r*r-4*r*current_value) >= 0){
			double numerator = r + sqrt(r*r - 4*r*current_value);
			double next_value = numerator / (2*r);
			if(0 < next_value and 1 > next_value){
				new_values.push_back(next_value);
				}
			
			numerator = r - sqrt(r*r - 4*r*current_value);
			next_value = numerator / (2*r);
			if(0 < next_value and 1 > next_value){
				new_values.push_back(next_value);
				}
			}
			
		}
	return reverse_logistic(new_values, r, steps-1);
}


int main() {
	vector<double> values {0.5};
	double r = 3.2;
	int steps = 10;
	vector<double> val = reverse_logistic(values, r, steps);
	cout << val.size() << endl;
	for (int i = 0; i < val.size(); i++){
		cout << std::setprecision (17) << val[i] << ',' << ' ';
	}
	return 0;
}
```

The initial point $x_n$ is the first (and before the function is called, the only) entry in `array`.  For example, here is the reverse logistic map function for a starting point at $x_n=0.5$ with $r = 3.999$ (back in python)

```python
r = 3.999
ls = [0.5]
s = set()
steps = 1
reverse_logistic_map(r, ls, steps, s)

print (s)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
{0.8535091826042803, 0.1464908173957197}
```

Thus there are two values that, if applied to the logistic map as $x_{n-1}$, give our original value $x_n$.  

```python
def logistic_map(r, start, steps):
	for i in range(steps):
		start = r*start*(1-start)
	return start
	
result_ls = [i for i in s]
for j in range(len(result_ls)):
	forward_value = logistic_map(3.999, result_ls[j], 1)
	result_ls[j] = forward_value

print (result_ls)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[0.4999999999999999, 0.5]
```

or at least an extremely good approximation of $x_n = 0.5$.  For two steps, the reverse logistic map yields four values of $x_{n-2}$

```python
{0.691231134195249, 0.30876886580475105, 0.03808210936845992, 0.96191789063154}
```
which when applied to the logistic map (forward) equation twice, yield

```python
[0.4999999999999996, 0.4999999999999999, 0.5000000000000003, 0.5000000000000013]
```

The list of possible values grows exponentially, as long as iterations in reverse stay in (0, 1).  As an example of when they do not, $x=0.00001$ for two reverse steps yields two possible $x_{n-2}$ points

```python
{0.9999993746854281, 6.253145719266672e-07}
```
rather than four.

Aside:  At first glance it may seem that having many (a countably infinite number, to be precise) values that eventually meet to make the same trajectory suggests that the logistic map is not sensitive to initial conditions.  This is not so because it requires an infinite number of iterations for these values to reach the same trajectory.  Remembering that aperiodic is another way of saying 'periodic with period $\infty$', this is to be expected.

Recall the four values of $x_{n-2}$, which give four estimates of $x_n$, respectively:

```python
{0.691231134195249, 0.30876886580475105, 0.03808210936845992, 0.96191789063154}
[0.4999999999999996, 0.4999999999999999, 0.5000000000000003, 0.5000000000000013]
```

Notice that all values of $x_{n-2}$ are not equally accurate starting points for the forward logistic map: $x_{n-2} = 0.961...$ is worse than $x_{n-2} = 0.308...$ in that it yields a more inaccurate $x_n$ value.  We can define approximation error for $x$ given the estimate $e_{est}$ as

$$
e = \lvert x - x_{est} \rvert
$$

Now setting $r=3.6$ and the step number to 30,

```python
ls = [0.5]
s = set()
steps = 30
r = 3.6
reverse_logistic_map(r, ls, steps, s)

result_ls = [i for i in s]
error_ls = []
original_ls = []

for i in range(len(result_ls)):
	error_ls.append(abs(logistic_map(r, result_ls[i], steps)-0.5))

error_ls.sort()
print ('smallest_error:', error_ls[:1])
print (' ')
print ('largest error:', error_ls[-1:])
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
smallest_error: [3.885780586188048e-16]
 
largest error: [0.07201364157312073]
[Finished in 0.7s]
```

The largest error is $10^{14}$ times larger than the smallest error, meaning that some values of $x_{n-30}$ computed with \eqref{eq2} to 64 bit precision are able to yield near-arbitrary accuracy when reversed with \eqref{eq1} to give $x_n$, whereas others are quite inaccurate.  And this is for a mere 30 iterations!

How is it possible that there is such a large difference in estimation accuracy using values given by \eqref{eq2} after so small a number of computations?  First one can limit the number of values added to the array at each step, in order to calculate \eqref{eq2} for many more steps without experiencing a memory overflow from the exponentially increasing number of array values. The following gives at most 100 values per step:

```python
def reverse_logistic_map(r, array, steps, s):
	...
			if 0 < y_next < 1:
				array_2.append(y_next)
	
	reverse_logistic_map(r, array_2[:100], steps-1, s)
```

Now we can iterate \eqref{eq2} for many more iterations than before without taking up more memory than a computer generally has available. But first, what happens when we look at a simpler case, when r=2 and the trajectory of \eqref{eq1} settles on period 1?  Setting $x_n = x_{n+1}$, 

$$
x_n = rx_n(1-x_n) \\
0 = x_n(-rx_n+r-1) \\
x_n = 1-\frac{1}{r}
$$

given $r=2$, there is a root at $x_n = 0$ and another at $x_n = 1/2$, meaning that if $x_n$ is equal to either value then it will stay at that value for all future iterations.  In \eqref{eq1}, the value $x_n = 1/2$ is an attractor for all values $x_n \in (0, 1)$.  For starting values greater than 1/2, \eqref{eq2} finds only complex values because $r^2 - 4rx_n$ becomes negative.  Which complex values can be found by modifying `reverse_logistic_map` as follows

```python
def reverse_logistic_map(r, array, steps, s):
	...
	for y in array:
		y_next = (r + (r**2 - 4*r*y)**0.5)/ (2*r)
		if 0 < y_next.real < 1:
			array_2.append(y_next)
	for y in array:
		y_next = (r - (r**2 - 4*r*y)**0.5)/ (2*r)
		if 0 < y_next.real < 1:
			array_2.append(y_next)

```

Why does \eqref{eq2} fail to find real previous values for an initial value $1/2 < x_n < 1$ if the initial value is attracted to $1/2$?  Iterating \eqref{eq1} with any starting point $1/2 < x_n < 1$ gives an indication as to why this is:

```python
[0.85, 0.255, 0.37995, 0.471175995, 0.4983383534715199, 0.49999447786162876, 0.49999999993901195, 0.5, 0.5, 0.5, 0.5]
```
$1/2 < x_n < 1$ can only be the first iteration of any trajectory of \eqref{eq1} limited to real numbers because $r^2 - 4rx_n$ is negative, and therefore there is no previous value $x_{n-1}$ in \eqref{eq1} if $1/2 < x_n < 1$. 

Thus not every $x_n \in (0, 1)$ has a next iteration of \eqref{eq2} (restricted to the reals), and therefore does not have an $x_{n-1}$ in \eqref{eq1}.  To avoid this problem, we can start iterating \eqref{eq2} with a value that exists along a trajectory of \eqref{eq1}, which can be implemented as

```python
r = 2
starting_point = 0.5
trajectory = []
for i in range(50):
	trajectory.append(logistic_map(r, starting_point, i))
# trajectory[-1] is our desired value
```

With `trajectory[-1]` as the starting value and a maximum of 100 values per iteration of \eqref{eq2}, we can calculate more iterations to try to understand why substantial error in calculating $x_n$ with \eqref{eq1} occurs after calculating $x_{n-p}$ with \eqref{eq2}.  With 100 steps of \eqref{eq2} then \eqref{eq1} at r = 2.95 (period 1) to recalculate $x_n \approx 0.661$ there is a large difference in the approximation error depending on which $x_{n-100}$ was used.  

```python
smallest_error: [0.0]
largest error: [0.002287179106238879]
```

To see why this is, the values of $x_{n-100}$ may be viewed:

```python
r = 2.8
starting_point = 0.5
trajectory = []
for i in range(100):
	trajectory.append(logistic_map(r, starting_point, i))

ls = [trajectory[-1]]
s = set()
steps = 100
reverse_logistic_map(r, ls, steps, s)

result_ls = [i for i in s]
result_ls.sort()
print (result_ls)
```
which gives 

```python
Result list [7.526935760170552e-17, 1.5053871520341105e-16, 4.516161456102331e-16, 1.2043097216272884e-15, 3.2365823768733374e-15, 1.0537710064238773e-14, 2.7849662312631042e-14, 9.205442434688585e-14, 2.410877523982628e-13, 8.05909011841461e-13, 2.0833805490576073e-12, 7.0663625610057165e-12, 1.799073131524445e-11, 6.20285743672743e-11, 1.5520278094720074e-10, 5.45169935560545e-10, 1.3372589664608297e-09, 4.798801055209755e-09, 1.1504929814858098e-08, 4.232121192100105e-08, 9.879781173962447e-08, 3.7414690848327905e-07, 8.464077734738537e-07, 3.318475220716627e-06, 7.228333600448949e-06, 2.956762489958464e-05, 6.145528865947984e-05, 0.00026525167524554897, 0.0005188860973583614, 0.0024055057511865553, 0.004323711646638956, 0.022166149207306658, 0.03467009226705414, 0.21625977507447144, 0.21625991839052272, 0.7837400816094773, 0.7837402249255285, 0.9653299077329459, 0.9778338507926934, 0.9956762883533611, 0.9975944942488134, 0.9994811139026417, 0.9997347483247544, 0.9999385447113406, 0.9999704323751004, 0.9999927716663994, 0.9999966815247793, 0.9999991535922266, 0.9999996258530914, 0.9999999012021882, 0.9999999576787881, 0.9999999884950702, 0.999999995201199, 0.9999999986627411, 0.9999999994548301, 0.9999999998447973, 0.9999999999379714, 0.9999999999820093, 0.9999999999929338, 0.9999999999979166, 0.999999999999194, 0.9999999999997589, 0.9999999999999079, 0.9999999999999721, 0.9999999999999895, 0.9999999999999967, 0.9999999999999988, 0.9999999999999996, 0.9999999999999999]
```
We can see that there are values very close to 0, and values very close to 1.  Now all $x \in (0, 1)$ converge on the period one attractor with \eqref{eq1}, meaning that with enough iterations in the forward direction all these values will end up arbitrarily close to $0.661...$.  But we can expect for values closer to 0 or 1 to require more iterations to do so compared to values farther from 0 or 1 simply by observing trajectories of \eqref{eq1} starting at very small initial points (note that initial points near 1 become very small after one iteration of \eqref{eq1}).

```python
[7.656710514656253e-17, 2.2204460492503128e-16, 6.439293542825906e-16, 1.8673951274195114e-15, 5.415445869516573e-15, 1.5704793021597974e-14, 4.554389976263341e-14, 1.3207730931163088e-13, 3.8302419700367896e-13, 1.1107701713102434e-12, 3.2212334967961275e-12, 9.341577140678679e-12, 2.70905737077151e-11, 7.856266375024548e-11, 2.278317248578128e-10, 6.60712001937126e-10, 1.916064804351698e-09...]
```

A guess, therefore, as to why certain $x_{n-100}$ yield bad estimates of $x_n$ after iterating \eqref{eq1} is that some very small (or close to 1) $x_{n-100}$ converge too slowly.  Removing small and large values of $x_{n-100}$ with`result_ls = result_ls[20:50]` gives

```python
smallest_error: [0.0]
largest error: [2.5417445925768334e-12]
```
The largest error is now reasonably small.  In general for r values giving period 1 attractors in \eqref{eq1}, back-calculating $x_n$ using only moderate values of $x_{n-p}$ gives accurate approximations by avoiding slow convergence.

Is removing small and large values capable of preventing larger error for r values giving period 2 attractors in \eqref{eq1}?  It is not: at r=3.3, both minimum and maximum errors are far larger even using the same restriction as above.

```python
smallest_error: [6.032329413763193e-09]
largest error: [0.34417022723336355]
```

If the calculated values of $x_n$ are observed (without restriction on $x_{n-100}$ size), 

```python
[0.4794047645286176, 0.4794270197446111, 0.4794270198235495, 0.4794270198237357, 0.4794270198241478, 0.47942701982417835, 0.4794270198242073, 0.4794270198242338, 0.4794270198242338, 0.47942701982423414, 0.47942701982423414, 0.4794270198242346, 0.4794270198242346, 0.4794270198242469, 0.4794270198258387, 0.47942701985138186, 0.4794270201094164, 0.47942706429511417, 0.47943305597270514, 0.47949483653227437, 0.6735095413758753, 0.8218213953164031, 0.8226215546105751, 0.8231044602006431, 0.8233353601396424, 0.823447283941356, 0.8234609986401885, 0.8235801688346698, 0.8235801688346698, 0.8235854109768287, 0.8235872120774619, 0.8235953650317183, 0.8235957506424327, 0.8235957506424327, 0.8235979860054629, 0.8235999110552038, 0.8236004238673749, 0.8236005654074291, 0.8236029304926894, 0.8236029304926894, 0.8236030164819234, 0.8236030164819234, 0.8236032496606793, 0.8236032814078792, 0.8236032832060614, 0.8236032832060685, 0.8236032832060689, 0.8236032832060689, 0.823603283206069, 0.823603283206069, 0.823603283206069, 0.823603283206095, 0.8236032883637385, 0.8236032892383981, 0.8236033564494317, 0.823613690976619, 0.8236956717788287, 0.8237174065220663, 0.8237894778288714, 0.8239864668392716, 0.8247958463410507, 0.824834242709513, 0.8249727017477269]
```
there are apparently two attractors: one at $x = 0.479...$ and another at $x = 0.823...$, the latter being our initial value $x_n$.  

This is seen for any r giving a periodic attractor greater than 1: for r=3.5 (period 4), 

```python
[0.38281260360403013, 0.3828139123252521, 0.38281968301557207, 0.38281968301732416, 0.38281968301732416, 0.38281968301732416, 0.38281968301732416, 0.38281968301732416, 0.38281968301732416, 0.38281968301732416, 0.38281968301732416, 0.38281968301732416, 0.38281968301732416, 0.38281968301732416, 0.5008842103072163, 0.5008842103072179, 0.5008842103072179, 0.5008842103072179, 0.5008842103072179, 0.5008842103128662, 0.5008842103181947, 0.5008842178162276, 0.5008842178162276, 0.8269407065902027, 0.8269407065914385, 0.8269407065914387, 0.8269407065914387, 0.8269407065914387, 0.8269407065914387, 0.8269407067389265, 0.8269407551975788, 0.8269407976758019, 0.8269408464554105, 0.8269408869648845, 0.8269408876533455, 0.8270219098976035, 0.8322724885292522, 0.8640473538477498, 0.8707577866023022, 0.8749955169069416, 0.8749971961401654, 0.8749971961401654, 0.8749971961401654, 0.8749971961401654, 0.8749972445225117, 0.8749972636003276, 0.8749972636024504, 0.8749972636024641, 0.8749972636024641, 0.8749972636024641, 0.8749972636024641, 0.8749972636024641, 0.8749972636024641, 0.8749972636024641, 0.8749972636024641, 0.8749972636024662, 0.8749972648925344, 0.8749972648925344, 0.8749972653362731]
```
where the four periodic values $0.38..., 0.50..., 0.82..., 0.87...$ are all obtained. This means that error in recalculating $x_n$ for periodic attractors can be attributed to 'iteration error', defined as follows: error in the specific iteration of a periodic trajectory, rather than error finding the trajectory. 

Do $r$ values yielding aperiodic iterations of \eqref{eq1} give worse estimates than $r$ values for periodic iterations of \eqref{eq1}? To get an idea of how this could be, let's look at what happens to the average error as $r=2.5 \to r = 4$.  The average error to any of the last four iterations may be found as follows:

```python
Y = [] # list of average error per R value
R = [] # list of R values

for i in range(1500):
	r = 2.5 + i/1000
	starting_point = 0.5
	trajectory = [starting_point]
	for i in range(100):
		start = trajectory[-1]
		trajectory.append(r*start*(1-start))
	ls = [trajectory[-1]]
	s = set()
	steps = 100
	reverse_logistic_map(r, ls, steps, s)

	result_ls = [i for i in s]
	error_ls = []
	original_ls = []

	for i in range(len(result_ls)):
		error_ls.append(abs(logistic_map(r, result_ls[i], steps)-trajectory[-1]))
		original_ls.append(logistic_map(r, result_ls[i], steps))

	R.append(r)
	if len(error_ls) > 0:
		Y.append(sum([i for i in error_ls])/ len(error_ls))
	else:
		# if there are no previous values
		Y.append(-0.1) # some value that can be discarded

fig, ax = plt.subplots()
ax.plot(R, Y, '^', color='white', markersize=0.5)
ax.set(xlabel='r value', ylabel='Average error')
plt.show()
plt.close()
```
which results in the following plot.

![error]({{https://blbadger.github.io}}misc_images/logistic_reverse_1.png)

If this is extended to the minimum error of any of the four last values of the `trajectory` from 0.5 in order to account for iteration error up to period 4, we instead have

![error]({{https://blbadger.github.io}}misc_images/logistic_reverse_4.png)

There is an increase in average error as the map becomes aperiodic, at around r = 3.58.  This is to be expected for any value of r that yields a periodicity larger than 4, because iterations of either \eqref{eq1} or \eqref{eq2} are attracted to periodic orbits and the above program only takes into account accuracy up to four previous values (near four periodic values).  As aperiodic trajectories have infinite period, the accuracy necessarily suffers.

To summarize, if the reverse logistic map is used to calculate values that result in accurate recapitulations of the intial value using the forward logistic map, error due to finding the right periodic trajectory but being on the 'wrong' iteration occurs because all periodic points are attractors.  For r values giving an aperiodic trajectory of the forward logistic map, this error cannot be prevented because periodicity is infinite.

### Some approximations of $x_{n-p}$ with \eqref{eq2} are necessarily better than others

The previous section saw some extimates of a previous value in the trajectory of the logistic map to yield more accurate vales of $x_n$ after p iterations of \eqref{eq1}. Is this necessarily the case, or in other words is there some way to compute the reverse logistic map such that all previous estimates are equivalently good?

Here is \eqref{eq2} restated for clarity:

$$
x_{n+1} = \frac{r \pm \sqrt{r^2-4rx_n}}{2r}
$$

Now consider what many iterations of \eqref{eq2} entail, specifically that many square roots of $r^2-4rx_n$ are taken.  For example, 

$$
x_{n+2} = \frac{r \pm \sqrt{r^2-4r(\frac{r \pm \sqrt{r^2-4rx_n}}{2r})}}{2r}
$$

Now some square roots are better approximated by rational numbers than others: most famously, the golden ratio $\phi = \frac{1 + \sqrt 5}{2}$ which is the positive root of $x^2-x-1$ is the irrational number that is 'furthest' from any rational in the sense that it is worst approximated by any rational.  This can be shown with continued fractions:

$$
\phi = 1 + \cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cdots}}}
$$

Now consider what makes a poorly approximated number: if the denominator contains a large first integer before addition, then everything that follows is less important than if it were smaller.  For example,  see that $5 + \cfrac{1}{5 + \cfrac{1}{5}} \approx 5.19$ is better approximated by $5 + \cfrac{1}{5} = 5.2$ than $2 + \cfrac{1}{2 + \cfrac{1}{2}} = 2.4$ is by  $2 + \cfrac{1}{2} = 2.5$ .  With $1$ being the smallest non-zero natural number (and dividing by zero is unhelpful here) means that this number is most poorly approximated by ending this continued fraction at any finite stage.  Approximation of the golden ratio is discussed in this excellent [video](https://www.youtube.com/watch?v=sj8Sg8qnjOg) from Numberphile.

Thus certain square root values are better approximated by rational numbers than others. Therefore, iterating \eqref{eq2} will yield numbers approximated better or worse depending on the value inside each radical, which in turn depends on whether addition or subtraction occurs at each iteration.

Now if the r value for \eqref{eq1} yields a periodic trajectory, \eqref{eq2} will be attracted to the same periodic trajectory and thus it does not matter whether addition or subtraction is chosen because either reaches the same value.  But for aperiodic trajectories values never repeat, meaning that each choice for addition or subtraction in \eqref{eq2} yields unique values.  Because  different radicals are approximated by rational with differing success, and as computable procedures must use rational (finite) approximations, \eqref{eq2} must yield numbers that, when iterated with \eqref{eq1} to recover the original $x_0$, are of varying accuracy.

###  Aperiodic logistic maps are not practically reversible

The logistic map is not because it is 2-to-1 for many values of $x_n \in (0, 1)$: there is no way to know which of the two points $x_{n-1}, x_{n-1}'$ the trajectory actually visited.  For values of r that yield an aperiodic logistic map trajectory, only one of the two $x_{n-1}, x_{n-1}'$ points may be visited because aperiodic trajectories never revisit previous points, and if either point was visited then $x_n$ is also visited.  Therefore aperiodic logistic map trajectories follow only one of the many possible previous trajectories, and which one is impossible to determine.

But what about reversibility with respect to future values $(x_{n+1}, x_{n+2} ...)$?  In other words, given that $x_{n-1}, x_{n-1}'$ both map to the same trajectory $(x_{n+1}, x_{n+2} ...)$, can we find any previous values of $x_n$ that yield the same fucutre trajectory?  This criteria for reversibility is similar to the criteria for one-way functions, which stipulates that either $x_{n-1}, x_{n-1}'$ may be chosen as a suitable previous value and that there is no difference between the two with respect to where they end up.  Is the logistic map reversible under this definition, or equivalently is the logistic map a one way function?

Symbolically, it is not: \eqref{eq2} may be applied any number of times to find the set of all possible values for any previous iteration.  But if this is so, why was it so difficult to use this procedure to find accurate sets of previous values, as attempted in the last section? 

Consider what happens when one attempts to compute successive iterations of \eqref{eq2}: first one square root is taken, then another and another etc.  Now for nearly every initial value $x_n$, these square roots are not of perfect squares and therefore yield irrational numbers.  Irrational numbers cannot be represented with complete accuracy in any computation procedure with finite memory, which certainly applies to any computation procedure known.  The definition for 'practical' computation is precisely this: computations that can be accomplished with finite memory in a finite number of steps.  The former stipulation is not found in the classic notion of a Turing machine, but is most accurate to what a non-analogue computer is capable of.

This would not matter much if approximations stayed accurate after many iterations of \eqref{eq2}, but one can show this to be untrue for values of r and $x_0$ such that \eqref{eq1} is aperiodic.  First note that \eqref{eq2} forms a tree, with one $x_n$ leading to one or two $x_{n_1}$.  If \eqref{eq1} is aperiodic for all starting $x_n$, then all paths through \eqref{eq2} are aperiodic because previous values are never revisited in the forward direction (with \eqref{eq1}), and therefore are never revisited in the reverse.  Therefore all paths through \eqref{eq2} are sensitive to initial values (becase they are aperiodic) and it follows that any approximation of $x_n$ yields is arbitrarily inaccurate (within limites) for $x_{n+p}$ given a large enough $p$.  

As iterations of \eqref{eq2} are (almost all) irrational, it follows that the reverse logistic map is impossible to compute practically (assuming aperiodicity) because finite representations of irrational numbers yield inaccurate estimates of future iterations.  

### The Henon map is invertible 

The [Henon map](https://blbadger.github.io/henon-map.html) is a two dimensional discrete dynamical system defined as

$$
x_{n+1} = 1 - ax_n^2 + y_n \\
y_{n+1} = bx_n \\
\tag{3} \label{eq3}
$$

Let's try the same method used above to reverse $x_{n+1}$ of \eqref{eq2} with respect to time, 

$$
y_{n} = bx_{n+1} \\
x_{n+1} = \frac{y_n}{b}
$$

using this, we can invert $y_{n+1}$ as follows:

$$
y_{n+1} = ax_{n+1}^2 + x_n - 1 \\
y_{n+1} = a \left(\frac{y_n}{b}\right)^2 + x_n - 1
$$

Therefore the inverse of the Henon map is:

$$
x_{n+1} = \frac{y_n}{b} \\
y_{n+1} = a \left( \frac{y_n}{b} \right)^2 + x_n - 1
\tag{4} \label{eq4}
$$

This is a one-to-one map, meaning that \eqref{eq3} is invertible.  

Does this mean that, given some point $(x, y)$ in the plane we can determine the path it took to get there?  Let's try this with the inverse function above for $a = 1.4, b=0.3$

```python
#! python3
def reversed_henon_map(x, y, a, b):
	x_next = y / b
	y_next = a * (y / b)**2 + x - 1
	return x_next, y_next
	
array = [[0, 0]]
a, b = 1.4, 0.3
for i in range(4):
	array.append(reversed_henon_map(array[-1][0], array[-1][1], a, b))

print (array)
```
Starting at the origin, future iterations diverge extremely quickly:

```python
[[0, 0], (0.0, -1.0), (-3.3333333333333335, 14.555555555555557), (48.518518518518526, 3291.331961591222), (10971.106538637407, 168511297.67350394)]
```

### The reverse Henon map is unstable

We know that a starting value on the Henon attractor itself should stay bounded if \eqref{eq1} is iterated in reverse, at least for the number of iterations it has been located near the attractor.  For example, the following yields a point that has existed near the attractor for 1000 iterations

```python
def henon_map(x, y, a, b):
	x_next = 1 - a*x**2 + y
	y_next = b*x
	return x_next, y_next

x, y = 0, 0
a, b = 1.4, 0.3

for i in range(1000):
	x_next, y_next = henon_map(x, y, a, b)
	x, y = x_next, y_next

# x, y is the coordinate of a point existing near H for 1000 iterations
```

One might not expect this point to diverge until after the 1000 iterations have been reversed, but this is not the case: plugging `x, y` into the initial point for the reverse equation as follows

```python
array = [[x, y]]
for i in range(30):
	array.append(reversed_henon_map(array[-1][0], array[-1][1], a, b))

print (array)
```
results in
```python
[[0.5688254014690528, -0.08255572592986403], (-0.2751857530995468, -0.3251565203383966), (-1.0838550677946555, 0.36945277807827326), (1.231509260260911, 0.03940601355707107), (0.13135337852357026, 0.25566445433028995), (0.8522148477676332, 0.14813158398142479), (0.4937719466047493, 0.19354987712301397), (0.6451662570767133, 0.07650724558327537), (0.25502415194425126, -0.2637814976184484), (-0.8792716587281613, 0.3373902617238522), (1.1246342057461742, -0.10854872330010212), (-0.3618290776670071, 0.3079225997696742), (1.026408665898914, 0.11309157153833649), (0.37697190512778833, 0.225359610056858), (0.7511987001895267, 0.16699118716079653), (0.5566372905359884, 0.1849818026908716), (0.6166060089695721, 0.08892144895232601), (0.29640482984108674, -0.260395838616055), (-0.8679861287201833, 0.3511647173519976), (1.1705490578399922, 0.05027300681394742), (0.16757668937982473, 0.20986378339289535), (0.6995459446429846, -0.14731297048715142), (-0.4910432349571714, 0.03711878667906987), (0.12372928893023291, -1.469610723242318), (-4.898702410807727, 32.71992872244504), (109.06642907481681, 16647.781629174056), (55492.60543058019, 4311201068.530109), (14370670228.433699, 2.8912262794014697e+20), (9.637420931338232e+20, 1.3003183509091478e+42), (4.334394503030493e+42, 2.6301765991061335e+85), (8.767255330353778e+85, 1.0761067243866343e+172)]
```

which demonstrates divergence in around 22 iterations, far fewer than the 1000 iterations that were performed by the Henon map in the forward direction to reach the initial point.  

Could this be due to propegation of error? Supposing a sufficiently small error $\varepsilon$ were added to both $y_n, x_n$, after one iteration of the reverse Henon map we have:

$$
x_{(n+1)'} = (y_n + \varepsilon)/b \\
y_{(n+1)'} = \frac{a}{b^2}(y_n^2 + 2y_n\varepsilon + \varepsilon^2) + x_n + \varepsilon - 1 \\
$$

Now for the attractor that forms given $a = 1.4, b = 0.3$, we know that $\lvert y_n \rvert < 1/2$.  Using this information, the distance between $x_{n+1}, y_{n+1}$ and $x_{(n+1)'}, y_{(n+1)'}$ can be calculated to see how much larger the error gets after each iteration of the inverse Henon map.  We will use Manhattan distance (which is $\lvert x_1 - x_2 \rvert + \lvert y_1 - y_2 \rvert$) as our metric for simplicity.

This distance can be calculated, assuming $\varepsilon < 1$ such that $\varepsilon^2 < \varepsilon$, as follows:

$$
\lvert x_{n+1} - x_{(n+1)'} \rvert + \lvert y_{n+1} - y_{(n+1)'} \rvert \\
= \frac{\varepsilon}{b} + \frac{a}{b^2}(2y_n\varepsilon + \varepsilon^2) + \varepsilon \\
< \frac{10}{3}\varepsilon + \frac{2\varepsilon}{10} + \varepsilon \\
< 10\varepsilon 
$$

Therefore the Manhattan distance between the next point with compared to the point without error is less than 10-fold the initial error size for each iteration.  Therefore a 100-fold increase in error requires more than 2 iterations.  Now comparing back to our points with ~16 decimal places, we can say that the error will remain relatively small as long as the iteration number is under 16, but after this iteration the initial error introduced by rounding starts to become large relative to the point's values.  

All this is to say that the divergence above occurred after 22 iterations because there were only ~16 decimal points of accuracy. What happens when this accuracy is increased?  Arbitrary precision (within memory and time limits, that is) math can be performed using the `decimal` library.  To find the number of iterations (in the inverse Henon map) until divergence (given 2000 forward iterations) at various decimal accuracies,

```python
import numpy as np 
from matplotlib import pyplot as plt
import copy
from decimal import *

# applies to an external x, y value or array 
def reversed_henon_map(x, y, a, b):
	x_next = y / b
	y_next = a * (y / b)**Decimal(2) + x - Decimal(1)
	return x_next, y_next

# the forward Henon map for accuracy checking
def henon_map(x, y, a, b):
	x_next = Decimal(1) - a*x**Decimal(2)+ y
	y_next = b*x
	return x_next, y_next
	
x_array, y_array = [], []

for i in range(2, 1000):
	getcontext().prec = i
	x, y = Decimal(1), Decimal(1)
	a, b = Decimal(14)/Decimal(10), Decimal(3)/Decimal(10)

	for j in range(2000):
		x_next, y_next = henon_map(x, y, a, b)
		x, y = x_next, y_next
	x_array.append(i)

	count = 0
	while True:
		x, y = reversed_henon_map(x, y, a, b)
		if x > 100 or y > 100:
			break
		count += 1
	y_array.append(count)
```

yields

![divergence vs precision]({{https://blbadger.github.io}}misc_images/divergence_vs_precision.png)

From this we can see that there is a linear relation between how many iterations it takes to diverge in the reverse direction and the precision used.  This is because with more precision, we can better approximate values of points on the true Henon map (which are irrational for $a, b$ values that yield an aperiodic attractor, see below for more).  

What happens to the number of reverse iterations until divergence if the precision is held constant (at 1000 decimal points) but the number of forward iterations changes? 

![forward vs reverse divergence]({{https://blbadger.github.io}}misc_images/forward_vs_reverse_iterations.png)

Which is what we would expect, as each forward iteration (within the limits prescribed by the accuracy of calculation) brings the point closer to $\mathscr H$ at the same rate as a reverse iteration moves the point away from $\mathscr H$. 

### Stable and unstable points of the inverted Henon map

Is the divergence specific to the point we chose above, or do all points near the Henon attractor eventually diverge in a similar manner?  This can be investigated as follows:

```python
def reverse_henon_stability(max_iterations, a, b, x_range, y_range):
	xl, xr = -2.8, 2.8
	yl, yr = 0.8, -0.8

	x_list = np.arange(xl, xr, (xr - xl)/x_range)
	y_list = np.arange(yl, yr, -(yl - yr)/y_range)
	array = np.meshgrid(x_list[:x_range], y_list[:y_range])

	x2 = np.zeros(x_range)
	y2 = np.zeros(y_range)
	iterations_until_divergence = np.meshgrid(x2, y2)

	for i in iterations_until_divergence:
		for j in i:
			j += max_iterations

	not_already_diverged = np.ones(np.shape(iterations_until_divergence))
	not_already_diverged = not_already_diverged[0] < 1000

	for k in range(max_iterations):
		x_array_copied = copy.deepcopy(array[0]) # copy array to prevent premature modification of x array

		# henon map applied to array 
		array[0] = array[1] / b
		array[1] = a * (array[1] / b)**2 + x_array_copied - 1

		r = (array[0]**2 + array[1]**2)**0.5
		diverging = (r > 10000) & not_already_diverged # arbitrarily large number here
		not_already_diverged = np.invert(diverging) & not_already_diverged
		iterations_until_divergence[0][diverging] = k

	return iterations_until_divergence[0]
```

which results (lighter color indicates more iterations occur before divergence)

![divergence]({{https://blbadger.github.io}}misc_images/henon_reversed_scale.png)

The baker's dough topology found by Smale is evident in this image, meaning that each iteration of the forward Henon map can be decomposed into a series of three stretching or folding events as shown [here](https://en.wikipedia.org/wiki/H%C3%A9non_map).  This topology is common for attractors that map a 2D surface to an attractor of 1 < D < 2: the Henon map for a=1.4, b=0.3 is around 1.25 dimensional. 

The attractor for \eqref{eq3} can be mapped on top of the divergence map for \eqref{eq4} as follows:

```python
steps = 100000
X = [0 for i in range(steps)]
Y = [0 for i in range(steps)]

X[0], Y[0] = 0, 0 # initial point

for i in range(steps-1):
	if abs(X[i] + Y[i])**2 < 1000:
		X[i+1] = henon_map(X[i], Y[i], a, b)[0]
		Y[i+1] = henon_map(X[i], Y[i], a, b)[1]

plt.plot(X, Y, ',', color='white', alpha = 0.5, markersize = 0.1)
plt.imshow(reverse_henon_stability(40, a, b, x_range=2000, y_range=1558), extent=[-2.8, 2.8, -0.8, 0.8], aspect= 2.2, cmap='inferno', alpha=1)
plt.axis('off')
plt.savefig('henon_reversed{0:03d}.png'.format(t), dpi=420, bbox_inches='tight', pad_inches=0)
plt.close()
```
which gives

![divergence]({{https://blbadger.github.io}}misc_images/henon_reversed_overlay.png)

where, as expected, the areas of slower divergence align with the attractor (in white). 

From a = 1 to a = 1.5, holding b=0.3 constant,

{% include youtube.html id='gb18hw3ndpU' %}

It is interesting to note that the divergence map for the forward Henon map \eqref{eq3} is not simply the inverse of the divergence map for the reverse Henon map \eqref{eq4}, which is presented [here](https://blbadger.github.io/henon-map.html), given that they are inverse functions of each other.  In particular, regions outside the attractor basin for \eqref{eq3} diverge, meaning that a trajectory starting at say (10, 10) heads to infinity.  But this region also diverges for \eqref{eq4}, which is somewhat counter-intuitive given that \eqref{eq4} yields the iterations of \eqref{eq3} in reverse.

For a=0.2, -1 < b < 0, \eqref{eq3} experiences a point attractor for initial values in the attractor basin: successive iterations spiral in towards the point

$$
x_n = \frac{(b-1) + \sqrt{(b-1)^2 + 4a}}{2a} \\
y_n = bx_n
$$

outside of which values diverge. For b <= -1, the attractor basin collapses, and nearly all starting points lead to trajectories that spiral out to infinity. Now looking at stable versus unstable values for \eqref{eq4} with b = -0.99, 

![divergence]({{https://blbadger.github.io}}misc_images/henon_reversed030.png)

The area in the center does not diverge after 40 iterations.  Do initial points in this area ever diverge?  This question can be addressed by increasing the maximum iterations number.  Doing so from 2 maximum iterations to 1010, iterating \eqref{eq4} for a=0.2, b=-0.99 we have

{% include youtube.html id='zbcgAlZtRGo' %}

As is the case for a=1.4, b=0.3 so also for \eqref{eq3} with a=0.2, b=-0.99, there are unstable points and regions elsewhere diverge.  

The transition from point attractors to divergence everywere except a point (or two) for the reverse Henon map occurs in reverse to that observed for the forward Henon.  For example, a=0.2 and b=0.95 exhibits two point attractors for \eqref{eq3} but diverges everywhere except unstable points for \eqref{eq4}, whereas a=0.2, b=1.05 diverges everywhere except unstable points for \eqref{eq3} but converges on points for \eqref{eq4}.  In the video below, $a$ is held constant while $b$ changes as follows:

$$
a = 0.2 \\
b = 0.95 \to b = 1.01 
$$

Iterating \eqref{eq4}, note how the change in basin behavior is the opposite to that found for the same transition with \eqref{eq3}.

{% include youtube.html id='IEbtIjFz6Bo' %}

### Aperiodic Henon maps are reversible from some starting points

Is the Henon map reversible?  In the sense that we can define a composition of functions to determine where a previous point in a trajectory was located, the Henon map is reversible as it is 1-to-1 and an inverse function exists.  Reversing the Henon map entails computing \eqref{eq3} for however many reverse iterations are required. 

But earlier our attempts to reverse the Henon map were met with very limited success: values eventually diverged to infinity even if they were located very near the attractor for \eqref{eq3}.  Moreover, divergence occurred in fewer iterations that were taken in the original forward trajectory.  This begs the question: is the Henon map, like the logistic map, practically irreversible?

If it is aperiodic (as is the case for a=1.4, b=0.3) then yes, for the special case where the point of interest is an element of the set of points in the Henon attractor $\mathscr H$, in symbols $x_n \in \mathscr H$, or more generally where $x_n \not \in \Bbb Q$.  The Henon map, iterated discontinuously, cannot be defined on the rationals (for more information, see [here](https://blbadger.github.io/most-discontinuous.html)).  As real numbers are uncountably infinite but rationals are countable, all but a negligable portion of values of the Henon attractor $\mathscr H$ are irrational.  

Now irrational numbers are of infinite length, and cannot be stored to perfect accuracy in finite memory.  How do we know that rational approximations of irrational numbers eventually diverge after many iterations of \eqref{eq4}?  This is because of sensitivity to initial conditions, which implies and is implied by aperiodicity (see [here](https://blbadger.github.io/chaotic-sensitivity.html)).  The proof that \eqref{eq4} is sensitive to initial conditions is as follows: \eqref{eq4} is the one-to-one inverse of \eqref{eq3}.  Being aperiodic, the trajectory of \eqref{eq3} never revisits a previous point.  Therefore we know that \eqref{eq4} is aperiodic as well, as it never revisits a previous point being that it defines the same trajectory as \eqref{eq3}.  As aperiodicity implies arbitrary sensitivity to initial values, \eqref{eq4} is arbitrarily sensitive to initial values. 

Any two starting points $p_1$ and $p_2$, arbitrarily close together but not exactly in the same place, can be considered approximations of each other.  If they are close enough then they are accurate approximations.  Sensitivity to initial conditions stipulates that, given enough iterations of the inverse Henon map \eqref{eq4}, $p_1$ and $p_2$ will separate.  If we take $p_1$ to be an irrational number and $p_2$ to be its rational approximation (or vice versa), an arbitrarily accurate rational approximation will, given enough iterations of \eqref{eq4}, become inaccurate.  Therefore all but a negligable portion of values on an aperiodic Henon map itself are practically non-invertible.

One might expect for the errors introduced in approximating irrationals in one direction with respect to time to cancel out if the reverse function is used to back-compute in the other direction.  If this were true, then even though iterations of \eqref{eq4} do not give accurate previous values beyond a certain number of iterations, the function would still be reversible in the sense of a one-way function definition (see the discussion for the logistic map for more on this topic) because either the true value or its inaccurate approximation (which is computed) would yield the same point.  But even though tempting, the idea that perhaps errors should cancel each other out can easily be disproven for aperiodic 1-to-1 dynamical systems as follows: if a true value and an inaccurate approximation cannot both yield the same point in reverse because otherwise the system would be 2-to-1, a contradiction.

The last statement does not necessarily mean that \eqref{eq3} is not practically invertible for periodic trajectories, because any finite number of iterations of \eqref{eq3} could still be reversed with the same number of iterations of \eqref{eq4}. 

What about if we start with a rational number not on the Henon attractor itself, given such an $x_n$ can we find $x_{n-p}$?  Given sufficient memory the Henon map is reversible for such points, as \eqref{eq4} is closed for rationals.  This means that in contrast to what we saw for the logistic map, the Henon map starting on a rational $x_n$ is reversible.  But memory demands are exponential as iterations increase.

### Aperiodicity and reversibility

In conclusion, aperiodic Henon but not logistic maps are practically reversible for some starting value, meaning that one cannot compute arbitrary previous values accurately using finite memory.  

If future values for the logistic map are difficult to predict, as is the case for aperiodic systems in general, does it come as any surprise that past values are hard to determine as well?  It is indeed surprising, but why this is requires explanation.

First consider what it means for an aperiodic dynamical system to be hard to predict.  More precisely, it is often stated that the approximate present does not determine the approximate future, which is a consequence of sensitivity to initial values.  But it should be understood that the exact present does indeed determine the exact future: every time \eqref{eq1} is iterated 30 times with idential (rational) starting values for a fixed r, the result is the same (assuming identical floating point arithmetic).  Real observations are necessarily imperfect, but if a rational value is used as a starting point to iterate the logistic equation then all future iterations are computable, given enough memory.  

On the other hand, this page demonstrates a separate difficulty in attempting to calculate previous values from the logistic map.  Choosing a rational value of $x_n$ is no guarantee that any previous value will be rational, and sensitivity to initial values necessitates that small approximation errors become large over time.  This means that no computation procedure with finite memory will be able to accurately determine past values for an arbitrary number of iterations, regardless of starting value.  The logistic map, therefore, is deterministic and computable in the forward time direction (given the right starting values and enough memory) but not in the reverse time direction.  






## Periodic trajectories and rational numbers

Here we establish an equivalence between the set of irrational numbers and the set of all aperiodic function trajectories (restricted to functions of discrete time intervals in finite precision in a given dimension), and conversely an equivalence between the set of rationals and periodic functions.  Then it is shown that aperiodic trajectories cannot be defined on rationals but instead require uncountably many elements, ie the set of irrational numbers.  These findings allow us form a short proof of sensitivity to initial conditions that typifies aperiodic trajectories, and also explore a connection between aperiodic trajectories and unsolveable problems. 

### The set of periodic trajectories is equivalent to the set of rational numbers

Here periodic trajectories are defined as trajectories of discrete dynamical equations in finite dimension at a given precision that eventually re-visit previous points.  Difference equations (or differential equations) that repeat previous points are periodic because there is no change in behavior over time, meaning that the trajectory from any given point at time 0 is identical to the trajectory from the same point at any other time.

As there is no universally-agreed upon notation for the set of irrational numbers, here we take the set of irrational numbers to be the real numbers that are not rational, $ \Bbb I = \Bbb R - \Bbb Q$, or equivalently

$$ \Bbb I = \{ x \; \vert \; x \in \Bbb R \land x \notin \Bbb Q\} $$

Rational numbers are expressible as fractions, whereas irrationals are not, and all real numbers are either rational or irrational.  As $\Bbb R $ is uncountably infinite but $\Bbb Q$ is countably infinite, $\Bbb I$ is uncountably infinite, meaning that the cardinality of the set of irrationals is much larger than the cardinality of rationals, or in other words nearly every possible number real number is irrational.

$$
card \; \Bbb I  >>  card \; \Bbb Q
$$

Now let's consider functions of discrete time intervals (difference equations or approximations of differential equations), the trajectories of which may be periodic or aperiodic. Aperiodicity means that values of the function $a(x)$ never are identical to values of previous iterations:

$$
a^n(x_0) \neq a^k(x_0) \; \mathbf {if} \; x, k \in \Bbb N \; \mathbf {and} \; k \neq x
$$

The set of all continuous functions with aperiodic outputs can be defined as $\Bbb A$

$$ 
A = \{a(x)\}
$$

Conversely, a periodic differential function is one which eventually does revisit a previous point

$$
p^n(x_0) = p^k(x_0) \; \mathbf {for} \; \mathbf {some} \; x, k \in \Bbb N \; \mathbf {given} \; k \neq x
$$

and the set of all continuous periodic functions can be denoted as
 
$$
P = \{ p(x) \}
$$

Note that included in this definition are eventually periodic trajectories, which in finite time become periodic even if they do not begin as such.  

Equivalence (of sets) is a property that is reflexive, symmetric and transitive just like equivalence between numbers or expressions. Equivalence can be expressed as $\sim$ and signifies that the sets are of equal size if they are finite, or that a one-to-one and onto (bijective) function can be established between the sets if they are not finite. Properties of one set may be used to inform properties of an equivalent set.

We can specify a bijective function from periodic trajectories to rational numbers as follows: for a given trajectory in finite dimensions, specify each point in coordinates $(x_1, x_2, x_3, ..., x_n)$ to an arbitrary finite precision (for example, if $x_1 = \pi$ can be specified as $x_1 = 3.14159265$).  Note that this precision is a representation, rather than a substitute, for the value of the trajectory coordinate.  Now for each time point, add the coordinates to $0.$ to yield a rational number.  For example, in two dimensions if 

$$(x_1, y_1) = (15.32, 10.15)$$

and 

$$(x_2, y_2) = (14.99, 11.1)$$

then the number yielded from these points is

$$ 0.1532101514991110 $$ 

Now being that the trajectory is periodic, the number will have digits that eventually repeat (in finite time) because future coordinates are identical to previous coordinates.  All numbers that have digits that eventually repeat (after a finite number of digits) are rational numbers, and therefore this function maps periodic trajectories of discrete time to the set of rational numbers $Q$. 

Is this function one-to-one? For trajectories of any one dimension, each individual trajectory maps to one digit sequence, and likewise each digit sequence represents only one trajectory (given our arbitrary precision) and therefore the function is one-to-one.  The function is onto as well, because any rational number can be represented with a periodic function in our dimension and precision constraints (the function is not restricted to certain digits at any time).  This means that our mapping function is bijective, and thus we can establish an equivalence between the set of all periodic trajectories of discrete time and the set of rational numbers:

$$
P \to \Bbb Q \; (bijective) \\
P \sim \Bbb Q 
$$

Now what about aperiodic functions of discrete time?  Using the same mapping function specified above, we can map these trajectories to digit sequences after $0.$ that do not repeat.  Irrational numbers are represented as digit sequences that do not repeat, so we have mapped aperiodic trajectories to irrational numbers.  By the reasoning in the last paragraph, the mapping is one-to-one and onto and therefore the set of aperiodic trajectories is equivalent to the set of irrational numbers,

$$
A \to \Bbb I \; (bijective) \\
A \sim \Bbb I
$$

Thus the set of all periodic trajectories (as defined above) is equivalent to the set of all rational numbers, whereas the set of aperiodic trajectories is equivalent to the set of irrational numbers.

### Periodic but not aperiodic trajectories may be defined on the rationals

Restated, discrete maps of continuous functions can only be defined on the rational numbers if the trajectory is periodic.  The proof that follows assumes that the function $f(x)$ is continuous, but the theorem also applies to discontinuous functions (see below).

The proof for this statement is as follows: for any aperiodic discrete trajectory of a continuous function $f(x)$, the Sharkovskii theorem states that the same $f(x)$ also contains (prime) period points of periods $1, \; 2, \; 4, \; 8, ...$.  Each point is unique for each prime period, meaning that there is at least one unique point in the domain of $x$ that satisfies each period.  We can classify each point by its period:

$$
x_0 = 1, \; x_1 = 2, \; x_2 = 4, \; x_3 = 8 ...
$$

which indexes $x_n$ over all the integers, meaning that there is a one-to-one and onto correspondance between periodic orbits and the integers.  The integers being countably infinite, there are countably infinitely many periodic orbits in the domain of $x$.  For periodic trajectories of $f(x)$, therefore, a countable set such as the rational numbers suffices.

The same argument shows that a countable number of inputs in the domain of $x$ is insufficient to yield an aperiodic trajectory, any element of this set would be mapped on a finite period.  This is a contradiction by definition, and therefore aperiodic trajectories must contain an uncountably infinite number of elements in their domain, ie the irrational numbers or reals.

This is perhaps most clearly demonstrated for the [logistic map](https://blbadger.github.io/logistic-map.html) where $r=4$: trajectories may be periodic only if they land on rational points on the line, whereas trajectories that exist on the irrationals are aperiodic.

### Sensitivity to initial values implies aperiodicity (for most trajectories)

Say that points close together are stable if they stay close together in the arbitrary future, or are unstable if they diverge.  

$$
stable \Rightarrow | f(x) - f({x+\epsilon}) | \leq |f^n(x) - f^n((x + \epsilon)) \; \forall \; n \in \Bbb N|
$$

where $\epsilon$ is an arbitrarily small number and iterations of $f(x)$ are denoted $f^n(x)$. 

Now suppose that all points are unstable, such that all points that start close together move apart after a number of iterations. This is equivalent to saying that a system is sensitive to changes in initial values.  Now being that every initial value will eventually have different outputs than the value's neighbor (for any 'neighbor size'), each initial value has a unique trajectory.  As there is an uncountably infinite number of possible starting values on the interval $(0,1]$, 

$$
| \{ x \; : \; x \in (0, 1] \} | = |\Bbb R|
$$

there is an uncountably infinite number of possible trajectories if no two trajectories are the same, because they can diverge anywhere.  

As we have above established an equivalence between the set of continuous functions with periodic trajectories and rational numbers, 

$$
\Bbb B \sim \Bbb Q
$$

and as there are countably many rational numbers, there are countably many periodic trajectories. But there are uncountably many trajectories if the system is unstable everywhere, so most cannot be periodic: there are simply too many (as there are far more objects in an uncountable set than a countable one). Thus, instability (sensitivity) at all initial points leads to (nearly all) trajectories being aperiodic.  

This too is reflected in the logistic map: values such as r=3.99 yield countably many (unstable) periodic trajectories and an uncountable number of aperiodic ones.


### The set of aperiodic trajectories is equivalent to the set of unsolvable problems

Let's define solvable equations to be those that are computable in finite time for all inputs.  There exists and elegent proof for the idea that nearly every decision problem (that outputs one of two options) is not computable in finite time [here](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-videos/lecture-23-computational-complexity/).  This proof establishes that nearly all decision (output 'yes' or 'no') problems are unsolveable in finite time, and can be tweaked to establish an equivalence between uncomputable (by a finite algorithm) problems and irrational numbers as follows:

Any program to compute a decision may be represented as a binary string of finite length, which is also a representation for any natural number.  

$$ 
program \approx binary \; string \sim x \in \Bbb N
$$

Now rational numbers are just two natural numbers divided by each other so any finite program can be represented by a rational number.

$$ 
x \in \Bbb Q \;if \; x = \frac{a}{b}, \; |\; a, b \in \Bbb N \\
program \approx binary \; string \sim x \in \Bbb Q
$$

The solution to any decision problem may be denoted as an infinite string of bits, 1 for 'yes' or 0 for 'no' for any number of infinite inputs, which corresponds to any real number.  As any program to compute a decision is finite, and as this program may be represented by a rational number then only infinite strings of bits that are also representations of rationals may be computable. All other problems are unsolvable in finite time (with a finite program), and as any member of this set of problems is not rational but in the real numbers

$$
\{unsolvable \; problems\} \sim \Bbb R - \Bbb Q \sim \Bbb I
$$

Now as we have established an equivalence between irrational numbers and aperiodic (chaotic) systems above, by transitivity we can establish an equivalence between the set of unsolvable decision problems and the set of all outputs of chaotic systems or 

$$ 
\Bbb A \sim \Bbb I \sim \{unsolvable \; problems\}
$$

Thus there are as many aperiodic trajectories as there are unsolveable problems.










## Autoencoders: Learning by Copying

### Introduction

Autoencoders are machine learning models that attempt to replicate the input in their output. This endeavor may not seem to be very useful for tasks such as generating images or learning how inputs are related to one another, particularly when the models used are very large and fully capable of learning the identity function on the input.  Indeed, in the past it was assumed that models capable of learning the identity function (typically models with more neurons per layer than input elements) would do so without other forms of regularization.

Historically, therefore, autoencoders were studied in two contexts: as undercomplete models that compressed the input in some useful way, and as possibly overcomplete models that were subject to certain restrictions to as to prevent the learning of the identity function.  Here 'overcomplete' is used more informally than the strict linear algebra term, and signifies a model that has more than enough elements (neurons) per layer to exactly copy the input onto that layer.

A number of useful models have resulted from those efforts, including the contractive autoencoder which places a penalty on the norm of the Jacobian of the encoder with respect to the input, and the variational autoencoder which simultaneously seeks to minimize the difference between autoencoder output and input together with the difference between the hidden layer code $z$ given model inputs and a Gaussian distribution $N(z; I)$.

The primary drawback of these regularized autoencoders is that they tend to be incapable of modeling the distribution of inputs given to the model to a sufficient degree, which manifests experimentally as blurriness in samples drawn from these models. For variational autoencoders, there is a clear reason why samples would be blurry: transforming a typical image using a Gaussian distribution (via convolution or a similar method) results in a blurry copy of that image, so it is little surprise that enforcing the encoding of an input to approximate a Gaussian distribution would have the same effect.  As a consequence of this challenge, much research on generative models (particularly for images) has shifted away to [generative adversarial networks](https://blbadger.github.io/generative-adversarials.html) and more recently [diffusion models](https://blbadger.github.io/generative-adversarials.html).

Recently, however, two findings have brought the original assumption that overcomplete autoencoders would memorize their inputs into question. First, it has been observed that deep learning models for image classification tend to lose a substantial amount of input information by the time the input has been passed to deep layers, and that trained models learn to infer this lost information ([reference](https://arxiv.org/abs/2211.06496)).  Second it has been found that image classification models are more or less guaranteed to generalize to some extent when trained using gradient descent ([reference](https://arxiv.org/abs/2211.09639)).

In light of these findings, it is worth asking whether overcomplete deep learning autoencoders will necessarily memorize their inputs.  The answer to this question is straightforward, as we will see in the next section.

### Overcomplete autoencoders do not learn the identity function

The question of whether overcomplete autoencoders memorize their inputs can easily be tested by first training an autoencoder to replicate inputs, and then observing the tendancy of that autoencoder to copy its inputs after small changes are made.  Autoencoders may be trained in various ways, and we choose to employ the minimization of Mean-Squared Error between the input $a$ and reconstructed input $D(E(a))$ where $D$ signifies the decoder of hidden code $h$, $E$ the encoder which maps $a \to h$ and $O(a, \theta)$ the output of the encoder and decoder given model parameters $\theta$.

$$
L(O(a, \theta), a) = || D(E(a)) - a ||^2_2
$$

Mean-squared error loss in this case is equivalent to the cross entropy between the model's outputs and a Gaussian mixture model of those outputs.  More precisely assuming that the distribution of targets $y$ given inputs $a$ is Gaussian, the likelihood of $y$ given input $a$ is

$$
p(y | a) = \mathcal{N}(y; \mu (O(a, \theta)), \sigma)
$$

where $\mu (O(a, \theta))$ signifies the mean of the output and $y=a$ for an autoencoder.

As for the case observed with variational autoencoders, it has been assumed that training autoencoders via MSE loss results in blurry output images because it implies that the input can be modelled by a Gaussian mixture model.  And for a sufficiently small model this is true, being that only a small number of Gaussian distributions may compose the mixture model.  

But for a very large model it is not the case that MSE loss (or any maximum likelihood estimation approach) will necessarily yield blurry outputs because the Gaussian mixture model is a universal approximator of computable functions if given enough individual Gaussian distributions independent from each other given weights $w_1, w_2, ... w_n$

$$
g(a; w, \mu, \sigma) = \sum_i w_i \mathcal{N}(a; \mu_i, \sigma_i)
$$

Therefore it can be theoretically guaranteed that a sufficiently large autoencoder trained using MSE loss on the output alone will be capable of arbitrarily good approximations on a given input $a$ (and will not yield blurry samples).  

This being the case, we can now test whether an overcomplete autoencoder trained using gradient descent to minimize MSE loss will memorize their inputs.  Memorization in this case is equivalent to learning the identity function, so we can test memorization by simply changing an input some small amount before observing the output.  If the identity function has been learned then the model will simply yield the same input back again.

One such change to an input is the addition of noise.  In the following experiment we train a 5-hidden layer feedforward model, each layer having more elements that the input. As shown in the figure below, we find that indeed the overcomplete autoencoder does not memorize its inputs: upon the addition of Gaussian noise $\mathcal N(a \in [0, 1]; \mu=7/10, \sigma=2/10)$

![overcomplete autoencoder]({{https://blbadger.github.io}}/deep-learning/overcomplete_cifar10_autoencoder.png)

This experiment provides evidence for two ideas: first that overcomplete autoencoders do not tend to learn the identity function as evidenced by the difference in the autoencoder's output given a noise-corrupted input, $O(\widehat a, \theta)$ compared to the original input $a$, and second that the output actually de-noises the input such that 

$$
||O(\widehat a, \theta) - a|| < || \widehat a - a ||
$$

The latter observation is far from trivial: there is no guarantee that a function learned by an autoencoder, even if this function is not the identity, would be capable of de-noising an input. We next investigate why this would occur.

### Autoencoders are capable of denoising without being trained to do so

To re-iterate, [elsewhere](https://blbadger.github.io/depth-generality.html) it was found that deep learning models lose a substantial amount of information about the input by the time the forward pass reaches deeper layers, and furthermore that moels tend to learn to infer that lost information upon training.  The learning process results in arbitrary inputs being mapped to the learned manifold, which for datasets of natural images should not normally be approximated by Gaussian noise.  The learning process for natural image classifiers is therefore observed empirically to be as analagous to learning how to de-noise an input. One may safely assume that the same de-noising would occur upon training an autoencoder.  

For this section and much of the rest of this page, we will employ a more sophistocated autoencoder than the one used above.  We use U-net, a model introduced by [Ronneburger and colleagues](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28) for medical image segmentation.  

The U-net was named after it's distinctive architecture, which can be visualized as follows:

![Unet architcture]({{https://blbadger.github.io}}/deep-learning/modified_unet_architecture.png)

For now, we make a few small adjustments to the Unet model: first we remove residual connections (which if not removed do lead to a model learning the identity function because it is effectively low-dimensional) and then we modify the stride of the upsampling convolutions to allow for increased resolution (code is available [here](https://github.com/blbadger/generative-models)).  Notably we perform both training and sampling without switching to evaluation mode, such that the batch normalization transformations do not switch the batch's statistics with averages accumulated during training.

Returning to the question of the tendancy of training to reduce noise in an autoencoder, take the last layer's representations of the input in trained versus untrained Unet model (without residual layers) shown in the following figure.

![landscape representations]({{https://blbadger.github.io}}/deep-learning/unet_landscape_representations.png)

The trained model has clearly learned to reduce the noise in the last layer's representation relative to the untrained model. It may thus be wondered whether autoencoders are capable of learning to de-noise inputs.

![denoising autoencoder]({{https://blbadger.github.io}}/deep-learning/unet_512_denoising.png)

The same ability of autoencoders to de-noise is observed for Unet applied as an autoencoder for lower-resolution images, here 64x64 LSUN church images.

![unet autoencoder]({{https://blbadger.github.io}}/deep-learning/unet_autoencoding_churches.png)

With more training, we see that images may be generated even with a large addition of noise

![unet autoencoder]({{https://blbadger.github.io}}/deep-learning/unet_autoencoding_churches_2.png)

and for pure noise, we have the following:

![unet autoencoder]({{https://blbadger.github.io}}/deep-learning/unet_autoencoding_churches_3.png)

Early in this section we have seen that autoencoder training leads to the de-noising of Unet's representation (in the output layer) of its input.  After we have seen how Unet is capable of an extraordinary amount of denoising even without being trained to do so.  It may be wondered if these two phenomena are connected, that is, if denoising occurs because the learned representation has less noise than the input. If this were the case then we would expect a noise-corrupted input to lose some of this noise not only in the autoencoder output but also in the autoencoder's output layer representation of its input.  Our expectation is correct, as we can see in the following figure:

![unet representations]({{https://blbadger.github.io}}/deep-learning/unet_representation_denoising.png)

It may be wondered then if there is necessarily an equivalence relation between the ability of an autoencoder to accurately represent its input (and hence to de-noise the original representation as we will see in the next section) and the ability to accurately autoencode.  The answer here is no, not necessarily: given a powerful enough decoder, most information from the input can be lost and an accurate encoding can be made. For example, if a restrictive three-hidden-layer module is inserted into the Unet (see later sections for more details) then the model is capable of accurate autoencoding (and substantial de-noising) but does not accurately represent its input as shown in the following figure.

![unet hidden module representation]({{https://blbadger.github.io}}/deep-learning/unet_hidden_representation.png)

### Why Noninvertibility introduces Gaussian Noise in Input Representations

So far we have seen empirically that autoencoders are capable of removing noise from an input even when they are not explicitly trained to do so.  This may be understood to be equivalent to the finding that autoencoders learn manifolds that are not noisy, and map arbitrary inputs (even noisy ones) to that manifold.

There is another related phenomenon that may also play a role in the ability of autoencoders to denoise: the non-invertibility of transformations in untrained models introduces noise into the representations of the input, but much of this noise is removed from the representation during the training process.  Noise is introduced between non-invertible (or approximately non-invertible) transformation representations because many possible inputs may yield one identical output for models composed of non-invertible (singular) linear transformations.  These many possible inputs closely resemble Gaussian noise if the models in question are composed of transformations that are independent which is indeed the case for typical deep learning models at initialization.

To be precise, suppose we are given a simple fully connected model that has been initialized with weights initialized to a uniform distribution, $w_n \sim \mathcal U$ (which is the default for Pytorch). For the purposes of this argument, these weights could actually be distributed in any number of ways so long as the initialization is random and does not vary too much between layers.  

First assume that an input to be transformed into a representation $x_i$ is initialized near the origin, perhaps to uniform values. Now suppose that the model were composed of one or more layers, each layer being a linear transformation (matrix multiplication) $A_l$ from dimension $n \to m: n > m$ such that the output dimension $d$ is much smaller than the input dimension $D$.  The representation of the output $O(a, \theta)$ by the input is defined as an input $x_i$ such that $O(x_i, \theta) = O(a, \theta)$.  We can show here that given these assumptions the input representation is approximately normal before training, or $O(x_i, \theta) \sim \mathcal N$.

To see this, first take a model composed of a single linear transformation. Assuming full rank for $A_1$, the number of dimensions in which $x_n$ can change is $n - m$.  For large $n$, the chance that an element of $x_i > \epsilon$ is equal to the volume of a point in an n-dimensional sphere greater than $\epsilon$ away from the origin, which is equal to

$$
vol(d) \leq 2 \mathrm{exp} (-n \epsilon^2 / 4) v_n
$$

whereas the probability for a point in the normal distribution to exceed $epsilon$ away from the expectation is

$$
p(|x-\mu| > \epsilon) = \mathrm{exp}(-2n\epsilon^2)
$$

This is for one layer: for $k$ layers in a model, the number of dimensions in which $x_n$ has freedom to change is $(n_1 - m_1) (n_2 - m_2) \cdots (n_k - m_k)$.

It may be wondered why trained models then do not typically have representations that resemble Gaussian noise.  The answer is that after training we can no longer assume that sucessive layers contain weights that are independent: to put it concretely, the linear combinations of successive layers are no longer guaranteed to be orthogonal such that the representation of an output layer in a deep model is lower-dimensional than that of an untrained model. 

But notably even trained models would be expected to have Gaussian noise-like corruption after one or a few non-invertible layers, if those layers were composed of non-invertible transformations $n \to m: m << n$.  This is because the dimensions in which an input of a linear transformation with $n > m$ may vary for some fixed $f(x) = y$ are necessarily orthogonal, which is a result of the fundamental theorem of linear algebra. Assuming that the orthogonal dimension number $m - n$ is sufficiently large, a point chosen near the origin from these dimensions is approximately Gaussian as shown above.

This theory is supported by experimental evidence: observe that for [vision transformers](https://blbadger.github.io/vision-transformers.html) the representation of block 1 (which has only two fully connected layers per patch, ignoring attention transformations between patches) for trained and untrained models alike contains noticeable Gaussian noise whereas the deep layers of untrained *but not trained* models exhibits any Gaussian noise.  This is precisely what the theory predicts if training removes independence of weights from one layer to the next.

### Image Generation with an Autoencoder Manifold Walk

Earlier on this page we have seen that even very large and overcomplete autoencoders do not learn the identity function but instead capture useful features of the distribution of inputs $p(a)$.  We were motivated to investigate this question by the earlier findings that deep learning classification models tend to generalize even when they are capable of memorization, and that deep layers are typically incapable of exactly representing the input due to approximate and exact non-invertibility.

We can postulate that the theory behind generalization of classifiers also applies to autoencoders as long as they are trained using gradient descent, as there is no stipulation in the [original work](https://arxiv.org/abs/2211.09639) that the dimensionality of the output is less than the input, or that the models must be trained as classifiers (ie with one-element $\widehat y$ target values).  

Likewise, it seems fairly obvious that an inability of a deep layer (which could be an autoencoder output) to represent the input at the start of training would also prevent memorization to some extent, being that it would be highly unlikely for a model to learn the identity function if deeper layers cannot 'know' exactly what the input looks like.

We next saw how autoencoders are effective de-noisers, and that at the start of training typical deep learning archictures induce Gaussian noise in subsequent layers such that the model must learn to reduce such internal noise during training.  Being that there is no intrinsic difference between internal and input-supplied Gaussian noise, it is not particularly surprising that sufficiently deep autoencoders are capable of powerful noise reduction.

This being the case, it should be possible to generate inputs from the learned distribution $A \sim \widehat{p}(a)$ assuming that the learned distribution approximates the 'true' data distribution, ie $\widehat{p}(a) \approx p(a)$.

For overcomplete autoencoders, however, we are presented with a challenge: how do we navigate the latent space to synthesize a sample, given that for a high-dimensional latent space may be large such that only a small subregion of this space is actually explored by the model's samples?  For example, say the dimensionality of the smallest hidden layer of the autoencoder (which we take to be the latent space) is larger than the dimensionality of the input.  Simply assigning the values of this latent space as a random normal distribution before forward propegating this information to the output is not likely to yield an image representative of $p(a)$ unless samples of $p(a)$ yielded a similar distribution in that latent space.  

This is extremely unlikely unless some constraints are placed on the model, and indeed a very similar model to the one we have considered here has been investigated.  These models are called variational autoencoders, and although effective in many aspects of probability distribution modeling they suffer from an inability to synthesize sharp images $A$, which can be attributed to the restrictive nature of mandating that the latent space be normally distributed.

Instead of enforcing restrictions on our model, we can instead map inputs to the learned manifold and perform a walk on this manifold, effectively creating a Markov process to generate images.  Suppose we split the Unet autoencoder into two parts, an encoder $\mathcal E$ that takes inputs $a$ and yields a hidden space vector $h$ along with a decoder $\mathcal D$ that takes as an argument the hidden space vector $h$ and yields the synthesized image $g$

$$
g = \mathcal D(\mathcal E(a)) = \mathcal D(h)
$$

Being that $h$ is not low-dimensional but $\mathcal D(\mathcal E(a))$ has learned $p(a)$, we can first find the hidden space vector $h$ corresponding to some set of $A \sim p(a)$

$$
h_1' = \mathcal E(a)
$$

Adding a small amount of normally distributed noise to this latent space gives a new latent space point

$$
h_1 = h_1' + \mathcal N(h_1'; 0, \epsilon)
$$

Then using this latent space vector the corresponding synthesized images are

$$
g_1 = \mathcal D(h_1)
$$

Now $g_1$ is likely to be extremely similar or identical to $a$ as long as the learned manifold is not too unstable.  But if we then repeat this procedure many times such that the Markov chain may be expressed as

$$
h_{n+1}' = \mathcal E(g_n) \\
h_{n+1} = h_{n+1}' + \mathcal N(h_n'; 0, \epsilon) \\
g_{n+1} = \mathcal D(h_{n+1})
$$

we find that the latent space is effectively explored.  For a Unet trained on 256 $512^2$ resolution images of landscapes, we have the following over 300 steps on a manifold walk:

{% include youtube.html id='SzzIJD05aVI' %}

Training a generative model on such a small number of images is not likely to yield very realistic inputs, and it appears that the images generated during the manifold walk are locally realistic but globally somewhat incoherent (observe how portions of sky and land tend to pop up over the entire image).

Increasing the dataset size from 256 to 2048 (and decreasing the resolution to $256^2$) we have much more coherent generated images.  It is also interesting to note that we see movement between images learned from the training dataset, which is not surprising given that these are expected to exit on the learned manifold.

{% include youtube.html id='Ui05wJ1ueso' %}

It is more curious that we see a qualitatively similar manifold even without the addition of noise at each step of the Markov process.  More precisely, if we perform the manifold walk as follows

$$
h_{n+1} = \mathcal E(g_n) \\
g_{n+1} = \mathcal D(h_{n+1})
$$

we find

{% include youtube.html id='qVitpElMCCM' %}

This shows us that much of the tendancy of repeated mapping to a manifold to lead to movement along that manifold occurs in party due to that manifold's instability.  We can pfind further evidence that most of the manifold walking ability is due to the manifold's instability by performing the same noise-less Markov procedure on autoencoders after varying amounts of training.  With increased training, the proportion of inputs that walk when repeatedly mapped to a manifold decreases substantially: with 200 further epochs, we find that this proportion lowers from 7/8 to around 1/8.

### Diffusion with no Diffusion

Given that autoencoders are effective de-noisers, we can try to generate inputs using a procedure somewhat similar to that recently applied for [diffusion inversion](https://blbadger.github.io/diffusion-inversion.html).  The theory here is as follows: an autoencoder can remove most noise from an input, but cannot generate a realistic input from pure noise.  But if we repeatedly de-noise an input that is also repeatedly corrupted, an autoencoder may be capable of approximating a realistic image.

To begin, we use a random Gassian distribution in the shape of our desired input $a_g$

$$
a_0 = \mathcal N(a; \mu, \sigma)
$$

where $\sigma$ and $\mu$ are chosen somewhat arbitrarily to be $7/10, 2/10$ respectively.  Now we repeatedly de-noise using our autoencoder but on a schedule: as the number of iterations increases to the final iteration $N$, the constant $c$ increases whereas the constant $d$ decreases commensurately.

$$
a_{n+1} = c * \mathcal D( \mathcal E(a)) + d * \mathcal N(a; \mu, \sigma)
$$

Arguably the simplest schedule is a linear one in which $c = n / N$ and $d = 1 - (n/N)$.  This works fairly well, and for 30 steps we have the following for a Unet trained on LSUN churches (note that each time point $t=n$ corresponds to the model's denoised output rather than $a_n$ which denotes a denoised output plus a new noise).

![denoising autoencoder]({{https://blbadger.github.io}}/deep-learning/churches_markov_30.png)

And for the same model applied to generate images over 200 steps, we have

{% include youtube.html id='JLxOUVdNblI' %}

This method of continually de-noising an image is conceptually similar to the method by which a random walk is taken around a learned manifold, detailed in the last section on this page.  Close observation of the images made above reveal very similar statistical characteristics to those generated using the random manifold self-map walk in the video below

{% include youtube.html id='HbdgA3i6JOA' %}

It is interesting to note that these statistical characteristics (for example, the dark maze-like lines and red surfaces on buildings that make a somewhat Banksy-style of generated image) are specific to the manifold learned. Observe that a similar model, Unet with a hidden MLP, as follows:

![unet modified architecture]({{https://blbadger.github.io}}/deep-learning/hidden_unet_architecture.png)

is capable of remarkable de-noising ability but tends to make oil painting-style characteristic changes during the denoising process.

![denoising autoencoder]({{https://blbadger.github.io}}/deep-learning/unet_hidden_128_landscapes_denoising.png)

This suggests that the manifold learned when autoencoding images of landscapes (by this particular model) will also have oil painting-like characteristics. Indeed this appears to be the case, as when we use the diffusion-like markov sampling process to generate input images, we find that most have these same characteristics as well.

![denoising autoencoder]({{https://blbadger.github.io}}/deep-learning/nodiffusion_landscapes_hidden.png)

Increasing the number of iterations of the markov sampling process to 200, we have

{% include youtube.html id='f7tX2kOkn_8' %}

For an increase in sampled image detail, one can increase the number of feature maps per convolutional layer in the Unet autoencoder architecture. With a model that has twice the layers per convolution as the standard Unet, we have the following:

![denoising autoencoder]({{https://blbadger.github.io}}/deep-learning/wide_unet_landscapes.png)

Although this model synthesizes images of greater detail and clarity that the narrower Unet, there are signs of greater global disorganization (black borders are often not straight lines, sky sometimes is generated under land).  

One way to prevent this disorganization is to increase the number of convolutional layers present in the model (assuming fixed kernal dimension), and another is to employ at least one spatially unrestricted layer such as the fully connected layers above, or self-attention layers. We will investigate self-attention in later sections on this page, and for now will focus on increasing the number of convolutional layers.

To gain intuition for why this is, consider the following example: suppose one were to first take a single convolutional layer as the encoder and single up-convolution as the decoder.  There would typically be no model parameters influencing both the pixels in the upper right and lower left of an input assuming that the convolutional kernal is significantly smaller that the input image dimension (as is usually the case).  But then there is no guarantee that the model should be able to generate an image observing the 'correct' relationship between these two pixels as determined by the training dataset.

On the other hand, consider the extreme case in which many convolutions compose the input such that each image is mapped to a single latent space element, and subsequently many up-convolutions are performed to generate an output. There are necessarily model parameters that control the relationship of every pixel to every other pixel in this case.

Some experimentation justifies this intuition: when we add an extra layer to both encoder and decoder of the wide Unet, we obtain much more coherent images.

![denoising autoencoder]({{https://blbadger.github.io}}/deep-learning/wide_deep_unet_landscapes_128.png)

It is interesting to note that the extra width (more precisely doubling the number of feature maps per convolutional layer) reduces the number of painting-like artefacts present in the synthesized images after the diffusion-like generation process.

![denoising autoencoder]({{https://blbadger.github.io}}/deep-learning/unet_deep_synthesis.png)

### The Role of Batch Normalization in Denoising

Comparing the fully connected autoencoder to the Unet autoencoder, it is apparent that the latter is a more capable denoiser than the former.  It is not particularly surprising that a larger and deeper model (the Unet) would be a better de-noiser than the former being that it can


$$
y = \frac{x - \mathrm{E}(x_{m, n})}{\sqrt{\mathrm{Var}(x_{m, n}) + \epsilon}} * \gamma + \beta
$$

where $m$ is the index of the layer output across all sequence items (ie convolutional filter outputs specified by height and width for Unet models) and $n$ is the index of the input among all in the batch.

Batch normalization odes play a role in the ability of Unet to denoise: removing batch normalizations from all convolutions in Unet results in autoencoder outputs that do not have noise per se, but are also statistically quite different than the original image. This does not affect the autoencoder's ability to de-blur an input, however.  Note that batch norm on this page is not switched to evaluation mode during image synthesis such that the model continues to take as arguments the mean and standard deviation statistics of the synthesized images.

![no batchnorm autoencoder]({{https://blbadger.github.io}}/deep-learning/unethidden_nobatchnorm_denoising.png)

On the other hand, it can be shown that this does not depend on the presence of a batch at all. We can retain Unet's batch normalizations and simply train and evaluate a the model with batch sizes of 1 to remove the influence of multiple batch members on any given input.  In effect, this removes index $n$ but not $m$ in the batch norm equation above.  After training we see that this model is capable of around the same de-noising abilities that was observed with normal batch norm (ie with 8- or 16-size batches).

![modified batchnorm autoencoder]({{https://blbadger.github.io}}/deep-learning/autoencoder_denoising_1batch.png)

This provides evidence for the idea that the learning of $\gamma$ and $\beta$ only across a single image gives outputs that are statistically similar to the uncorrupted original input. One can think of batch normalization as enforcing general statistical principles on the synthesized images. For each layer (also called a feature map) of a convolution, batch normalization learns the appropriate mean and standard deviation across all samples $n$ in that batch and all spatial locations $m$ necessary for copying a batch of images.  During the image synthesis process, batch normalization is capable of enforcing these learned statistics on the generated images.

It may be wondered whether batch normalization is necessary for accurate input representation in deep layers of the Unet autoencoder after training.  The answer to this question is no, as with or without batch normalization the trained Unet output layer has equivalent representation ability of the input.

![no batchnorm autoencoder]({{https://blbadger.github.io}}/deep-learning/unet_nobn_representations.png)

### Representations in Unet generative models with Attention

It may be wondered why representation clarity (or lack thereof) would be significant. For tasks of classification, the importance of accurate input representation is unclear.  One might assume that a poor representation accuracy in deep layers prevents overfitting, but as observed [here](https://arxiv.org/pdf/2211.09639.pdf) the gradient descent process itself serves to prevent overfitting regardless of representational accuracy for classifiers.  For generative models, however, there is a theoretical basis as to why imperfect deep representation is favorable.

To see why perfect input representation in the output may be detrimental to generative models, take a simple autoencoder designed to copy the input to the output.  For a useful autoencoder we need the model $\theta$ to learn parameters that capture some important invariants in the data distribution $p(x)$ of interest.  But with perfect input representation capability in the output the model may achieve arbitrarily low cost function value by learning to simply copy the input to output.

Next consider the case of a denoising autoencoder.  This model is tasked with recoving the input $x$ drawn from the distribution $p(x)$ followed by a corruption process $C(x) = \tilde{x}$, minimizing the likelihood that the model output $ O(\tilde{x}, \theta) $ was not drawn from $p(x)$.  Under some assumptions this is equivalent to minimizing the mean squared error 

$$
L = || O(\tilde{x}, \theta) - x ||_2^2
$$

Now consider the effect of a perfect representation of the input $\tilde{x}$ in the output with respect to minimizing $L$.  Then we have

$$
L = || \tilde{x} - x ||_2^2
$$

which is the MSE distance taken by the corruption process. Therefore the model is capable of inferring the corruption process without training.

These ideas have practical influence on the design of autoencoders when one attempts to add self-attention modules to the Unet backbone. Attention-augmented Unet architectures are typically used for Diffusion models in order to increase the expressivity of the model in question, and consist of Linear attention between high-dimensional (introduced by [Katharopoulos and colleagues](https://arxiv.org/abs/2006.16236)) and standard dot-product attention between lower-dimensional convolutional maps in parallel to residual connections. 

The architecture we will use employs only linear attention, and may be visualized as follows:

![attention unet architecture]({{https://blbadger.github.io}}/deep-learning/attention_unet_architecture.png)

But after training, it is apparent that this model is not very effective at de-noising

![attention autoencoder]({{https://blbadger.github.io}}/deep-learning/Unet_attention_representation.png)

It may be wondered how much information is capable of passing though each linear attention transformation.  If we restrict our model to just one linear attention transformation and input and output convolutions (each with 64 feature maps), we find that the model tends to learn to copy the input such that the model denoises the input poorly and is capable of near-perfect representation of its input, shown as follows:

![attention autoencoder]({{https://blbadger.github.io}}/deep-learning/linear_attention_copying.png)

Linear attention was introduced primarily to reduce the quadratic time and memory complexity inherent in the dot product operation in the normal attention module with linear-time complexity operations (aggregation and matrix multiplication).  If we replace the linear attention module used above with the standard dot-product attention but limiting the input resolution to $32^2$ to prevent memory blow-up (while maintaining the input and output convolutions), we find that after training the autoencoder is capable of a less-accurate representation of the input than that which was obtained via linear attention.

![attention autoencoder]({{https://blbadger.github.io}}/deep-learning/dotprod_attention_copying.png)

The input representation (of information present in the the output layer of our modified Unet) of the dot-product attention autoencoder in the last figure is surprisingly good given that dot-product attention is a strictly non-invertible operation. Indeed, if we remove input and output convolutions such that only dot-product attention transforms the input to the output, an effective autoencoder may be trained that is capable of good input representation (but does not denoise) as shown in the following figure.

![attention autoencoder]({{https://blbadger.github.io}}/deep-learning/dotprod_attention_only_copying.png)

This appears to be mostly due to conditioning: it is very difficult to generate an accurate representation across a dot-product attention transformation without a residual connection.

### Conclusion

On this page we have seen that autoencoders learn to remove the noise introduced into their input representations via non-invertible transformations, and thus become effective denoising models without ever receiving explicit denoising training. The effect of this is that autoencoders may be used to generate images via a diffusion-like sampling process, and that one can observe the manifold learned by autoencoders is a similar way to that learned by generative adversarial networks.

Does this mean that we should all start using autoencoders to generate images and stop using diffusion models or GANs? Probably not: when typical steps towards scaling up the process of autoencoder training (increasing the batch size, distributing to many GPUs, etc.) are taken, the images generated after the sampling process are not much better than when the relatively small compute used for this page (one RTX 3060 for 12 - 36 hours) is used. This means that these are not particularly efficient learning algorithms.














### Aperiodicity implies sensitivity to initial conditions

As we have seen for the [logistic map](https://blbadger.github.io/logistic-map.html), small changes in starting values lead to large changes after many iterations.  It turns out that a fundamental feature of all chaotic systems is that their lack of periodicity implies extreme sensitivity to initial values, and this was shown by Lorenz in his pioneering work on convection.  Here follows a proof using contraposition, based on the work of Ed Lorenz.

### Theorem: Aperiodicity implies sensitivity to initial values (discrete version)

Restated, take $f$ to be a nonlinear function in finite dimensional phase space that is bounded by finite values and iterated discretely.  This entails that trajectories are unique, and that if $f$ returns to a previous point then $f$ is periodic.  Define $f$ to be aperiodic when future iterations do not revisit previous points in the space.  In symbols, an aperiodic $f$ is defined as follows:

$$
f(x) : f^n(x_0) \neq f^k(x_0) \; \forall n, k : n \neq k
$$

(An example of an $f$ satisfying these conditions is seen in the link at the top of this page, proving that this is not a vacuous statement)

And in contrast, $f$ is periodic when there is some period $f^n$ such that this future iteration is located at a current or past point:

$$
f(x) : f^n(x_0) = f^k(x_0) \; \exists n, k: n \neq k
$$

Note that these definitions restrict the following to discrete maps.  In higher dimensional continuous maps, $f(x)$ may be strictly aperiodic in that it never revisits a previous point, but this $f(x)$ may be decomposed into independant lower-dimensional trajectories that are periodic, revisiting previous points arbitrarily often.  See below for more on this subject.

**Proof:** 

There is nothing special about our initial value $x(0)$ relative to the others obtained by iterating an equation.  So any value that is iterated from $f$ can be considered a 'starting value'.  Now suppose that we make a small change to an iterated value $x_n$ to produce $x_n^*$

$$ x_n^* =  x_n + \epsilon  $$

where $\epsilon$ is an arbitrarily small finite number. Now suppose that this small change does not change future values, such that for any iteration number $i$,

$$\lvert f^i(x_n) - f^i(x_n^* ) \rvert \le \epsilon $$ 

ie $f^i(x_n)$ and $f^i(x_n^* )$ stay arbitrarily close to each other for all iterations.

As phase space trajectories are unique (meaning that any given point of the system has only one future trajectory), this system must be periodic if in is insensitive to initial values: whenever $x_{n+i}$ is within $\epsilon$ to $x_n$, the same iteration pattern obtained between these two points must repeat (recall that $f$ is defined to be bounded). The period may be very large, in that it may take many iterations of $f$ to come within $\epsilon$ of $x_n$, but if $\epsilon$ is finite then so will the period be.  

The geometric argument for this statement is as follows: if the conditions above are satisfied and we assume that future iterations $x_{n+a}$ finitely close to a current point $x_n$ travel together, there is a small but finite ball $B$ around each point on the trajectory $T$ where the radius of $B$ is $\epsilon$.  As the trajectory remains in a finite area by definition, it must revisit one of $B$ after infinite time because a finite area can be tiled by a finite number of $B$.  As $f$ is nonlinear,$B$ either grows or shrinks over time in the general case.  If $B$ is revisited, then $x_{n+a} - x_n \to 0$ as $t \to \infty$ and previous values are visited asymptotically.

Revisiting a previous value is equivalent to periodicity, and therefore insensitivity to initial values (in this case $x_n$) implies periodicity.  Taking the contrapositive of this statement, we have it that aperiodicity implies sensitivity to initial values (for discrete maps of $f$). All together, 

$$
f(x) : f^n(x(0)) \neq f^k(x(0)) \implies \\
\forall x_1, x_2 : \lvert x_1 - x_2 \rvert < \epsilon, \\
\exists n \; : \lvert f^n(x_1) - f^n(x_2) \rvert > \epsilon
$$

### Aperiodicity and sensitivity to initial values: continuous version

Lorenz defined sensitivity to initial conditions using a $\delta, \epsilon$ -style definition analagous to that for continuity.  This is especially interesting because it implies that trajectory which is sensitive to initial values is discontinuous in the output space with respect to the input. Other pages on this site show that there are too many possible aperiodic trajecotires for them to be continuous, as well as more direct proofs that aperiodicity implies discontinuity (with respect to the output space).  

### Decomposably periodic or quasiperiodic functions that are insensitive to initial values

The case for periodic versus aperiodic functions mapped continuously is mostly similar, but with a few extra considerations. The first and probably the most obvious is that with no discrete unit of time and therefore no iterations to speak of.  Instead there is a continuum, which one can think of as an infinite number of iterations between any two points in the trajectory.  Therefore rather than a finite $k$ number of iterations defining a period, there is some finite time $t$ that defines it.  Secondly, trajectories cannot cross one another's path in continuous maps, whereas they may for discrete cases where the points do not fall on top of one another.  

The theorem above extends to continuous functions in higher dimensions that do not necessarily have periodic outputs, but that may be decomposed into periodic trajectories that are independant of one another.  A simple case of this can be seen for two points rotating on circular orbit independantly of one another: if one has a rotational period of 1 and another of $\pi$, after starting in the same place on their respective circles they never again would do so because $\pi$ is irrational but 1 is rational.  Both circular orbits are periodic but the combination of the two is strictly aperiodic, but not sensitive to initial values because it can be composed of two independant periodic systems.

Note that the above argument does not apply to discrete maps.  This is because if an equation is iterated discretely, any period must have a finite number of iterations between $x_n$ occurrences.  Therefore the circle map above is only periodic if both trajectories reach the same point after a finite number of iterations.  Without loss of generality, say the first orbit has period $p$ and the second period $q$.  Then both points will be located in their initial position at iteration $k = pq$, which is finite as $p$ and $q$ are.  Therefore the map is necessarily periodic for discrete iterations.

Another example of an aperiodic map that can be decomposed into a periodic map: consider the case of a point traveling around a circle in discrete jumps, at some rational value between iterations.  This is technically an aperiodic system (as previous points are never revisited), but is clearly not sensitive to initial values because changing the starting position changes the final position by an identical amount.  This orbit is dense along the circle meaning that after an arbitrary number of iterations any point is arbitrarily close to any other.  If the coordinate system is changed such that a rational-valued rotation occurs upon each iteration, however, this system is periodic.  

This last function is often called 'quasiperiodic', meaning that while previous points are not actually revisited they are approximately revisited, to any degree of precision wished.  Lorenz defined quasiperiodicity as follows: given trajectory $P(t)$ for any $\epsilon > 0$, there exists a time $t_1 > t_0$ such that $P(t_1) - P(t_0) < \epsilon$ for all points on trajectory $P$. Clearly the point traveling along a circle in rational-size steps is quasiperiodic because given any point along the circle, one will find a future point arbitrarily nearby if one waits long enough.


For pages on this website, quasiperiodicity is included in the more general label of 'periodicity' for this reason.























## Clifford attractor

### Introduction

The Clifford attractor, also known as the fractal dream attractor, is the system of equations:

$$
x_{n+1} = \sin( ay_n) + c \cdot \cos (ax_n) \\
y_{n+1} = \sin( bx_n) + d \cdot \cos (by_n)
\tag{1}\label{eq1}
$$


where a, b, c, and d are constants of choice.  It is an attractor because at given values of a, b, c, and d,
any starting point $(x_0, y_0)$ will wind up in the same pattern. See Vedran Sekara's [post](https://vedransekara.github.io/2016/11/14/strange_attractors.html) for a good summary on how to use Python to make a plot of the Clifford attractor.  Code for this page may be found [here](https://github.com/blbadger/2D_strange_attractors).  

with $a = 2, b = 2, c = 1, d = -1$ the following map of \eqref{eq1} is made:
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_1.png)

with $a = 2, b = 1, c = -0.5, d = -1.01$ , 
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_2.png)

A wide array of shapes may be made by changing the constants, and the code to do this is 
[here](https://github.com/blbadger/2D_strange_attractors/blob/master/clifford_attractor.py)
Try experimenting with different constants!

### Clifford attractors are fractals

In what way is this a fractal?  Let's zoom in on the lower right side: 
In the central part that looks like Saturn's rings, there appear to be 6 lines.
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_zoom1.png)

At a smaller scale, however, there are more visible, around 10
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_zoom2.png)

And at a smaller scale, still more,
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_zoom3.png)

and more at a smaller scale,
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_zoom4.png)

Fractals are objects that appear similar at different scale, or more precisely objects that have a scaling dimension greater than their topological dimension.  In this case, we can idealize the topological dimension to be $d=1$, as after many iterations curves are formed.  But upon an increase in scale, more and more lines within one boundary are visible: there are an infinite number of idealized curves that form a Cantor set, which has a scaling dimension greater than 0. For more on what fractals are and why they are important, see [this page](https://blbadger.github.io/fractal-geometry.html).

### Clifford attractors shift dimension as constants change

For $a = 2.1, b = 0.8, c = -0.5, d = -1$, the attractor is three points:

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_0d.png)

For $b = 0.95$, more points:

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_0d2.png)

For $b = 0.981$, the points being to connect to form line segments

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_1d1.png)

And for $b = 0.9818$, a slightly-larger-than 1D attractor is produced

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_1d2.png)

From $b=0.95$ to $b \approx 1.18$, incremented evenly:

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_1.gif)

The flashes are not a result of video artefacts, but instead represent small changes that lead to the attractor shifting back to points.  Observe that the transition between a 0 and >1 dimensional attractor is not completely smooth!  Small changes to $b$ lead to large changes in attractor dimension, even though the >1-dimensional attractor's shape changes slowly.  

This is not peculiar to the Clifford attractor: any one-to-one mapping from $1$ to $2$ dimensions must be discontinuous.  And as there is no function that will map a point to a line (because any such function would have to map a point to many points, and would therefore not be a function), a vacuous observation is that there is no continuous mapping from 0 to 1 dimensions.

And when $b = 1.7$, a nearly-2d attractor is produced
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_2d.png)

### Semi-continuous mapping

Say you want to model an ordinary differential equation:

$$
\cfrac{dx}{dt} = f(x), \\
$$

such that the change in the variable $x$ over time $t$ is defined by a function $f(x)$, starting at an initial point:

$$
x(0) = x_0
$$

ie the position of x at time 0 is some constant equal to $x_0$. 

If the equation is nonlinear, chances are that there is no analytic solution.  What is one to do? Do an approximation! Perhaps the simplest way of doing this is by using discrete approximations to estimate where a point will go given its current position and its derivative.  This is known as Euler's method, and can be expressed as follows:

$$
x_{next} \approx x_{current} + \cfrac{dx_{current}}{dt} \cdot \Delta t
$$

where $\cfrac{dx_{current}}{dt}$ signifies the differential equation evaluated at the current value of x.

With smaller and smaller values of $\Delta t$, the approximation becomes better and better but more and more computations are required for the same desired time interval:

$$
x_{next} = x_{current} + \cfrac{dx_{current}}{dt} \cdot \Delta_t \quad as \, \Delta_t \to 0
$$

For a two dimensional equation, the approximations can be made in each dimension:

$$
\cfrac{dx}{dt} = sin(ay_n) + c \cdot cos(ax_n) \\
\cfrac{dy}{dt} = sin(bx_n) + d \cdot cos(by_n) \\ 
\\
x_{n+1} \approx x_n + \cfrac{dx}{dt} \cdot \Delta t \\
y_{n+1} \approx y_n + \cfrac{dy}{dt} \cdot \Delta t 
\tag{2}
$$

To make these calculations and plot the results in python, the wonderful numpy and matplotlib libraries are used and we define the Clifford attractor function with constants $a=-1.4, \; b=1.7, \; c=1, \; d=0.7$:
```python
# import third party libraries
import numpy as np 
import matplotlib.pyplot as plt 
plt.style.use('dark_background')

def clifford_attractor(x, y, a=-1.4, b=1.7, c=1.0, d=0.7):
	'''Returns the change in arguments x and y according to 
	the Clifford map equation. Kwargs a, b, c, and d are specified
	as constants.
	'''
	x_next = np.sin(a*y) + c*np.cos(a*x) 
	y_next = np.sin(b*x) + d*np.cos(b*y)
	return x_next, y_next
```
Setting up the number of iterations and the time step size, we then initialize the numpy array with 0s and add a starting $(x_0, y_0)$ coordinate:

```python
# number of iterations
iterations = 1000000
delta_t = 0.01

# initialization
X = np.zeros(iterations)
Y = np.zeros(iterations)

# starting point
(X[0], Y[0]) = (10.75, 8.2)
```

For computing (2), let's loop over the clifford function, adding in each next computed value to the numpy array.
```python 
# euler's method for tracking differential equations
for i in range(iterations-1):
	x_next, y_next = clifford_attractor(X[i], Y[i])
	X[i+1] = X[i] + x_next * delta_t
	Y[i+1] = Y[i] + y_next * delta_t
```
For continuous differential systems, one can follow a path along a vector grid in order to get an intuitive understanding of how the differential system influences a point's path.  To get a vector grid started, we define a grid using numpy with minimum, maximum, and intervals in the $(x,y)$ plane:

```python
# Vector plot initialization
x = np.arange(10.7, 11.2, 1/110)
y = np.arange(7.6, 8.4, 1/30)

X2, Y2 = np.meshgrid(x, y)
```
The next step is to calculate the size and direction of each vector at the designated grid points according to the clifford map, and assign each value a color.

```python
# calculate each vector's size and direction
a = -1.4
b = 1.7
c = 1.0
d = 0.7

dx = np.sin(a*Y2) + c*np.cos(a*X2)
dy = np.sin(b*X2) + d*np.cos(b*Y2)

color_array = (np.abs(dx) + np.abs(dy))**0.7
```

Now let's plot the graph, adding both vector plot and differential iterations to the same plot.

```python
# make and display figure
plt.figure(figsize=(10, 10))

# differential trajectory
plt.plot(X, Y, ',', color='white', alpha = 0.2, markersize = 0.05)

# vector plot
plt.quiver(X2, Y2, dx, dy, color_array, scale = 20, width=0.0018)
plt.axis('on')
plt.show()
```

For a starting point situated at $ (x_0, y_0) = (10.75, 8.2)$, at $\Delta t = 0.01$ a smooth path along the vectors is made.  The path is 1D, and the attractor is a point (which is zero dimensional).
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_0.01t.png)

at $\Delta t = 0.1$
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_0.1t.png)

at $\Delta t = 0.8$ (points are connected for clarity)
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_0.8t.png)

$\Delta t = 1.1$ the point attractor continues to unwind
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_1.1t.png)

$\Delta t = 1.15$
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_1.15t.png)

$\Delta t = 1.2$, the first few iterations reveal four slowly rotating lattice points
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_1.2t_lines.png)

with more iterations at $\Delta t$ = 1.2, it is clear that the attractor is now 1 dimensional, and that the path is 0-dimensional.  We have swapped a dimension in path for attractor!
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_1.2t.png)

$\Delta t = 1.3$ there are now 4 point attractors, and successive iterations come closer and closer to bouncing between these points. 
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_1.3t.png)

$\Delta t = 1.35$
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_1.35t_lines.png)

$\Delta t = 1.35$, a shape similar to the discrete map has formed.
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_1.35t.png)

From $\Delta t = 0.1 \to \Delta t = 1.35$,

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semiclifford_vid1.gif)

<!---
TODO: better zoom on the clifford attractor
Is the attractor for $\Delta t = 1.35$ a fractal? Zooming in on the bottom right section suggests that it is:

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_zoom1.png)
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_zoom2.png)
![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_zoom3.png)
-->

For $\Delta t=1.35$, the final position after a sufficient number of iterations is extraordinarily sensitive to initial conditions, as detailed [here](https://blbadger.github.io/pendulum-map.html).  Sensitivity to initial values implies aperiodicity, and bounded aperiodic maps may contain fractals attractors (also called strange attractors.

### Why semi-continuous?

Discrete differential systems are recurrence relations: a point's next position is entirely determined by its current position according to the equation system given.  In contrast, both a point's next position in a continuous system is almost entirely determined by its current position: the equation system merely determines the direction and rate of change.  

One can think of Euler maps with large $\Delta t$ values to be 'semicontinuous' because they represent amix between recurrence relations and (approximately) continuous maps: a point's next position depends on both its current position as well as on the equation system's output for that step. 

Thus as is the case for continuous systems (and unlike that for discrete systems), one can trace a point's path using a vector plot on a semicontinuous map.  On the other hand, semi-continuous but not continuous maps of dissipative nonlinear equations may be fractals in two dimensions, as is the case for discrete maps (see [here](https://blbadger.github.io/continuity-poincare.html) for an explanation). 

### Many attractors from one semi-continuous Clifford map

To generate a different Clifford attractor as defined in a discrete map, a change in the values of at least one of $a, b, c, d$ is required.  But this is not the case for a semicontinuous map: merely by changing the starting $(x_0, y_0)$ coordinate, many (possibly infinitely many) attractors are possible. 

For example, take the semicontinuous map with the same constants as before,  $a=-1.4, \; b=1.7, \; c=1, \; d=0.7$ and with $\Delta t=1.35$.  If the starting position is changed from $(x, y) = (10.75, 8.2)$ to $(x_0, y_0) = (25, 25)$, the following attractor is produced:

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_(25,25).png)

and at $(x_0, y_0) = (90, 90)$:

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_(90,90).png)


and as $\Delta t = 0.1 \to \Delta t = 1.35$ the attractor changes from point to line to fractal, but once again not smoothly: 

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_9090.gif)


The regions that attract all points into a certain attractor are called the basins of attraction (see the [henon map](/henon-map.md) page for more information).  For example, the attractor (shown above) of the starting point $(x_0, y_0) = (10.75, 8.2)$ is also the attractor of the point $(x_0, y_0) = (10.5, 8)$ and the attractor of the point $(x_0, y_0) = (9, 7)$.  Other starting points yield other attractors, so does steadily moving the starting point lead to smooth changes between attractors? 

For $(x_0, y_0) = (7.5, 7.5) \to (x_0, y_0) \approx (12, 12)$, the transition from one basin of attraction to another is both abrupt and unpredictable: very small changes in starting position lead to total disappearence or change of the attractor for certain values. This causes the attractors to flash when $(x_0, y_0) = (7.5, 7.5) \to (x_0, y_0) \approx (12, 12)$

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_ranged.gif)

For an investigation into which point ends up where, see [this page](/clifford-boundary.md).
## Clifford map boundary

Recall that the Clifford map (see [here](/clifford-attractor.md) for more information) is a two dimensional discrete map defined by the following equations:

$$
x_{n+1} = \sin(ay_n) + c \cdot \cos(ax_n) \\
y_{n+1} = \sin(bx_n) + d \cdot \cos(by_n)
\tag{1}
$$

Regions of space (in this case in the $(x, y)$ plane) that attract all points into a certain attractor are called the basins of attraction (see the [henon map](/henon-map.md) page for more information).  For example, the attractor of (1) with the starting point $(x_0, y_0) = (10.75, 8.2)$ is also the attractor of the point $(x_0, y_0) = (10.5, 8)$ and the attractor of the point $(x_0, y_0) = (9, 7)$.  Other starting points yield other attractors, so does steadily moving the starting point lead to smooth changes between attractors? 

For $(x_0, y_0) = (7.5, 7.5) \to (x_0, y_0) \approx (12, 12)$, the transition from one basin of attraction to another is both abrupt and unpredictable: very small changes in starting position lead to total disappearence or change of the attractor for certain values. This causes the attractors to flash when a movie is compiled of  $(x_0, y_0) = (7.5, 7.5) \to (x_0, y_0) \approx (12, 12)$

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/clifford_ranged.gif)

### Fractal boundaries for semicontinuous Clifford maps

We can get an idea for the boundaries of a basin of attraction by iterating (2) over a meshgrid, and seeing whether the starting point ends up sufficiently near an attractor, with the assumption that if near enough then future iterations will be stuck at the attractor.  This is only an estimate, but seems to provide an accurate portrayal of the behavior of various starting points of the semicontinuous clifford attractor.

For the attractor of the starting point $(x_0, y_0) = (10.75, 8.2)$, 

```python
import copy

def clifford_boundary(max_iterations, a=-1.4, b=1.7, c=1.0, d=0.7):
	''' A function to map the boundaries of attraction of the
	Clifford system, iterated semicontinuously. Takes a number of
	maximum iterations and Clifford constants as inputs, outputs
	a color-coded map of which values end up in a given basin of
	attraction over time.
	'''
	x_range = 3000
	y_range = 3000

	x_list = np.arange(8, 12, 4/x_range)
	y_list = np.arange(10, 6, -4/y_range)
	array = np.meshgrid(x_list, y_list)

	x2 = np.zeros(x_range)
	y2 = np.zeros(y_range)
	iterations_until_in_basin = np.meshgrid(x2, y2)
	for i in iterations_until_in_basin:
		for j in i:
			j += max_iterations

	not_already_in_basin = iterations_until_in_basin[0] < 10000

	for k in range(max_iterations):
		array_copied = copy.deepcopy(array[0]) # copy array to prevent premature modification of x array

		# clifford map applied to array 
		array[0] = array[0] + (1.35*t)*(np.sin(a*array[1]) + c*np.cos(a*array[0]))
		array[1] = array[1] + (1.35*t)*(np.sin(b*array_copied) + d*np.cos(b*array[1]))

		# note which array elements are enter the basin of attraction
		in_basin = np.abs(array[0] - 10.95) + np.abs(array[1] - 8.1) < 1
		entering_basin = in_basin & not_already_in_basin
		iterations_until_in_basin[0][entering_basin] = k
		not_already_in_basin = np.invert(entering_basin) & not_already_in_basin

	return iterations_until_in_basin[0]
```
can be called by

```python
plt.imshow(clifford_boundary(30), extent=[8, 12, 6, 10], cmap='twilight_shifted', alpha=1)
plt.axis('on')
plt.show()
plt.close()
```

which when combined with iterations of (1) starting at $(x_0, y_0) = (10.75, 8.2)$ yields

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/Clifford_boundary.png)

The dark area around the attractor is a basin: points are attracted to the region of interest within one iterations.  The border of this region are the lighter points, but these form all kinds of fractal patterns such that it is very difficult to tell where exactly the boundary is.  With the intricacies of this attractor basin boundary in mind, it is little wonder why slowly moving from one starting point to another causes such drastic changes in which attractor the point heads toward.

This basin seems quite complicated.  Is it a self-similar fractal?  Lets find out by zooming in on the point $(x, y) = (9.829945, 7.8592)$.

![clifford]({{https://blbadger.github.io}}clifford_attractor/clifford_bound_zoom1.gif)

There are more and more lines visible as the scale increases.  These lines form an irregular Cantor set, a one-dimensional fractal.

So the basin boundary (or at least part of it) is indeed a self-similar fractal.  How does this fractal basin boundary form?  We can observe what happens when iterating (2) going from $\Delta t=1.05 \to \Delta t=1.35$:

![clifford]({{https://blbadger.github.io}}clifford_attractor/clifford_boundary_20.gif)

What about the attractor for the starting point $x_0, y_0 = 90, 90$? We can see that it too has an extremely intricate boundary

![clifford]({{https://blbadger.github.io}}clifford_attractor/clifford_boundary_9090.png)

and that this boundary transforms from a smooth, relatively simple shape to this more complicated one as $\Delta t=0.5 \to \Delta t=1.5$.  

![clifford]({{https://blbadger.github.io}}clifford_attractor/clifford_boundary_9090.gif)

Is this boundary a fractal? We can zoom in to find out.  Increasing the scale near the point 

$$
(x, y) = (91.82168, 90.47914)
$$

yields

![clifford]({{https://blbadger.github.io}}clifford_attractor/clifford_bound_zoom2.gif)
## Programs to compute things

### Matrix Determinant

The determinant of a matrix (a two-dimensional array) is a number that corresponds to change in volume for a linear transformation encoded by that matrix, and determinants are useful for everything from testing vectors for linear dependance to solving systems of linear equations (which are two tasks that are not really as different as they may seem).  Not all matricies have determinants, but all square matricies do (although the value may be 0).

Determinant can be computed recursively, with the base case being the 2x2 matrix:

$$
\lvert A \rvert = 
\begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix} = ad - bc
$$

The determinant of a 3x3 matrix can be expressed as a linear combination of 2x2 matrix determinants as follows:

$$
\begin{vmatrix}
a & b & c \\
d & e & f \\
g & h & i \\
\end{vmatrix} = 
a
\begin{vmatrix}
e & f \\
h & i \\
\end{vmatrix}
-b
\begin{vmatrix}
d & f \\
g & i \\
\end{vmatrix}
+c
\begin{vmatrix}
d & e \\
g & h \\
\end{vmatrix}
$$

Similarly, determinants of 4x4 matricies can be computed by converting to a combination of 3x3 determinants, which can then be converted to a combination of 2x2 determinants.

Computing the determinants of small matricies using this process is not too difficult, but becomes tedious for matricies larger than 3x3.  The following matrix, for example, would take quite a long time to compute using the recursive method:

```python
matrix = [
[1, 2, 1, 4, 3, -1, 4, 1], 
[1, 2, 3, 2, 9, 1, 10, 2], 
[1, 2, 1, 1, 9, 0, 15, 3], 
[8, 0, 1, 0, 2, 3, 4, -8], 
[2, 3, 4, 0, 1, 2, -1, 3], 
[2, 1, 0, 0, 1, 1, -5, -6], 
[5, -6, 3, 7, -4, 0, 0, 1],
[1, 3, -5, 1, 7, 0, 4, -1]
]
```

Let's try to write a program in Python that can compute matrix determinants for us.  Whenever a problem consists of some sort of base case (here the method to compute 1x1 and 2x2 matricies) that is required for other cases, recursion is a good way to proceed.  Our strategy here is as follows: we know how to compute determinants for 1x1 and 2x2 matricies, and the determinants of larger matricies can be computed by reducing down to the determinants of many 2x2 matricies.  

To begin, let's import a useful class `copy` and define our function, including a docstring specifying inputs and outputs.  Now we can add computations for 1x1 and 2x2 matricies such that any time this function is called with a matrix of either size, the determinant is returned.

```python
# standard library
import copy

def determinant(matrix):
    ''' Returns the determinant of a matrix of arbirtary size 
    (note that only nxn matricies have determinants).  Takes
    one argument, a list of lists corresponding to an array
    with numerical values (float or int) for the matrix of interest
    '''
    # base case #1: if the matrix is 1x1
    if len(matrix) == 1: 
        return matrix[0][0]

    # base case #2: if the matrix is 2x2
    if len(matrix) == 2:
        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]
 ```

For larger matricies, we first need to construct slightly smaller matricies (without the row and column of each of the entries in the top row) which will then be further reduced if they are larger than 2x2 or else their determinants will be found.  First we assign the variable `result` to 0, which become our determinant value, and then make a loop that will iterate through the matrix width.  Next we use some list comprehension to make a new matrix with the first row removed, and copy this (to prevent problems with variable assignment in the recursion) and remove the column corresonding to the particular place in the first row designated by `i`.

```python
    else:
        result = 0
        for i in range(len(matrix)):
            new_matrix = [matrix[j] for j in range(len(matrix)) if j != 0]
            new_matrix2 = copy.deepcopy(new_matrix)
            for row in new_matrix2:
                del row[i]
```

Now comes the recursion: using the rule that every other determinant is added (and the rest are subtracted) as one iterates through the top row, the function `determinant()` is called on the smaller matrix made above.  If this matrix is larger than 2x2, the process continues until the determinant can be directly computed, whereupon it is multiplied to the appropriate top row value and subtracted or added to the variable `result`.  Outside the `else` clause, we return the result and are all done!

```python
            if i%2 == 0:
                result += matrix[0][i] * determinant(new_matrix2)
            else:
                result -= matrix[0][i] * determinant(new_matrix2)

    return result
```

Let's test the program out on our large matrix!  By printing out the results using `print (determinant(matrix))`, we get
```python
355329
[Finished in 0.4s]
```

Which can be checked against the built-in matrix determinant calculator found in numpy:

```python
import numpy

m = numpy.matrix(matrix)
print (numpy.linalg.det(m))

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
355328.9999999997
[Finished in 0.2s]
```

Although this program can theoretically compute the determinant of a matrix of any size, it is practically limited to matricies smaller than 11x11 due to time.

Can we make this program faster?  Thinking carefully about what the recursion is doing, we can see that the same computations will be performed over and over again.  This is a perfect opportunity to employ some heroic dynamic programming in order to save the computations we did previously in memory and simply refer to the answer we got last time we performed the computations.

To begin, we add a dictionary that will store our computed results.
```python
# standard library
import copy
matrix_dictionary = {}
```
we call the a function with the dictionary as an argument.  The base cases are the same (and omitted here for brevity) but then insteady of simply calling the smaller matrix determinant, we instead only do so if a tuple version of the matrix does not exist in our dictionary (it must be a tuple because lists are mutable and python dictionaries can only hash non-mutable datatypes like strings or tuples).  The determinant of this smaller matrix is saved in the dictionary such that whenever we want to know what the determinant is of this matrix is in the future, we can just look it up instead of recalculating it. 

```python
def determinant(matrix, matrix_dictionary):
    ... 
    ...
    # iterate through the values of the matrix, removing the row and 
    # col of each value contributing to the determinant. Find the 
    # value recursively only if it has not already been found and added to 
    # matrix_dictionary.  If it has already been computed, it is added to 
    # the dictionary.
    else:
        result = 0
        for i in range(len(matrix)):
            new_matrix = [matrix[j] for j in range(len(matrix)) if j != 0]
            new_matrix2 = copy.deepcopy(new_matrix)
            
            for row in new_matrix2:
                del row[i]

            new_tuple = tuple(tuple(i) for i in new_matrix2)
            if i % 2 == 0:
                if new_tuple not in matrix_dictionary:
                    new_matrix_det = determinant(new_matrix2, matrix_dictionary)
                    result += matrix[0][i] * new_matrix_det
                    matrix_dictionary[new_tuple] = new_matrix_det
                else:
                    result += matrix[0][i] * matrix_dictionary[new_tuple]
            else:
                if new_tuple not in matrix_dictionary:
                    new_matrix_det = determinant(new_matrix2, matrix_dictionary)
                    matrix_dictionary[new_tuple] = new_matrix_det
                    result -= matrix[0][i] * new_matrix_det
                else:
                    result -= matrix[0][i] * matrix_dictionary[new_tuple]

    return result
```

When we compare the running time of the pure recursive version with the 10x10 matrix:
```python
matrix = [
[1, 2, 1, 4, 3, -1, 4, 1, 5, 10], 
[1, 2, 3, 2, 9, 1, 10, 2, 9, -4], 
[1, 2, 1, 1, 9, 0, 15, 3, 5, -3], 
[8, 0, 1, 0, 2, 3, 4, -8, 3, -8], 
[2, 3, 4, 0, 1, 2, -1, 3, 1, -3], 
[2, 1, 0, 0, 1, 1, -5, -6, 1, 1], 
[5, -6, 3, 7, -4, 0, 0, 1, 2, 6],
[1, 3, -5, 1, 7, 0, 4, -1, 3, 0], 
[3, 4, 2, 7, 2, 1, 5, -9, 3, 13], 
[6, 7, -4, 1, 0, 1, 9, -1, 3, 1],
]
```
which yields

```bash
(base) bbadger@bbadger:~/Desktop$ time python matrix_determinant.py
18639282

real	0m31.581s
user	0m31.565s
sys	0m0.012s

```
to the running time of the top-down dynamically programmed version (using memoized recursion),
```bash
(base) bbadger@bbadger:~/Desktop$ time python matrix_determinant_memoized.py
18639282

real	0m0.169s
user	0m0.157s
sys	0m0.012s
```

we get the same answer, but the memoized version of the program is much faster! This one calculates matricies of under 19x19 in a reasonable amount of time, a substantial improvement over the standard recursive program.

The memoized version is a little faster than the numpy `numpy.linalg.det()` for this matrix 

```bash
(base) bbadger@bbadger:~$ time python ~/Desktop/matrix_determinant_memoized.py
18639282.00000001

real	0m0.195s
user	0m0.323s
sys	0m0.221s
```

For larger matricies, `numpy.linalg.det()` is faster than our memoized recursive solution.  How is this possible?  It turns out that while the recursive approach to matrix determinant finding is perfectly good, this is not the fastest approach: instead, there is a property of the determinant that allow for an approach that does not use recursion at all.  This property is that scalar multiple of one column to another does not change the determinant's value.  Therefore, determinants can be computed very rapidly by simply eliminating columns (transforming values to zeros in that column) thereby avoiding the above algorithm.

### Trailing factorial zeros

The factorial $n!$ is equal to the product of all the natural numbers (not including 0) the same size or smaller than the number in question.  The factorial grows extremely large as $n$ increases, such that the factorial of large numbers are practically uncomputable in a reasonable amount of time!  

But say that we are not interested in the exact value of the factorial, instead the quetions is how many zeros the number has at the end.  To make this as general as possible, let's extend this question to how many zeros the factorial of a number `n` has at the end, in a given base `base`. For example, 5 factorial in base ten is 120, which has one trailing zero.

$$ 
5!_{10} = 12\mathbf{0} \to 1 
$$

whereas 20 facotorial in base 3 has 8 trailing zeros

$$
20!_{3} = 1210121221100100101120122022221\mathbf{00000000} \to 8 
$$

How is one to find the number of trailing zeros of a factorial without calculating the entire number?  Every natural number is composed of prime numbers, so looking at primes might be a good idea.  Let's start in base 10, which is the most familiar for many of us.  Every number in base 10 may be represented by a string of digits 0-9.  Taking a factorial of a number is the same as multiplying all smaller natural numbers together, so we are interested in how to get 0s at the end of a number as the result of multiplication. 

So multiplication is the key transformation we are applying to a pair of numbers at a time, but which digits contribute to trailing 0s?  The last digit of each number is important here, because if the last digits do not multiply together to make 0 then there are no trailing 0s!  Now we can ask: which primes in range 0-9 (ie which digits) multiply to make a 0?  The answer is 2 and 5, and although 0 is not a prime it can be included in this list because it will also yield a 0. 

Now the question is slightly different: how do we find the number of 5, 2, and 0s that will be multiplied together in the last non-zero digit of the factors of $n!$?  Fortunately for us, there exists a formula for determining the largest power of any given prime number of the factorial of a number: [Legendre's formula](https://en.wikipedia.org/wiki/Legendre%27s_formula).  This formula is wonderful because it allows us to bypass calculating the trailing zeros factor by factor and simply focus on the single factorial number itself.  Legendres formula is as follows: for a prime number $p$ and factorial $n!$, $v_p(n!)$ is the exponent of the largest power of prime $p$ that divides $n$ without remainder and is computed as follows:

$$
v_p(n!) = \sum_{i = 1}^\infty \lfloor\frac{n}{p^i}\rfloor
$$

For the general case in any base $p$, the formula becomes

$$
v_p(n!) = \frac{n - s_p(n)}{p-1}
$$

Where $s_p(n)$ is the sum of the digits of the base-$p$ version of $n$.

Now we are ready to tackle the problem of trailing zeros!  First we will need to define a function (including a doc string) and in this function, make a helper method to determine if a number is prime. The library `math` is also imported, as it contains many useful mathematical functions.  The function to test whether a number is prime or not is fairly straightforward: going from 2 up to the square root of the number in question, if this number is divisible by any of these smaller ones then it is not prime. We can stop at the square root of the number in question because any larger number that is a factor of our original must also have a factor smaller than the square root as well, and this was already checked.

```python
# Import standard library
import math

def zeros(base, n):
    '''Returns the number of 0s trailing the factorial of a number (n) supplied in any given base (argument base)
    using Legendre's method.  This allows for the number of trailing zeros to be computed for inputs whose factorial
    is not computable in any reasonable amount of time.
    '''

    def is_prime(number):
        '''Determines if the argument number is prime.
        Outputs True if prime, else False.
        '''
        if number == 2:
            return True
        for i in range(2, int(number**0.5) + 1):
            if number % i == 0:
                return False
        return True
```

Now let's consider the effect of different bases on the number of trailing zeros.  Say we are in base 10.  Digits 5 and 2 multiply to make 10, and so add a zero. But in base 7, no smaller primes compose 7 because it is prime and so the only digit that can make more zeros in this base is 7 and 1.  We assign an empty list to `ls`, which will store the primes that compose our base and the number of times they are multiplited to make the base. 

If the base is not prime, we find its composition.  Ranging from 2 to half the base, we can use our `is_prime()` function to determine if a digit smaller than `base` is prime.  If it divides `base` evenly (with no remainder), then we incrementing `j` until it no longer does.  We then assign the value of `j` (minus one to account for the initial value of 1) to the variable `exponent`, and add the prime `i` with the exponent associated with this prime we just found by dividing `base` repeatedly.  At the end of the `for` loop, we have a `ls` with all primes that compose the `base` together with the exponents that multiply these primes.  Adding each prime `i` to the power of its `exponent` should return our `base`.  If the base is prime, we add only the `base` with `1` to the list.

```python
    exponent = 1
    ls = []
    if not is_prime(base):
        for i in range(2, base//2 + 1):
            if is_prime(i) and base%i == 0:
                j = 1
                while base % (i ** j) == 0:
                    j += 1
                exponent = j - 1
                ls.append([i, exponent])
    else:
        ls.append([base, 1])
```

So now we have the composition of our base, in the form of a list `ls`.  

Now it is time for another helper function. We want to find the sum of the digits of $n$ for the general Legendre's formula, so the function `sum_digits(base, n)` is made to compute the sum of the digits of $n$ in the base $base$ provided. 

```python
  def sum_digits(base, n):
        '''A helper function that adds the digits 
        of the argument n in the base provided.
        '''
        number_ls = []

        while n >= base:
            remainder = n % base
            number_ls.append(remainder)
            n -= remainder
            n = int(n // base)
        number_ls.append(n)
        return sum(number_ls)
```

Now we can put these pieces together and apply Legendre's formula!  First we can assign a variable `power_ls` to an empty list.  Then loop over `ls`, storing the exponent of each prime in `base` (`pair[0]`) for this prime in $n1$ as determined by the general Legendre's formula.  This number is assigned to the variable `power`, which is then divided by the exponent of that prime in the composition of `base` (`pair[1]`).  We use floor division here because the interest is in the maximum number of times the prime composition `pair[1]` can fit in `power`.  This result is added to `power_ls`, and the loop continues for all values in `ls`.  The minimum of `power_ls` is returned because each zero in the factorial requires at least one of each prime factor, so the minimum power of these factors gives us the number of trailing zeros in our base of interest.  

```python
    # Legendre's method for computing trailing zeros 
    power_ls = []
    for pair in ls:
        power = int((n-sum_digits(pair[0], n))/(pair[0] - 1))
        power_ls.append(power // pair[1])

    if not power_ls: return 0
    
    return min(power_ls)
```

And now we are done! Let's check the work.

```python
# example inputs
base = 3
n = 20

# example function call
print (zeros(base, n))

~~~~~~~
8
[Finished in 0.1s]
```

It checks out for $20!$ in base $3$.  More tests can be done (unit testing is particularly useful here) to convince us that this function does what it says it does.  Does it run in reasonable time for large values of n?  It does!

```python
# example inputs
base = 7
n = 1349182374091283740932184

# example function call
print (zeros(base, n))

~~~~~~~~~~~~
224863729015213959151616
[Finished in 0.0s]
```



### Continuity and Poincar-Bendixson

For this page, *aperiodic* signifies dynamical systems that are bounded (ie do not diverge to infinity) and lack periodicity.  Specifically, continuous phase space portraits of differentiable ODEs are considered, so the trajectories of such systems are necessarily differentiable (as they are defined on differential equations).  Unbounded dynamical systems may also be thought of as being aperiodic but are not considered.  On this page but not elsewhere, a 'map' is synonymous with a 'function'. 'dimensions' are taken to be plane dimensions.

### Why discrete but not continuous maps in 2 dimensions exhibit fractal attractors

As seen with examples on [this page](https://blbadger.github.io/), movement from a continuous to a discrete-like map of continuous equations in phase space mapping $\Bbb R^2$ into $\Bbb R^2$ results in a shift from a point to a fractal attractor.  Note that these examples are only approximations of continuous maps, as in most cases one requires an infinite number of mathematical operations to produce a true continuous map of a nonlinear equation.  

There is a good reason for this shift: the Poincare-Bendixson theorem, which among other findings states that any attractor of a continuous map in two dimensions must be periodic: the attractor is either a point or a circuit line, corresponding to a zero- or one-dimensional attractor.  

### The Poincar-Bendixson Theorem: continuous dynamical systems in two dimensions are periodic

If continuous maps may form aperiodic attractors in three or more dimensions, why are they unable to do so in two or less?  

A two dimensional curve's intersection with a line may be discontinuous as well. Why can't two dimensional continuous dynamical trajectories also be aperiodic?  The answer is that though the intersection may be discontinuous, it is restricted over time because trajectories are unique in phase space (ie lines cannot cross).

Here is a geometric argument: suppose a bounded trajectory passes through an arbitrary area $A$ of phase space in $D=2$, perhaps in $R^2$.  Reaching the edge of $A$, the trajectory can eventually explore one or the other area $a$ or $a'$, because the trajectory is bounded and must change direction eventually.

![poincare_1]({{https://blbadger.github.io}}misc_images/poincare_restriction.png)
(1)

Whichever of $a$ or $a'$ the trajectory enters, it is unable to cross into the other if moving through $A$.  This is not necessarily the case if the trajectory exits $A$ and then re-enters.  As the boundary region $R$ of the trajectory is finite by definition, exit and re-entry into $R$ is impossible.  

We can fill $R$ with areas $A_1, A_2, A_3 ...$ none of which are necessarily congruent with $A$.  Some of $A_1, A_2, A_3 ...$ necessarily share a border with $R$ because the latter has finite area.  At these areas, the choice $a, a'$ restricts all future trajectory locations because exit from $R$ is not allowed.  Now note that the trajectory is defined as continuous and differentiable, meaning that if $A$ is small enough then the trajectory path approximates a line.  Therefore (1) is general to any arbitrary point in $R$, and future trajectories are restricted to either $a$ or $ a'$ and $R$ shrinks.  As $t \to \infty$, $R \to 0$ and a trajectory's future values are arbitrarily close to previous ones, meaning that the trajectory is periodic. More formally, the result is

$$
D=2 \implies \\
\forall f\in \{f_c\} \; \exists n, k: f^n(x) = f^k(x) \; if \; n \neq k
$$

To see why this theorem does not apply to instances where $D=3$, observe that if another dimension is present, a bounded trajectory is not limited to $a$ or $a'$ but can exit the plane and re-enter an abitrary number of times in either.  This means that $A$ is not restricted over time, and therefore the trajectory is not necessarily periodic. 

Note also that this does not apply to a nowhere-differentiable map.  Such a map is undescribable by ordinary differential equations regardless.

### Postulate: $D-2$ dimensions are required for unrestricted, discontinuous cross-sections

For continuous maps, aperiodic attractors seem to form in $n-2$ dimensions.  For the case of a 2-dimensional map considered by the Poincare-Bendixson theorem, this means that the aperiodic trajectory forms in 0 dimensions, ie at a point.  As a point is by definition a period 0 attractor in phase space, there is no aperiodic trajectory in 2 dimensional phase space.

Note that this only applies to phase space in which every trajectory must be unique.  Boundary maps of continuous ODEs are unsrestricted are capable of forming fractals in 2 or fewer dimensions. 

### Discontinuous maps may be aperiodic in 1 or more dimension

In other pages on this site, it was made apparent that moving from a continuous to a discontinuous map is capable of transforming a periodic trajectory to an aperiodic one.  From the Poincar-Bendixson theorem, it is clear why this is the case: a restriction to any space as seen above is impossible if the trajectory may simply discontinuously pass through (jump through) previous trajectories.  

Careful observation of discontinuous 1-dimensional or 2-dimensional maps shows that trajectories cross each other if the map is aperiodic, or at least they would if the trajectories were continuous.  The meaning here is not that one point in phase space heads in two directions (as this would no longer be specified by a function) but instead that if one connects each individual point in succession according the the order in which they are plotted, the connecting lines must cross each other if the map is aperiodic.  An example of this was seen for the [clifford attractor](https://blbadger.github.io/clifford-attractor.html), where the map became aperiodic precisely when the trajectories crossed each other.

On the other hand, the [pendulum map](https://blbadger.github.io/pendulum-map.html) iterated discontinuously is eventually periodic.  This is because the pendulum map trajectories do not cross each other. Therefore the available phase space shrinks as time passes.  This means that this map is not aperiodic (for all time), and thus is insensitive to initial values.




## Feature Visualization II: Deep Dream

### Introduction with Feature Optimization

In [Part I](https://blbadger.github.io/feature-visualization.html), deep learning model convolutional features were investigated by constructing an input that resulted in the highest total activation in that feature, starting with random noise.  But the training dataset for the models used (ImageNet) does not contain any images that resemble pure noise, making the initial input $a_0$ somewhat artificial.  What happens if we start with a real image for $a_0$ rather than noise and modify that image using gradient descent?

To recap, an input $a$ may be modified via gradient descent where the gradient at step $n$, denoted $g_n$, is some loss function of the output $J(O)$ on the input $a_n$, given model parameters $\theta$,

$$
g_n = \nabla_a J(O(a_n, \theta))
$$

On the page referenced above, we optimized the output activation of interest $\widehat y_n$ by assigning the loss function of the output layer $J(O)$ to be the difference  $J(O(a, \theta)) = C - \widehat y_n$ where $C$ is some large constant and the initial input $a_0$ is a scaled normal distribution.  But gradient descent alone was not found to be very effective in producing recognizable images on $a_0$, so two additional Bayesian priors were added: smoothness (ie pixel cross-correlation) via Gaussian convolution $\mathcal{N}$ and translational invariance with Octave-based jitter, here denoted $\mathscr{J}$, were employed with the gradient for input $a_n$, denoted $g_n$.  The actual update procedure is

$$
a_{n+1} =\mathscr{J} \left( \mathcal{N}(a_n + \epsilon g_n) \right)
$$

Now we will investigate a similar update, but using octaves without jitter (denoted $\mathscr{O}$).  This is done by increasing the image resolution and feeding the entire image into the model, rather than a randomly cropped version.  

$$
a_{n+1} =\mathscr{O} \left( \mathcal{N}(a_n + \epsilon g_n) \right)
$$

A three-octave approach was found to result in clear images.

```python
# original image size: 299x299
single_input = directed_octave(single_input, target_output, 100, [0.5, 0.4], [2.4, 0.8], 0, index, crop=False)

single_input = torchvision.transforms.Resize([380, 380])(single_input)
single_input = directed_octave(single_input, target_output, 100, [0.4, 0.3], [1.5, 0.4], 330, index, crop=False)

single_input = torchvision.transforms.Resize([460, 460])(single_input)
single_input = directed_octave(single_input, target_output, 100, [0.3, 0.2], [1.1, 0.3], 375, index, crop=False)

```

We will start by optimizing one or two features at a time.  To do this, we want to increase the activation of each neuron in that feature using gradient descent. One way to accomplish this is to assign the loss function $J(O(a, \theta))$ to be the sum of the difference between each neuron's activation in the feature of interest, as a tensor denoted $z^l_f$, and some large constant $C$ (denoted as a tensor as $\pmb{C}$.

$$
g = \nabla_a(J(O(a; \theta))) = \nabla_a(\pmb{C} - z^l_f) \\
= \nabla_a \left(\sum_m \sum_n C - z^l_{f, m, n} \right) \\
$$

As was done for feature visualization, computation of the gradient can be implemented with $C=200$ as follows:

```python
def layer_gradient(model, input_tensor, desired_output, index):
	...
	input_tensor.requires_grad = True
	output = model(input_tensor).to(device)
	focus = output[0][index][:][:] # optimize the feature of interest
	target = torch.ones(focus.shape).to(device)*200
	loss = torch.sum(target - focus)
	loss.backward()
	gradient = input_tensor.grad
	return gradient
```

For this procedure applied to 8 features from layer Mixed 6e in InceptionV3 where the original input $a_0$ is an image of flowers, we have

![Dream]({{https://blbadger.github.io}}/neural_networks/InceptionV3_mixed6d_dream.png)

It may be wondered if there is any meaningful difference between this procedure and [feature visualization](https://blbadger.github.io/feature-visualization.html) in which $a_0$ is scaled Gaussian noise.  Are there noticable differences after gradient descent when certain features are optimized with noise or real images as the original inputs?  We can investigate this question by comparing the images generated above to those using the same optimization method but starting from the following scaled normal distribution

```python
single_input = (torch.rand(1, 3, 299, 299))/10 + 0.5 # scaled distribution initialization
```
and it is apparent that the input does influence the optimized image noticably for a number of features in Layer Mixed 6e.

![Dream]({{https://blbadger.github.io}}/neural_networks/inception_dream_layer_comparison.png)

One can also modify the input $a$ such that multiple layers are maximally activated: here we have features from layers 'Mixed 6d' and 'Mixed 6e' jointly optimized by assigning the total loss to be the sum of the loss of each feature, again using a collection of flowers $a_0$.

![Dream]({{https://blbadger.github.io}}/neural_networks/InceptionV3_mixed6d_Mixed6e_dream.png)

### Layer Optimization

What happens if the input image were optimized without enforcing smoothness, or in other words omitting the Gaussian convolutional step during gradient descent?  We can implement gradient descent using octaves but without Gaussian convolution,

$$
a_{n+1} =\mathscr{O} (a_n + \epsilon g_n)
$$

as follows:

```python

def octave(single_input, target_output, iterations, learning_rates, sigmas, index):
	...
	start_lr, end_lr = learning_rates
	start_sigma, end_sigma = sigmas
	for i in range(iterations):
		single_input = single_input.detach() # remove the gradient for the input (if present)
		input_grad = layer_gradient(newmodel, input, target_output, index) # compute input gradient
		single_input -= (start_lr*(iterations-i)/iterations + end_lr*i/iterations)* input_grad # gradient descent step
	return single_input
```

The other change we will make is to optimize the activations of an entire layer $\pmb z$ rather than only one or two features.  One way to do this is to optimize the sum $z^l$ of the activations of layer $\pmb z^l$

$$
z^l = \sum_f \sum_m \sum_n \pmb{z}^l_{f, m, n}
$$

which is denoted as the sum of the tensor `[:, :, :]` of layer $l$.  The optimization of all the features in a layer may be implemented as follows: 

```python
def layer_gradient(model, input_tensor, desired_output, index):
	...
	input_tensor.requires_grad = True
	output = model(input_tensor).to(device)
	focus = output[0][:][:][:] # optimize all features of an input
	target = torch.ones(focus.shape).to(device)*200
	loss = torch.sum(target - focus)
	loss.backward()
	gradient = input_tensor.grad
	return gradient
```

Using InceptionV3 as our model and applying gradient descent in 3 octaves to an input of flowers, when optimizing layer Mixed 6b we have

![Dream]({{https://blbadger.github.io}}/neural_networks/dream_mixed6b.png)

Other models yield similar results, for example optimizing layer 3 of ResNet gives

![Dream]({{https://blbadger.github.io}}/neural_networks/Resnet50_layer3_dream.png)

### Image Resolution Flexibility

One interesting attribute of convolutional layers is thay they may be applied to inputs of arbitrary dimension.

As for feature maps, we can apply the gradient descent procedure on inputs of abitrary resolution because each layer is convolutional, meaning that only the weights and biases of kernals are specified such that any size of image may be used as input $a$ without having to change the model parameters $\theta$.  Note that this is only true if all layers up to the final are convolutional: as soon as one desires use of a fully connected architecture, it is usually necessary to use a fixed input size. Right click for images at full resolution.

![Dream]({{https://blbadger.github.io}}/neural_networks/Inception3_dream_mixed6b_layer.png)

![Dream]({{https://blbadger.github.io}}/neural_networks/Inception3_dream_mixed6b_layer_hres2.png)

Note, however, that there is a clear limit to this procedure: although the input image resolution may be increased without bound, the convolutional kernals of the model in question (here InceptionV3) during training were learned only for the resolution that training inputs existed at.  This is why an increase in resolution yields smaller details introduced relative to the whole image, as the changes during gradient descent are made by a model that learned to expect images of a lower resolution.

### How Deep Dream makes Coherent Images

It may be wondered how deep dream makes any kind of recognizable image at all.  For [input image generation](https://blbadger.github.io/input-generation.html), we saw that gradient descent on an initial input of random noise did not yield any recognizable images, and this was for only one neuron or feature at a time rather than for many features as we have here.  It was only after the addition of a smoothness Bayesian prior that gradient descent was able to begin to produce recognizable images, but smoothness is typically not added during deep dream.

Futhermore, when one considers how a convolutional layer works for image classification, it is not immediately clear how optimizing the activation of many layers together would give anything other than an incoherent jumble.  This is because during feed-forward operation each feature map in a convolutional layer is expected to have a different activation corresponding to which features are found in the image one wants to classify.  Activating all the feature maps is equivalent to saying that one wants an image that has all possible features, and for deeper layers with many (>= 256) features one may reasonably expect that this image will not look like much of anything at all, especially when no smoothness constraint is enforced.

After some consideration, the situation may not seem as bad as at first glance.  The deep dream process begins using a natural image rather than noise, and therefore although we don't enforce statistical characteristics of natural images during gradient descent we have enforced them at the beginning of gradient descent.  

To see why optimizing the activation of many features at once does not necessarily lead to a nonsensical image, consider the following example in which first one, then five, and then all 768 features of InceptionV3's layer Mixed 6c are optimized.  As we saw in [part 1](https://blbadger.github.io/feature-visualization.html), the following maps were found to maximally activate each of the first five features of layer Mixed6c:

![deep dream explanation]({{https://blbadger.github.io}}/neural_networks/deep_dream_explanation2.png)

These inputs were generated from noise, but nevertheless if one particular feature were to be activated by a certain pattern, one would expect this to be the case if the starting point were not noise but instead an image of some kind.  Indeed we see that is the case: observe how feature 4 gives a very similar yarn-like appearance when the starting point is a picture of a dalmation (top right below) as when it is noise (above).

![deep dream explanation]({{https://blbadger.github.io}}/neural_networks/deep_dream_explanation.png)

On the bottom row, multiple features are optimized simultaneously.  It is necessary to scale back the gradient to avoid producing very high-frequency or saturated final inputs, and we can do this by simply weighting the gradient of the entire layer by a fraction corresponding to the inverse of the number of features in that layer: ie if there are around 1000 features in a given layer, we can divide the gradient of the layer by 1000. This is because the gradient is a linear operator and is therefore additive, meaning that the gradient of an entire layer $z^l$ is equivalent to the gradient of each feature added together,

$$
g = \nabla_a(J(O(a; \theta))) = \nabla_a(z^l) \\
= \nabla_a \left( \sum_f \sum_m \sum_n z^l_{f, m, n} \right) \\
= \sum_f \nabla_a \left( \sum_m \sum_n z^l_{f, m, n} \right)
$$

Therefore the gradient descent update performed may be scaled by the constant $b$ while keeping the same update $\epsilon$ as was used for optimization for an individual feature.  In the example above, $b=1/1000$ and

$$
a_{n+1} = a_n - \epsilon * b * g_n
$$

How then can we hope to make a coherent image if we are adding small gradients from nearly 1000 features?  The important thing to remember is that a single image cannot possibly optimize all features at once.  Consider the following simple example where we want to optimize the output of two features, where the gradient of the first feature $g_0$ for a 2x2 input $a$ is

$$
g_0 = 
\begin{bmatrix}
1 & -1  \\
-1 & 1  \\
\end{bmatrix}
$$

whereas another feature's gradient $g_1$ is

$$
g_1 = 
\begin{bmatrix}
-1 & 1  \\
1 & -1  \\
\end{bmatrix}
$$

now as gradients are additive, the total gradient $g = g_0 + g_1$ is

$$
g = 
\begin{bmatrix}
0 & 0  \\
0 & 0  \\
\end{bmatrix}
$$

which when applied to the original input $a$ will simply yield $a$, so clearly neither feature's activations are optimized.

What this means is that the deep dream procedure gives what can be thought of as a kind of 'average' across all features in the layers of interest.  As each layer must pass the entirety of the necessary information from the input to the output for accurate classification, one can expect for the dream procedure to produce shapes that are most common in the training dataset used, provided the features recognizing these images may be optimized from the input.  This is why deep dream performed on sufficiently deep layers generally introduces animal objects, whereas a dataset trained on landscapes generates images of landscapes and buildings during deep dream ([reference](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)).

We can observe this inability to optimize all features during the dream process. Observe how the average feature activation does not change for many features even after 3 octaves of deep dream:

{% include youtube.html id='l__HrW5spn0' %}

Omitting smoothness has a notable justification: the starting input is a natural image that contains the smoothness and other statistical features of other natural images, so as long as we do not modify this image too much then we would expect to end with something that resembles a natural image.  But if we try to optimize the activation of some output category without smoothness, say to make an image of a 'Junco, Snowbird', we find that only noise has been added to the original image.

![deep dream explanation]({{https://blbadger.github.io}}/neural_networks/flower_directed_nonsmooth_13.png)

This suggests that simply starting with a natural image is not the reason why smoothness can be omitted from gradient descent during deep dream.  Instead, observe how the average feature activation increased smoothly in the last video, and compare that to the discontinuous increases seen in the first video on [this page](https://blbadger.github.io/input-generation.html) (in which gradient descent without smoothness is used to optimize an output category).  As the loss function in both cases is a measure of the values displayed at each time step of those videos, it is apparent that the output of a layer is continous with respect to the input, but the output of the final layer is not.

Discontinuities in the output of the final layer with respect to the input are to be expected for most classification models of sufficient power to approximate a one-to-one function (see [this link](https://blbadger.github.io/nn-limitations.html) for the reason why this is) but it appears that internal layers are not discontinuous with respect to the input.  Exactly why this is the case is currently unclear.

### Enhancements with Dreams

In the previous section it was observed that not all features of the layer of interest were capable of increasing their mean activation during deep dream.  Do certain features tend to exhibit an increase in activation, leading to a similar change in the input image for all starting images or does the image generated (and therefore the features that are activated) depend on the original input?  In other words, if the dream procedure tends to generate an image that is representative of the more common examples in the dataset used to train a network, what influence does the original input image have?  In the original deep dream [article](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html), it was observed that the dream procedure tends to amplify objects that the model's layer of interest 'sees' in original images.  It remains to be seen what influence the input has on a dream, or whether the same objects tend to be amplified no matter what input is given.

We will use GoogleNet as the model for this section. For reference, this model's architecture is as follows:

![annotated googlenet]({{https://blbadger.github.io}}/neural_networks/annotated_googlenet.png)

On [this page](https://blbadger.github.io/input-generation.html) it was observed that a gradient descent on the input was capable of producing images that were confidently predicted to be of one output category.  To test whether or not deep dream enhances characteristics of those categories or chooses new ones, these generated representative images will be used as initial inputs $a_0$.  We apply the 3-octave gradient descent without smoothness or positional jitter, meaning the update is

$$
a_{n+1} = a_n + \epsilon * g_n
$$

 Optimizing layer 4c leads to an increase in the resolution on most representative ImageNet categories, for example on this image for 'Stoplight'

![deep dream stoplight]({{https://blbadger.github.io}}/neural_networks/googlenet_stoplight_4cdream.png)

For some layers it is clear that certain characteristics are introduced irrespective of the original image $a_0$: layer 4a has introduced animal eyes and fur and layer 4b tends to add animal faces whereas layer 5a does not seem to have contributed much beyond some general textures.  Particularly of note is that optimization of layer 4c has enhanced the city features of the original image (note the bridges, doors, clouds, and trees) and is of somewhat higher resolution.

![deep dream stoplight]({{https://blbadger.github.io}}/neural_networks/stoplight_dreams.png)

For another non-animal $a_0$, we see a different result: optimization of layer 4c for 'Volcano' leads to the introduction of surprisingly high-resolution animal faces as well as clouds and other more realistic features

![deep dream stoplight]({{https://blbadger.github.io}}/neural_networks/googlenet_volcano_4cdream.png)

although other layers yield similar additions as for 'Stoplight'

![deep dream stoplight]({{https://blbadger.github.io}}/neural_networks/volcano_dreams.png)

and indeed we can see that the same patterns exist for most other input images, as for example

![deep dream badger]({{https://blbadger.github.io}}/neural_networks/badger_dreams.png)

It is apparent that the final image after deep dream is of higher resolution than the initial, which is partly but not completely (see the next section) because the input generation procedure uses Gaussian blurring, but deep dream does not.  This increased resolution comes at the price of some amount of precision, however, as the resulting image may not match the target category quite as well. Observe how optimizing layer 4c for an input representing 'Lion'

![deep dream lion]({{https://blbadger.github.io}}/neural_networks/googlenet_lion_4cdream.png)

and optimizing layer 4c for an input representing 'Guinea Pig'

![deep dream guinea pig]({{https://blbadger.github.io}}/neural_networks/googlenet_guineapig_4cdream.png)

both lead to a substantial increase in resolution without high-frequency noise but do introduce certain characteristics of animals that are not the target class.

It is interesting that optimizing the activations of a hidden layer of our feed-forward network is capable of increasing the resolution of an input image.  Performing gradient descent using only the output as a target nearly inevitably leads to the presence of high-frequency, incoherent structures unless some form of smoothness or local correlation is applied to the input or else the gradient.

Upon closer investigation of the representations that layer outputs are capable of forming of the input, there appears to be an explanation for why maximizing these early and middle-layer outputs would lead to an increase in resolution.  For ResNet in particular, it was found that training leads to early layer kernals adopting similar weight configurations as those used to sharpen images such that the these layers learn to 'see' the input in more resolution than at the start of training.  If these layers are activated most strongly by sharp edges, it is natural that maximizing their activations would lead to a higher-resolution image. See [here](https://blbadger.github.io/depth-generality.html) for research on layer representations.

### Directed Dream

As we observed in the last section, the dream procedure tends to be somewhat arbitrary with the modifications it makes to a given input: for some it introduces many new objects (usually animal faces) and for others it does not.  Can the dream procedure be directed in order to introduce features that we want?

Perhaps the simplest way to direct a dream towards one object is to perform layer optimization as well as a target optimization.  This can be done using gradient descent on both the layer of interest, whose gradient is $g_l$, as well as on the target with gradient $g_t$.  These gradients usually have to be scaled independently, accomplished by multiplying by tensors of some constants $a$ and $b$,

$$
a_{n+1} = a_n + \epsilon * (ag_l + bg_t)
$$

A bit of experimentation is enough to convince on that this alone does not yield any kind of coherent image, as the same problems experienced for optimizing the gradient of the target output on [this page](https://blbadger.github.io/input-generation.html).  Namely, optimization of an output without smoothness leads to the presence of an adversarial negative with near-certainty, meaning that we cannot direct our dream by simply adding the gradient of the output class. 

Instead we can enforce a small amount of smoothness using Gaussian convolution $\mathcal N$ on the modified image

$$
a_{n+1} = \mathcal{N}(a_n + \epsilon * (ag_l + bg_t))
$$

where \mathcal{N} is applied only every 5 or 10 steps.  Now the dream image usually contains recognizable images of the target class along with additional features that the dream might introduce.  For the target class 'Bubble' and optimizing the activation of Layer 4c, observe how bubbles are present along with a house and some animals

![deep dream bubbles]({{https://blbadger.github.io}}/neural_networks/flower_bubble_dream.png)

Besides introducing some desired element into the dream, it is also apparent that this procedure results in higher resolution than occurs for [image transfiguration](https://blbadger.github.io/input-generation.html) where only the target class is optimized. If we apply the same smoothing procedure but with a newly scaled gradient

$$
a_{n+1} = \mathcal{N}(a_n + \epsilon * (cg_t))
$$

we find that the bubbles resulting are of somewhat lower resolution, mirroring what was observed in the last section of this page.

![deep dream bubbles]({{https://blbadger.github.io}}/neural_networks/flower_bubble_transfiguration.png)

The increase in resolution for directed dreams relative to image transfigurations (using only the gradient of the target class) is most apparent when the target is an animal.  For the target class 'Lion', observe how the directed dream with Layer 4c which is as follows

![deep dream bubbles]({{https://blbadger.github.io}}/neural_networks/flower_lion_dream4c.png)

introduces lions with much higher resolution than transfiguration of the same target.

![deep dream bubbles]({{https://blbadger.github.io}}/neural_networks/flower_lion_transfiguration3.png)

Directed dreams can introduce practically any of the 1000 ImageNet target categories, or a combination of these categories. Observe the result of the dream on Layer 4c with a target class of 'Torch': 

![deep dream torch]({{https://blbadger.github.io}}/neural_networks/googlenet_4c_torch_dream.png)

or 'Junco, Snowbird' (note the birds on the left and right hand side of the image)

![deep dream torch]({{https://blbadger.github.io}}/neural_networks/directed_dream_4c_junco.png)











## Input Representations and Depth

This page is part IV in a series on input generation, follow [this link](https://blbadger.github.io/input-representation.html) for part III.

### Trivial autoencoding ability decreases with model depth

It is worth examining again what was explored in the [part III](https://blbadger.github.io/input-representation.html): an input $a$ is fed to a model $\theta_1$ to make a target output

$$
\widehat{y} = O(a, \theta_1) 
$$

This target output may be understood as the coordinates in a latent space ($\Bbb R^{1000}$ for ImageNet categories), and the gradient of the distance between the target output $\widehat{y}$ and the actual model's output $y$ is minimized via gradient descent.  In effect the model recieves an input and attempts to copy it using only the knowledge of the coordinates of the chosen latent space, which means that gradient descent on the input to match some latent space coordinates can accurately be described as an autoencoding of the input. 

Autoencoders have long been studied as generative models.  The ability of an autoencoder to capture important features of an input without gaining the ability to exactly copy the input makes these models useful for dimensional reduction as well. And it is this feature that we will explore with our classification model-based autoencoder.

Perfect representation requires that a model be able to pass all the input information to the model's final layer, or else an identity function would not be approximated as many inputs could give one output.  Are hidden layers of common deep learning classification models capable of perfect representation?  The method we will use is the capability of each layer to perform an autoencoding on the input using gradient descent on a random vector as our decoder.  First we pass some target image $a_t$ into a feedforward classification network of choice $\theta$ and find the activations of some given output layer $O_l$

$$
\widehat y = O_l(a_t, \theta)
$$

Now that the vector $\widehat y$ has been found, we want to generate an input that will approximate this vector for layer $l$.  The approximation can be achieved using a variety of different metrics but here we choose $L^1$ for speed and simplicity, making the gradient of interest 

$$
g = \nabla_{a_n} \sum_i \lvert \widehat y - O_l(a_n, \theta) \rvert
$$

The original input $a_0$ is a scaled normal distribution 

$$
a_0 = \mathcal {N}(0.7, 1/20)
$$

and a Gaussian convolution $\mathcal{N_c}$ is applied at each gradient descent to enforce smoothness.

$$
a_{n+1} = \mathcal{N_c} (a_n - \epsilon * g)
$$

Thus the feed-forward network encoder may be viewed as the forward pass to the layer of interest, and the decoder as the back-propegation of the gradient combined with this specific gradient descent procedure.

First we will investigate the ResNet family of models which have the following architecture:

![Resnet layer autoencoding]({{https://blbadger.github.io}}/neural_networks/resnet_architecture.png)

We will first focus on the ResNet 50 model.  Each resnet module has a residual connection, which is formed as follows:

![Resnet layer autoencoding]({{https://blbadger.github.io}}/neural_networks/residual_connection_diagram.png)

Now we will test for each layer's ability to represent the input using our autoencoding method.  We find that early layers are capable of approximately copying the input, but as depth increases this ability diminishes and instead a non-trivial representation is found.

![Resnet layer autoencoding]({{https://blbadger.github.io}}/neural_networks/resnet_autoencoding_perlayer.png)

For each layer, the image formed can be viewed as a result of a trivial (ie approximate copy) and non-trivial (not an approximate copy) representation of the input. In particular, observe the pattern of spots on the dog's fur.  Trivial representations would exactly copy this patter whereas non-trivial ones would not unless this precise patter were necessary for identification (and it is not).  Intuitively a trivial representation would not require any model training to approximately copy the input, as long as the decoder function is sufficiently powerful.  Thus we can search for the presence of trivial representations by repeating the above procedure but for an untrained version of the same model.

![Resnet layer autoencoding]({{https://blbadger.github.io}}/neural_networks/resnet_autoencoding_perlayer2.png)

Thus we see that the untrained (and therefore necessarily trivial) representation of the input disappears in the same deeper layers that the learned (and in this case non-trivial) representation are found for trained models. 

### Imperfect representations are due to nonunique approximation

Why do deep layers of ResNet50 appear to be incapable of forming trivial autoencodings of an input image?  Take layer Conv5 of an untrained ResNet50. This layer has more than 200,000 parameters and therefore viewing this layer as an autoencoder hidden layer $h$ would imply that it is capable of copying the input exactly, as the input has only $299*299=89401$ elements.  

To understand why a late layer in an untrained deep learning model is not capable of very accurate input representation, it is helpful to consider what exactly is happening to make the input representation.  First an input $a$ is introduced and a forward pass generates a tensor $y$ that is the output of a layer of interest in our model,

$$
y = O(a, \theta)
$$

This tensor $y$ may be thought of as containing the representation of input $a$ in the layer in question.  To observe this representation, we then perform gradient descent on an initially randomized input $a_0$ 

$$
a_{n+1} = a_n - \epsilon \nabla_{a_n} m \left( O(a_n, \theta), O(a, \theta) \right)
$$

in order to find a generated input $a_g$ such that some metric $m$ between the output of our original image and this generated image is small.  We employ the $L^1$ metric for gradient descent minimization, in which case the measure is

$$
m = \sum_i \vert O(a_g, \theta)_i - O(a, \theta)_i \vert
$$

and typically employ the $L^2$ norm of the vector difference between representations $O(a_g, \theta)$ and $O(a, \theta)$ (with a reference value) as a way of observing how effective our miminization procedure was.  The $L^2$ norm between output and target output is denoted as $\vert \vert O(a_g, \theta) - O(a, \theta) \vert \vert_2$ and is defined as follows:

$$
L^2 = \sqrt{\sum_i \left(O(a_g, \theta)_i - O(a, \theta)_i \right)^2}
$$

One can think of each step of the representation visualization process as being composed of two parts: first a forward pass and then a gradient backpropegation step.  An inability to represent an input is likely to be due to one of these two parts, and the gradient backpropegation will be considered first.

It is well known that gradients tend to scale poorly over deeper models with many layers.  The underlying problem is that the process of composing many functions together makes finding an appropriate scale for the constant of gradient update $\epsilon$ very difficult between different layers, and for some instances for multiple elements within one given layer.  Batch normalization was introduced to prevent this gradient scale problem, but although effective in the training of deep learning models it does not appear that batch normalization actually necessarily prevents gradient scaling issues. 

However, if batch normalization is modified such that each layer is determined by variance factor $\gamma = 1$ and mean $\beta = 0$ for a layer output $y$,

$$
y' = \gamma y + \beta
$$

Then the gradient scale problem is averted.  But the initialization process of ResNet50 does indeed set the $\gamma, \beta$ parameters to $1, 0$ respectively, meaning that there is no reason why we would expect to experience problems finding an appropriate $\epsilon$.  Futhermore, general statistical measures of the gradient $\nabla_a O(a, \theta)$ are little changed when comparing deep to shallow layers, suggesting that the gradient used to update $a_n$ is not why the representation is poor.  
Thuse we consider whether the forward pass is somehow the culprit of our poor representation.  We can test this by observing whether the output of our generated image does indeed approximate the tensor $y$ that we attempted to approximate: if so, then the gradient descent process was successful but the forward propegation loses too much information for an accurate representation.

We can test whether the layer output $O(a, \theta)$ approximates $y$ using the $L^2$ metric above, thereby findin the measure of the 'distance' between these two points in the space defined by the number of parameters of the layer in question.  Without knowing the specifics of the values of $\theta$ in all layers leading up to the output layer, however, it is unclear what eactly this metric means, or in other words exactly how small it should be in order to determine if an output really approximates the desired $y$.  

Therefore a comparative metric is introduced: a small (ie visually nearly invisible) shift is made to the original input $a$ and the resulting $L^2$ metric between the output of the original image $y = O(a, \theta)$ and the output of this shifted image

$$
m_r = ||O(a, \theta) - O(a', \theta)||_2
$$

is obtained as a reference.  This reference $m_r$ can then be compared to the metric obtained by comparing the original output to the output obtained by passing in the generated image $a_g$

$$
m_g = ||O(a, \theta) - O(a_g, \theta)||_2
$$

First we obtain $a'$ by adding a scaled random normal distribution to the original,

$$
a' = a + \mathcal N (0, 1/25)
$$

and then we can obtain the metrics $m_r, m_g$ in question.

![Resnet layer distances]({{https://blbadger.github.io}}/neural_networks/layer_distances.png)

Observe that an early layer (Conv2) seems to reflect what one observes visually: the distance between $a', a$ is smaller than the distance between $a_g, a$ as $a'$ is a slightly better approximation of $a$.  On the other hand, the late layer Conv5 exhbits a smaller distance between $a_g, a$ compared to $a', a$ which means that according to the layer in question (and assuming smoothness), there generated input $a_g$ is a better approximation of $a$ than $a'$.

It may be wondered whether this phenomenon is seen for other architectures or is specific to the ResNet50 model.  Typical results for an untrained ResNet18,

![Resnet layer distances]({{https://blbadger.github.io}}/neural_networks/layer_distances_resnet18.png)

and an untrained ResNet152

![Resnet layer distances]({{https://blbadger.github.io}}/neural_networks/layer_distances_resnet152.png)

show that early and late layer representations both make good approximations (relative to a slightly shifted $a'$) of the input they attempt to approximate, even though the late layer representations are visually clearly inaccurate.  Furthermore, observe how the representation becomes progressively poorer at Layer Conv5 as the model exhibits more layers.  These results suggest that in general layer layers of deep learning models are incapable of accurate (trivial) untrained representation of an input not because the gradient backpropegation is necessarily inaccurate but because forward propegation results in a non-unique approximations to the input.

It may be wondered if a better representation method could yield a more exact input. In a later section, we will see that the $a'$ reference is atypically close to $a$ compared to other points in the neighborhood of $O(a', \theta) - O(a, \theta)$ of $O(a, \theta)$ and thus may not be an ideal reference point.  To avoid the issue of what reference in output space to use, we can instead observe the output metric $m_g$ anc compare this to the corresponding metric on the input,

$$
m_i = || a_g - a ||
$$

The $m_g$ metric corresponds to the representation accuracy in the output space and $m_i$ corresponds to the representation accuracy with respect to the input.  Therefore we can think of $m_g$ as being a measure of the ability of the gradient descent procedure to approximate $O(a, \theta)$ while $m_i$ is a measure of the accuracy of the representation to the target input.

For even relatively shallow layers there may exist an increase in $m_i$ while $m_g$ tends towards 0. For ResNet layer Conv2, we see the latter behavior for a fixed $\epsilon$

![Resnet50 layer distances]({{https://blbadger.github.io}}/neural_networks/resnet50_conv2_limitations.png)

while for ResNet152 layer Conv2, we find an asymptote of $m_i$ far from 0 while $m_g$ heads towards the origin for a variable (linearly decreasing) $\epsilon$.

![Resnet152 layer distances]({{https://blbadger.github.io}}/neural_networks/resnet152_conv2_limitations.png)

For deeper layers this effect is more pronounced: it is common for $m_i$ to increase while $m_g$ tends towards the origin for layer conv5 of resnet50.

![Resnet layer distances]({{https://blbadger.github.io}}/neural_networks/resnet50_conv5_limitations.png)

### Why depth leads to nonunique trivial representations

From the previous few sections, it was seen first that deeper layers are less able to accurately represent an input image than earlier layers for untrained models, and secondly that this poor representation is not due to a failure in the input gradient descent procedure used to visualize the representation but instead results from the layer's inability to distinguish between very different inputs $(a, a_g)$.  It remains to be explored why depth would relate to a reduction in discernment between different inputs.  In this section we explore some contributing factors to this decrease in accuracy from a theoretical point of view before considering their implications to model architectures.

The deep learning vision models used on this page are based on convolutional operations.  For background on what a convolution is (in the context of image processing) and why it is useful for deep learning, see [this page](https://blbadger.github.io/neural-networks.html#convolutions-explained). 

What is important for this page is that these models make use of mathematical operations that are in general non-invertible.  For convolutions themselves, given some input tensor $f(x, y)$, the convolution $\omega$ may be applied such that

$$
\omega f(x, y) = f'(x, y)
$$

does not have a unique inverse operation 

$$
\omega^{-1}f'(x, y) = f(x, y)
$$

Specifically, a convolutional operation across an image is non-invertible with respect to the elements that are observed at any given time.  Thus, generally speaking, pixels within the kernal's dimensions may be swapped without changing the ouptut.

To see why this is, consider a simple example: a simple case of a one-dimensional convolution of shape with a uniform kernal. This kernal is 

$$
\omega = 
\frac{1}{2}
\begin{bmatrix}
1 & 1
\end{bmatrix}
$$

such that the convolutional operation is 

$$
\omega * f(x_1, y_1) =
\frac{1}{2}
\begin{bmatrix}
1 & 1
\end{bmatrix} 
*
\begin{bmatrix}
2 \\
3 \\
\end{bmatrix}
$$

$$
\omega * f(x_2, y_2) = 1/2 (1 \cdot 2 + 1 \cdot 3) = 5/2 \\
$$

Now observe that if we know the output of the convolutional operation and the kernal weights we cannot solve for the input: an infinite number of linear combinations of 2 and 3 exist that satisfy a sum of $5/2$. Invertibility may be recovered by introducing padding to the input and scanning over these known values, but in general only if the stride length is $1$.

In the more applicable case, any convolutional operation that yields a tensor of smaller total dimension ($m \mathtt{x} n$) than the input is non-invertible, as is the case for any linear operation.  Operations that are commonly used in conjunction with convolutions (max or average pooling, projections, residual connections etc.) are also non-invertible. 

Because of this lack of invertibility, there may be many possible inputs for any given output.  As the number of non-invertible operations increases, the number of possible inputs that generate some output vector also increases exponentially.  Therefore it is little wonder why there are different input $a_g, a, a'$ that all yield a similar output given that many input can be shown to give one single output even with perfect computational accuracy. 

One can experimentally test whether or not non-uniqueness leads to poor representations using simple fully connected architectures.  It should be noted that nearly all fully connected architectures used for classification are composed of non-invertible operations that necessarily lead to non-uniqueness in representation. Specifically, a forward pass from any fully connected layer $x$ to another that is smaller than the previous, $y$, is represented by a non-square matrix multiplication operation. Such matricies are non-invertible, an in particular the case above is expressed with a matrix $A_{m\mathrm{x}n}, m < n$ such that there are an infinite number of linear combinations of elements of $x$.

$$
y = Ax \\
A^{-1}y = x
$$

Whithin a specific range, there are only a finite number of linear combinations but this number increases exponentially upon matrix multiplication composition, where

$$
y = ABCDx \\
$$

This theory is borne out in experimentation, where it appears impossible to make a unique trivial representation of an input of size $a$ with a layer of size $b < a$. In the following figures, the representation visualization method has been modified to have a minimum learning rate of 0.0001 and Gaussian normalization has been removed.  Inputs have been down-scaled to 29x29 for fully connected model feasibility.

![covercomplete depth and representation]({{https://blbadger.github.io}}/neural_networks/under_versus_overcomplete.png)

From the above figure, it may be argued that perhaps the representation generation algorithm is simply not strong enough to capture an accurate representation of the input for smaller layers.  This can be shown to be not the case: plotting the representation accuracies achieved using various iterations of our representation generator, we find that only models with a minimum layer size greater than the input dimension are capable of making arbitrarily accurate representtions.  For the below figure, note that if we continue to trade the 3000-width model, an exponential decay is still observed such that very low (0.1) distance is achieved after 200,000 iterations. 

![iterations and width]({{https://blbadger.github.io}}/neural_networks/width_increased_iterations.png)

On the other hand, the 2000-width model has no decrease in distance from 1,000 to 100,000 iterations even as the embedding distance follows an exponential decay.  These observations provide evidence for the idea that the input representation quality is poor for models with at least one layer with fewer nodes than input elements because of non-uniqueness rather than poor output approximation.

![iterations and width]({{https://blbadger.github.io}}/neural_networks/middle_width_accuracy.png)

But something unexpected is observed for fully connected architectures in which all layers are the same size and identical (or larger than) the input: increased depth still leads to worse representational accuracy for any given number of iterations of our representation visualization method.  Note that increasing the number of iterations in the representation visualization method (from 500 to 10,000 in this case) is capable of compensating for increased depth.

![covercomplete depth and representation]({{https://blbadger.github.io}}/neural_networks/overcomplete_depth_different_layers.png)

How could this be?  Each layer is uniquely defined by the last, so non-uniqueness is no longer an issue.  And indeed, if we increase the number of iterations of our gradient descent method for visualization the representation does indeed appear to approximate an input to an arbitrary degree. To be precise, therefore, it is observed for deep models that there are two seemingly contradictory observations: 

$$
m_g = ||O(a, \theta) - O(a_g, \theta)||_2 \\
m_{a'} = ||O(a, \theta) - O(a', \theta)||_2
$$

we can find some input $a_g$ such that

$$
m_{a'} > m_g 
\tag{1}
$$

but for this input $a_g$, 

$$
|| a - a' ||_2 < || a - a_g ||_2
\tag{2}
$$

which is an exact way of saying that the representation is worse even though it makes as good an approximation of $O(a, \theta)$ as $a'$.  

Ignoring biases for now, fully connected linear layers are equivalent to matrix multiplication operations.  If the properties of composed matrix multiplication are considered, we find that there is indeed a sufficient theory as to how this could occur.  Consider first that a composition of matrix multiplications is itself equal to another matrix multiplication,

$$
ABCDx = Ex
$$

Now consider how this multiplication transforms space in $x$ dimensions.  Some basis vectors end up becoming much more compressed or expanded than others upon composition.  Consider the case for two dimensions such that the transformation $Ex$ shrinks $x_1$ by a factor of 1000 but leaves $x_2$ unchanged.  Now consider what happens when we add a vector of some small amount $\epsilon$ to $x$ and find 

$$
|| E(\epsilon) ||
$$ 

the difference of between transformed points $x$ and $x + \epsilon$.  We would end up with a value very near $\epsilon_2$.  For example, we could have 

$$ 
\epsilon = 
\begin{bmatrix}
1 \\
1 \\
\end{bmatrix}
$$

But now consider all the possible inputs $a$ that could make 

$$
|| E(a) || \approx || E(\epsilon) ||
$$

If we have the vector

$$ 
a = 
\begin{bmatrix}
1000 \\
0 \\
\end{bmatrix}
$$

For the choice of an $L^1$ metric, clearly

$$
||E(x - a)|| = 1 < ||E(x - \epsilon)|| = 1.001
$$ 

even though 

$$
||x - a|| = 1000 > 2 = ||x - \epsilon||
$$  

This example is instructive because it shows us how equations (1) and (2) may be simultanously fulfilled: all we need is a transformation that is contractive much more in some dimensions rather than others.  Most deep learning initializations lead to this phenomenon, meaning that the composition of linear layers gives a transformation that when applied to an n-dimensional ball as an input gives a spiky ball, where the spikes correspond to dimensions that are contracted much more than others.

For an illustration of how this can occur in four dimensions, take an instance where two dimensions denoted in blue that exist on the plane of the page and two more are denoted in red, one of which experiences much more contraction than the other three.  The points that will end up within a certain $L^2$ distance from the target $E(x)$ are denoted for in different dimensions by color.  Two more points are chosen, one in which a small amount $\epsilon$ is added to $a$ and another which could be generated by gradient descent, $a_g$. Observe how the mapping leads to an inversion with respect to the distance between these points and $a$

![spiky ball explanation]({{https://blbadger.github.io}}/neural_networks/spiky_ball_explanation.png)

Why would a representation visualization method using gradient descent tend to find a point like $a_g$ that exists farther from $a$ than $a'$?  We can think of the gradient descent procedure as finding an point $E(a_g)$ as close to $a$ as possible under certain constraints. The larger the difference in basis vector contraction that exists in $E$, the more likely that the point found $E^{-1}(a_g) = a_g$ will be far from $a$.  

As the transformation $E$ is composed of more and more layers, the contraction or expansion difference (sometimes called the condition number) between different basis vectors is expected to become larger for most deep learning initialization schemes.  As input representation method is very similar to the gradient descent procedure of training model parameters, poor conditioning leading to a poor input representation then it likely also leads to poor parameter updates for the early layers as well.  Conditioning can be understood as signifying approximate invertibility: the poorer the conditioning of a transformation, the more difficult it is to invert accurately.

In some respects, $a'$ provides a kind of lower bound to how accurate a point at distance $E(a') - E(a)$ could be.  Observe in the figure above how small a subset of the space around $E(a)$ that $E(a')$ exists inside. Therefore if one were to choose a point at random in the neighborhood of $E(x)$, a point like $E(a')$ satisfying the specific conditions that it does is highly unlikely to be chosen.  

This an be investigated experimentally. Ceasing to ignore biases, we can design a model such that each layer is invertible by making the number of neurons per layer equal to the number of elements in the input.  We design a four-layer network of linear layers only, without any nonlinearities for simplicity.  For each layer, any output $o$ will have a unique corresponding input $x$ that may be calculated by multiplying the output minus the bias vector by the inverse of the weight matrix.

$$
o = Wx + b \implies \\
x = W^{-1}(o-b)
$$

Inverting large matricies requires a number of operations, and it appears that the 32-bit `torch.float` type is insufficient for accurate inversion for the matricies used above.  Instead it is necessary to use `torch.double` type elements, which are 64-bit floating point numbers.  Once this is done, it can be easily checked that the inverse of $O(a, \theta)$ can be found.

With this ability in hand, we can investigate how likely one is to find a point within a certain distance from a target $O(a, \theta)$, denoted $O(a'', \theta)$ such that the input is within some distance of $a$.  We can do this by finding random points near $O(a, \theta)$ by 

$$
O(a'', \theta) = O(a, \theta) + \mathcal{N}(0, 1/1000)
$$

and we can compare the distances between $a''$ and $a$ to the distance between $a'$ and $a$.  The latter is denoted in the upper portion of the following figure, and the distribution of the former in the lower portion.  Observe how nearly all points $a''$ are much farther from $a$ (note the scale: the median distance if more than 40,000) than $a'$ (which is under 3).  This suggests that indeed $a'$ is unusually good at approximating $a$ for points in the neighborhood of $O(a, \theta)$, which is not particularly surprising given that $a'$ was chosen to be a small distance from $a$.

What is more surprising is that we also find that the gradient descent method for visualizing the representation of the output is also far more accurate to the orginal input $a$ than almost all other points in the neighborhood.  In the below example, a short (220 iterations) run of the gradient descent method yields an input $a_g$ such that $m(a, a_g)=2.84$ for an $L^2$ metric but $m(O(a, \theta), O(a_g, \theta)) = 0.52$ with the same metric, which is far larger in output space than the neighborhood explored above but far smaller in input space. 

![inverted distances]({{https://blbadger.github.io}}/neural_networks/inverted_distances.png)

It is worth exploring why gradient descent would yield an input $a_g$ that is much closer to $a$ than nearly all other input $\{ a'' \}$ within a certain radius.  Upon some close inspection, there is a clear explanation for this phenomenon: nearly all elements of $\{ a'' \}$ contian pixels that are very far from the desired range $[0, 1]$ and therefore cannot be accurate input representations.  Note that our gradient descent procedure was initialized with all pixels $a_0 \in \mathcal(0.7, 1/20)$ which is near the center of $[0, 1]$. Therefore it is no surprise that gradient descent starting with $a_0$ leads to an input representation $a_g$ that is much closer to the target input $a$, given that each element of the target input $a_{nm}$ also exhibits $a_{nm} \in [0, 1]$.

### Theoretical lower bounds for perfect input representation

How many neurons per layer are required for perfect representation of the input? Most classification models make use of Rectified Linear Units (ReLU), defined as

$$
y = f(x) =
\begin{cases}
0,  & \text{if $x$ $\leq$ 0} \\
x, & \text{if $x$ > 0}
\end{cases}
$$

In this case, the number of neurons per layer required for non-uniqueness is usually much greater than the number of input elements, usually by a factor of around 2.  The exact amount depends on the number of neurons that fulfill the first if condition in the equation above, and if we make the reasonable assumption that $1/2$ of all neurons in a layer do get zeroed out then we would need twice the number of total neurons in that layer compared to input features in order to make an arbitrarily accurate representation.

This increase in the number of neurons per layer is true not just for the first but also for all subsequent layers if ReLU activation is used.  This is because inverting any given layer requires at least as many unique inputs as there are previous neurons. For a probability $p$ that some chosen neuron will satisfy $y > 0$, assuming that this probability is constant for all layers we have:

![necessary widths for completeness]({{https://blbadger.github.io}}/neural_networks/overcomplete_architectures.png)

It should be noted that this phenomenon is extremely general: it applies to convolutional layers as well, and also approximately to other nonlinear activations such as tanh or sigmoid neurons.  That said, however, it is by no means a given that a model will not learn to adopt some special configuration, perhaps $p=0$ such that there are no zero-valued activations per layer.

Input representations require a substantial amount of information from the gradient of the representation with respect to the input in order to make an accurate representation visualization.  This means that one would ideally want to observe $a_g$ after a huge number of gradient descent steps, but for practicality iterations are usually limited to somewhere in the hundreds. 

But curiously enough there is a way to reduce the number of steps necessary: add neurons to the later layers.  Experimentally, increasing the number of neurons in these layers leads to a more accurate representation.  As this cannot result from an increase in information during the forward pass, it instead results from a more accurate gradient passed to the input during backpropegation. 

![middle layer influence]({{https://blbadger.github.io}}/neural_networks/gradient_middle_layer.png)

It is interesting that increasing the number of deep layer neurons is capable of leading to a better input representation for a deep layer even for overcomplete architectures with more layer neurons than input elements. It is probable that increased deep layer neurons prevent scaling problems of gradients within each layer.

In conclusion, poor representation of an input may be due to non-uniqueness caused by non-invertible functions commonly used in models in addition to poor conditioning (which can be thought of as approximate non-invertibility) resulting in difficulties of sufficiently approximating $O(a, \theta)$.  For ResNet, it appears that the non-uniqueness phenomenon is the root of most of the inaccuracy in deep layers' representations due to the observation that input distance tends to increase while embedding distance decreases upon better and better embedding approximation.  

There one final piece of evidence for non-uniqueness being the primary cause of poor representation.  Observe that the representation for ResNet50 layer Conv1 before batchnorm and max pooling (trained or untrained) is near-perfect, whereas the representation after applying batchnorm and more importantly pooling is not (especially for the untrained model).  This is precisely what is predicted by the non-uniqueness theory, as the first convolutional layer input contains $299x299x3 = 268,203$ elements and the output has over one million and thus is invertible, but the addition of max pooling leads to non-invertibility.

### The effect of training on layer approximation accuracy

What happens to the poor representations in deeper layers upon model training?  We have already seen that training leads to the formation of what was termed a non-trivial representation, ie something that is not simply an approximate copy of the input.  The human eye can distinguish recognizable features of this representation if Gaussian blurring is used during gradient descent. It may be illuminating to investigate how training changes or does not change the ability of a deep layer to represent an input.

As it is extremly unlikely that training would lead to the transformation of $O(a, \theta)$ from the spiky ball geometry to a non-spiky ball (ie lower the condition number ro near unity), and it is not possible for training to remove the non-uniqueness of a representation for any given output, one would expect for the same disconnect between embedding and input distances upon increased iterations of gradient descent.  Indeed this is observed, as for example for layer Conv5 of a trained ResNet50 exhibits the same inverse correlation between input and embedding distances as the untrained ResNet50 model does.

![trained approximations]({{https://blbadger.github.io}}/neural_networks/ResNet50_trained_conv5_distance.png)

Therefore training does not change the underyling inability in forming an accurate representation of an input for deeper layers.  But it may change the ease of approximating those representations. Successful training leads to a decrease in some objective function $J(O(a, \theta))$ such that some desired metric on the output is decreased. It may be hypothesized that training also leads to a decrease in the distance between the representation of the generated input $a_g$ and the representation of the actual input $a$ for some fixed number of iterations of our representation method.  For representations without Gaussian convolutions applied at each step, this does appear to be the case.

![trained approximations]({{https://blbadger.github.io}}/neural_networks/resnet50_untrained_vs_trained.png)

Upon close examination of the trained versus untrained early and middle layers of ResNet50, however, training leads to a representation of the input that is noticeably sharper than for the untrained case.  It may be wondered if representations of images that would not be recognized by a model trained on Imagenet would also be sharpened or not.  This may be explored by observing the clarity of representations $a_g$ for an image of a [Tesla Coil](https://blbadger.github.io/tesla-coils.html) at different layers. We find that indeed input representations early and middle layers of ResNet50 are sharpened upon training.

![tesla representation]({{https://blbadger.github.io}}/neural_networks/tesla_representation.png)

This observation gives a hypothesis as to why the feature maps for early layers are repeating pattern, as observed [here](https://blbadger.github.io/feature-visualization.html#mapping-resnet-features).  The parameters of these layers are capable of learning to restrict the possible representations of the input to be sharp images that retain most of the detail in the input.  

It has ben observed that kernals of convolutional models applied to a wide range of image-recognition tasks tend to form Gabor filters.  These filters are the product of sinusoidal functions with Gaussians and are used for (among other purposes) edge detection.  It has been suggested that these filters allow early convolutional layers to perform selection for simple features such as edges, which is followed by selection for objects of greater abstractions in later layers.

This perspective undoubtedly has merit, but we can provide a different one: early layers learn convolutional weights that serve to restrict the possible inputs that could yield a similar representation in that layer, which leads to the visualization of that representation becoming visually sharpened and more clear in certain areas of the input.  What is sharpened and what is not can be gradually selected for in each layer such that the model only allows information to pass to the output if it is important for classification.

In conclusion, the generated input representation $a_g$ does indeed change noticeably during training, but that this change does not affect the tendancy for deep layers to lack uniqueness in their representations.  Indeed this is clear from the theory expoused in the last section, as the convolutional operation remains non-invertible after training and the spiky ball geometry would not necessarily be expected to disappear as well.

Instead, it appears that during training the possible inputs that make some representation close to the target tensor are re-arranged such that the important pieces of information (in the above case the snout and nose of a dog) are found in the representation, even as non-uniqueness remains.  And indeed this would be expected to be helpful for the task of input classification, as if the classification output for some hidden layer's activation vector $h = O(a, \theta)$ is correct then all inputs that map to this value $h$ would also be correct.

### Late layer representations are mostly inference

In various cases it is clear that early convolutional layers learn to more precisely represent the input, whereas later layers introduce new information as to what the input should be (rather than is).  This is apparent for the image of the Dalmatian in which the spot pattern changes noticeably in middle layers and in later layers that exaggerate the snout.  

This question may be addressed by training ResNet50 to recognize images of random noise. In this experiment, 10,000 images of random scaled Gaussian noise $\mathcal{N}(1/2, 1/4)$, each sample labelled as one of 100 categories.  As seen in other work, deep learning vision models are somewhat surprisingly capable of accurately classifying these images of noise.  An example input is as follows:

![tesla random representation]({{https://blbadger.github.io}}/neural_networks/scaled_gaussian_noise.png)

It is clear that attempting to classify thousands of such images requires memorizing each one, which is extremely difficult to do for the human visual system, although it is abundantly easy for deep learning vision models such as ResNet. This suggests that the process of image recognition is fundamentally different for natural images compared to random ones, and we might expect that difference to manifest itself in the representations that a random-trained model forms of a natural image.

Applying gradient descent to form input representations of the embeddings at various layers for a random-trained ResNet50, we see that indeed the information is much different than for ResNet50 trained on ImageNet.  The process of generating an input is noticably changed: applying a Gaussian convolution to smooth the representation results in inputs $a_g$ that do not approximate the input's embedding as well as $O(a', \theta)$. To be specific, 

$$
||O(a, \theta) - O(a_g, \theta)|| > ||O(a, \theta) - O(a', \theta)||
$$ 

for $\theta$ trained on noise and $a_g$ generated using Gaussian convolutions at each gradient update step, with $a'$ being the shifted version of $a$ noted in previous sections.

This contrasts with input representations for ResNet50 trained on ImageNet in which it is straightforward to find an $a_g$ such that 

$$
||O(a, \theta) - O(a_g, \theta)|| < ||O(a, \theta) - O(a', \theta)||
$$ 

even for a limited number of gradient descent steps.  But when one considers the dataset that $\theta$ was trained on, this finding is not surprising: the training input are not smooth and unlike natural images would be poorly approximated by Gaussian-convolved versions. 

Commensurately, if apply the gradient descent procedure without Gaussian convolution we find that obtaining a $a_g$ that yields an output that approximates $O(a, \theta)$ better than $O(a', \theta)$ is not difficult.  But the results look nothing like the case for models trained on imagenet: layers Conv3 through Conv5 contain almost no information on the input but rather infer that it must resemble the random training data seen by this model.

![tesla random representation]({{https://blbadger.github.io}}/neural_networks/random_representations_nogaussian.png)

It can also be appreciated that the early layer representations are not noticeably clearer after training on random inputs compared to the untrained model. In the following figure Gaussian convolution was performed at each gradient descent step for consistency.

![tesla random representation]({{https://blbadger.github.io}}/neural_networks/random_representations.png)

From these observations can be drawn two conclusions: first that representations in later layers reflect the training dataset's content far more than the input itself, and second that the tendancy of early layer representations to become visually sharpened (via restriction of possible feasible inputs) is specific to tasks using natural images rather than existing in every trained model regardless of training dataset.

### Implications of imperfect input representation

For models applied to the task of classification, particularly where there are fewer classification categories than input elements, then necessarily all the information from the input does not reach the output such that backpropegation cannot instruct the network to exactly copy an input (in the general case, barring certain weight and bias configurations).  But for hidden state models or for classification in which there are many more output elements than inputs, exact representation becomes important.

The theory developed on this page also gives an explanation for the common architecture for autoencoders. Typically these seek to avoid copying the input to the output and thus have fewer elements in the latent space than inputs.  On the other hand, they also typically do not want to lose information between the latent space and output, which may be why an increasing-size architecture has been found to be so successful.

The theory of representation accuracy goes some way towards explaining the types of architectures that have been found to be successful in a variety of tasks in recent times as well.  Transformers use stacks of self-attention and feed-forward fully connected units to create representations of inputs, both of which are typically non-invertible (for example see that a single self-attention output vector value can be any linear combination of softmax outputs of $q*k$ values at each input).  Alternatives to transformers that have also proven effective in recent years combine fully connected subcomponents in non-invertible methods (for example the mlp-mixer architecture's output can be a linear combination of any given weight vector). 
It remains to be seen, however, whether or not vision transformers and MLP-mixers (and in general any patch-based method) tend to learn to sharpen the input before mapping it to a manifold as occurs for convolutional models.



## Diffusion Inversion Generative Models

An exploration of diffusion (aka denoising diffusion probabalistic) models. 

### Introduction with Denoising Autoencoders

Suppose one were to wonder how to de-noise an image that was corrupted some small amount.  One way to approach this problem is to specify what exactly noise looks like such that it may be filtered out from an input. But if the type of noise were not known beforehand, it would not be possible to identify exactly what the noisy versus original parts of the corrupted image were.  We therefore need some information on what the noise compared to the original image is, given samples of the original input $x$ and the corrupted samples $\widehat{x}$, it may be difficult to remove such noise in the general case.

As is often the case for poorly defined problems (in this case finding a general de-noiser), another way to address the problem of noise removal is to have a machine learning program learn how to accomplish noise removal in a probabalistic manner.  A trained program would ideally be able to remove each element of noise with high likelihood.

One of the most effective ways of removing noise probabalistically is to use a deep learning model as the denoiser.  Doing so allows us to refrain from specifying what the noise should look like, even in very general terms, which allows for a greater number of possible noise functions to be approximated by the model.  Various loss functions may be used for the denoising autoencoder, one of which is Mean Squared Error (MSE),

$$
L = || x - g(f(\widehat{x})) ||^2_2  \\
L = || x - g(h) ||^2_2
$$

where $f(x)$ signifies the encoder which maps corrupted input $\widehat{x}$ to the hidden state $h$, also known as the 'code', and $g(h)$ maps this hidden state to the output.

In order to be able to successfully remove noise from an input, it is necessary for a model to learn something about the distribution of all uncorrupted inputs $p(x_i)$.  Intuitively, the more a model is capable of recovering $p(x_i)$ the more it is capable of removing an arbitrary amount of noise.

### Introduction to Diffusion Inversion

Now suppose that instead of recovering an input that has been corrupted slightly, we want instead to recover an input that was severely corrupted and is nearly indistinguisheable from noise.  We could attempt to train our denoising autoencoder  on samples with larger and larger amounts of noise, but doing so in most cases does not yield even a rough approximation of the desired input.  This is because we are asking the autoencoder to perform a very difficult task because most images that one would want to reconstruct are really nothing like a typical noise sample at all. 

The property that natural images are very different from noise is the motivation behind denoising diffusion models, also called denoising diffusion probabalistic modeling or simply diffusion models for short, originally introduced by [Sohl-Dickinson and colleagues](https://arxiv.org/pdf/1503.03585.pdf).  

To make the very difficult task of removing a large amount of noise from a distribution approachable, we can break it into smaller sub-tasks that are more manageable.  This is the same approach taken to optimiziation of any deep learning algorithm: finding a minimal value via a direct method is intractable, so instead we use gradient descent and the model will learn how many small steps towards a minimal point in the loss function space.  Here we will teach a denoising autoencoder to remove both small and larger amount of noise from a corrupted input so that samples may be generated over many steps in order the reconstruct an input from pure noise.  

### Diffusion Theory

Diffusion inversion is defined on a forward diffusion process, 

$$
q(x_t | x_{t-1})
$$ 

where $x_t$ signifies the input at time step $t$ in the diffusion process from 0 to the final time step $T$, ie $t \in {O, 1, ... T}$.  Therefore $x_0, x_1, ..., x_T$ are latent variables of the same size as the input.  This forward diffusion process is fixed and adds Gaussian noise at each step, such that for a fixed variance amount (known as a variance 'schedule') $\beta_t$ we can find the next forward diffusion step as follows:

$$
q(x_t | x_{t-1}) = \mathcal {N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t \mathbf(I))
$$

The variance schedule is typically linear, with later work finding that a cosine schedule is sometimes more effective. Rather than compute $q(x_t \vert x_{t-1})$ by iterating the above equation the necessary number of times, there is happily a closed form 

$$
q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar \alpha_t}x_0, (1-\bar \alpha_t)\mathbf{I})
$$

where $\alpha_t = 1 - \beta_t$ and $ \bar \alpha_t = \prod _ {s=1}^T \alpha_s$ is the cumulative variance by the end point $T$.

Inverting the diffusion process is equivalent to starting with pure noise $x_T$ and ending with a sample $x_0$.  

$$
p_{\theta} (x_0) = \int p(x_{0:T}) dx_{1:T}
$$

With the (pure noise) starting point $x_T = \mathcal{N} (x_T; 0, I)$, a single step in the inverse diffusion process is as follows:

$$
p_{\theta}(x_{t-1} | x_t) = \mathcal{N} \left ( x_{t-1}; \mu_{\theta} (x_t, t), \Sigma_{\theta}(x_t, t) \right )
$$

Training the model $\theta$ involves estimating one (or more) of three quantities: the mean $\mu_{\theta} (x_t, t)$, variance $\mu_{\theta} (x_t, t)$, or else the Gaussian noise distribution $\epsilon \sim \mathcal{N}(0, I)$ which was introduced by [Ho and colleagues](https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf).  Ho and colleagues innovated by showing that superior empricial results could be obtained by training on a variant of the variational lower bound in which the model $\epsilon _ \theta$ attempts to learn the noise distribution $\epsilon$ via MSE loss,

$$
L = || \epsilon  - \epsilon_{\theta}(\sqrt{\bar \alpha_t}x_0 + \sqrt{1 - \bar \alpha_t}\epsilon, t)) ||_2^2
$$

where $t$ is chosen to be uniform in $[1, T]$ and $T$ typically is on the order of 1000.  The training process then consists of repeatedly sampling an input $x_0 \sim q(x_0)$ before choosing a time step $t$ and a Gausian distribution $\mathcal{N}(0, \mathbf{I})$ and then performing gradient descent on

$$
\nabla_\theta || \epsilon  - \epsilon_{\theta}(\sqrt{\bar \alpha_t}x_0 + \sqrt{1 - \bar \alpha_t}\epsilon, t)) ||_2^2
$$

Here input images are expected to be scaled to have a minimum of -1 and maximum of 1  $x \in 0, 1, 2, ..., 255 \to x \in [-1, 1]$.  Ho and colleagues then re-weight $L$ to place more importance on larger corruptions (corresponding to larger $t$ values) reasoning that these would be more difficult for the model $\epsilon_\theta$ to learn than smaller corruptions.

To summarize, training a diffusion inversion model as presented by Ho and colleagues consists of teaching the model to predict the noise in a corrupted image, with the reasoning that after this occurs the model will be able to remove noise from a Gaussian distribution to generate new samples. Later work finds that switching between predicting $\epsilon$ and predicting $x_0$ leads to more effective training.  THe model parameters $\theta$ are the same for all $t$ steps.

To generate samples, we want to learn the reverse diffusion process, $p_{\theta}(x_{t-1}, x_t)$.  For diffusion inversion, this is the Markov chain where transitions are Gaussian distributions learned during the training process (which adds Gaussian distributions). Specifically, the input sampling process consists of first sampling the pure noise input $x_T \sim \mathcal{N}(0, \mathbf I) $ and then iterating for $t=T, T-1, ..., 2$, first choosing noise $z \sim \mathcal{N}(0, \mathbf I) $ and then sampling

$$
x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar \alpha_t}}\epsilon_\theta(x_t, t)  \right) + \sigma_t z
$$



### Using Diffusion to generate handwritten digits

Let's try to generate images of handwritten digits using diffusion inversion.  First we need an autoencoder, and for that we can turn to a miniaturized and fully connected version of the well-known [U-net](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28) architecture introduced by Ronnenberger and colleagues.  The U-net architecture will be considered in more detail later, as for now we will make do with the fully connected version shown below:

![fully connected unet architecture]({{https://blbadger.github.io}}/neural_networks/fc_unet.png)

This is a variation of a typical undercomplete autoencoder which compresses the input somewhat (2k) using fully connected layers followed by de-compression where residual connections are added between layers of equal size, forming a distinctive U-shape.  It should be noted that layer normalization may be ommitted without any noticeable effects.  Layer sizes were determined upon some experimentation, where it was found that smaller (ie starting with 1k) widths trained very slowly. 

Next we chose to take 1000 steps in the forward diffusion process, meaning that our model will learn to de-noise an input over 1k small steps in the diffusion inversion generative process as well.  It is now necessary (for learning in a reasonable amount of time) to provide the time-step information to the model in some way: this allows the model to 'know' what time-step we are at and therefore approximately how much noise exists in the input.  [Ho and colleagues](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html) use positional encoding (inspired by the original transformer positional encodings for sequences) and therefore encode the time-step information as a trigonometric function added to the input (after a one-layer feedforward trainable layer).  Although this approach would certainly be expected to be effective for a small model as well as large ones, we take the simpler approach of encoding time-step information in a single input element before appending that element to the others after flattening.  Specifically, this time-step element $x_{t, n} = t / T$ where $t$ is the current time-step and $T$ is the final time-step.

An aside: it is interesting that time information is required for diffusion inversion at all, being that one might expect for an autoencoder trained to estimate the variance of the input noise relative to some slightly less-noisy input to be capable of estimating the accumulated noise as well, and therefore estimate the time-step value for itself.  Removing the time-step information from the diffusion process of either the original U-net or the small autoencoder above yields poor sample generation but curiously the models are capable of optimizing the objective function without much trouble.  It is currently unclear why time-step information is required for sample generation, ie $x_{T} \to x_{0}$, but not for minimizing $\epsilon - \epsilon(\sqrt \alpha_t x_0 + \sqrt{1 - \alpha_t}\epsilon)$. 

Concatenation of a single element time-step value to the input tensor may be done as follows,

```python
class FCEncoder(nn.Module):

	def __init__(self, starting_size, channels):
		super().__init__()
		starting = starting_size
		self.input_transform = nn.Linear(32*32*3 + 1, starting)
    ...

	def forward(self, input_tensor, time):
		time_tensor = torch.tensor(time/timesteps).reshape(batch_size, 1)
		input_tensor = torch.flatten(input_tensor, start_dim=1)
		input_tensor = torch.cat((input_tensor, time_tensor), dim=-1)
    ...
```

and the fully connected autoencoder combined with this one-element time information approach yields decent results after training on non-normalized MNIST data as shown below.

![mnist diffusion]({{https://blbadger.github.io}}/neural_networks/mnist_diffusion.png)

These results are somewhat more impressive when we consider that generative models typically struggle somewhat with un-normalized MNIST data, as most elements in the input are identically zero.  Experimenting with different learning rates and input modifications convinces us that diffusion inversion is happily rather insensitive to exact parameter configurations, which is a marked difference from GAN training. 

### Attention Augmented Unet Diffision

For higher-resolution images, the use of unmodified fully connected architectures is typically infeasible due to the very large number of parameters.  Certain restrictions can be placed on the transformations between one layer and the next, and here we use both convolutions and attention in our denoising model to produce realistic images of churches.

We employ a modification on the original Unet architecture such that attention has been added at each layer, modified from Phil Wang's [implementation](https://github.com/lucidrains/denoising-diffusion-pytorch) of the model introduced by [Ho and colleagues](https://arxiv.org/abs/2006.11239).  Here we use a four-deep Unet model with linear attention applied to residual connections and dot-product attention applied to the middle block. The general architecture is as follows:

![churches diffusion model]({{https://blbadger.github.io}}/deep-learning/churches_diffusion_architecture.png)

MLP activations for this model are Sigmoid Linear Units (SiLU)

$$
f(x) = x * \sigma(x) = \frac{x}{1 + e^{-x}}
$$

which is a very similar function to GeLU, and group norms are applied to each block and attention block (before the self-attention transformation).  Time information is encoded into each blockvia a trainable MLP applied to fixed cosine-sine positional embeddings in the input.  It is somewhat curious that this encoding of time into each block is beneficial, as positional encodings are usually applied only on the input.

After a couple hundred epochs of training on LSUN churches at $64^2$ resolution we have the following sampled images:

![lsun churches 64 diffusion]({{https://blbadger.github.io}}/neural_networks/diffusion_cover.png)


## Entropy 

### Introduction: the second law of thermodynamics

The second law of thermodynamics may be stated in may equivalent ways, and one is as follows: spontaneous (and irreversible) processes increase the entropy of the universe.  Another way of saying this is that for irreversible reactions, $S_f > S_i$ or the final entropy is greater than the initial entropy.

Here entropy may be thought of as the amount of disorder.  'Disorder' is a somewhat vague term, and so in statistical mechanics entropy is defined as the number of states a system can exist in, with more possible states corresponding to higher entropy.  Spontaneous processes are those that occur during the passage of time without any external input.  In Gibb's celebrated equation that relates free energy $\Delta G$ (the energy available to do work) to change in enthalpy $\Delta H$ (heat) and entropy $\Delta S$, 

$$
\Delta G = \Delta H - T \Delta S
$$

spontaneous processes are those for which $\Delta G < 0$ for the system.  Therefore, either heat escapes into the surroundings (signified by a negative $\Delta H$) which increases the entropy of the surroundings ($\Delta S = \Delta q / T$), or else entropy increases in the system itself.

The second law may also be stated as follows: the universe becomes more disordered over time. This sort of definitions abounds when entropy is discussed, but before proceeding to examine how accurate they are it is helpful to understand what thermodynamics is, and where entropy came from.  Thermodynamics is the study of heat change over time, just like the name implies.  This study got underway during the 19th century when engineers attempted to make steam engines more efficient, and so the theory of thermodynamics predates atomic theory by many decades.  

The source of the second law is important because it was derived using assumptions which, while useful, are not accurate.  Specifically, the ideal gas law ($PV = nRT$) was used to calculate isothermal gas expansion that was then used to calculate the change in entropy over time ([see here](https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Supplemental_Modules_(Physical_and_Theoretical_Chemistry)/Thermodynamics/The_Four_Laws_of_Thermodynamics/Second_Law_of_Thermodynamics)).  This is important because no gas is actually ideal: real gases all exhibit attractive forces that do not exist in the ideal gas equations.  

Consider the following thought experiment: an exothermic, negatively entropic reaction floating in the cold vacuum of space, billions of light years from any other matter.  Now the standard interpretation is that this reaction produces heat which spreads to the outside universe, thereby increasing total entropy.  So our floating reaction radiates heat into space but does this increase the universal entropy, in the statistical mechanical sense? Not unless the radiation encounters matter, which may never occur. Will the reaction still proceed even though there actually a decrease in universal entropy?   

### A first consideration: fundamental forces and order

Consider the statement "the universe becomes more disordered over time".  Is this true of the cosmos over the timescales we can observe, nearly 14 billion years?  As far as has been observed, it is not: galactic structures (the ones we can see at least) have become less, rather than more, chaotic over time.  ~13.7 billion years is a long time, and it seems strange for the observable universe to appear more ordered over that time scale if indeed every spontanous process caused the universe to become more disordered.  What stops this from happening?

The spontaneous expansion of gas (that can be used to do work) may be explained using entropy as follows: by increasing the volume each gas molecule can exist in, the number of available states to that molecule increases and so entropy increases. At our scale and temperatures, gas indeed expands.  But not consider our atmosphere: it does not expand, even though it is entropically favorable to do so. The force acting against expansion is gravity, and the cosmos is full of examples of this force acting to prevent and reverse the expansion of gas. Contraction of gas may but usually does not occur isothermally, meaning that gravity acts to restrict molecular motion but usually at the expense of energy dispersion.

At very small scale, the force of electromagnetism confines electrons to exist near nuclei in distinct cloud-like orbitals.  A freely moving electron and proton are confined to become a hydrogen atom by the electromagnetic force if they interact. This interaction will release a photon, which may or may not interact with matter.  The same is true of the strong nuclear force: quarks that potentially move freely are constrained by the strong force.  Once again, these processes act to restrict motion and thereby reduce disorder, but also usually disperse energy in the form of photons (that may or may not in turn come into contact with other matter).

### Second consideration: infinite states 

Work can be defined as directed movement: when one does work to move a log, all molecules in the log move in approximately concerted motion.  In contrast, thermal motion is not directed: heat causes molecules to move faster but not in concerted motion.  In statistical mechanics, the number of internal states (formally, the microcanonical ensemble) that a molecule can assume is used to calculate its entropy.  

$$
S = k \; ln W
$$

where $W$ signifies the number of microstates available in a collection of particles and $k$ is Boltzmann's constant.  

If we restrict ourselves to energy levels alone, it is often thought that quantum mechanics allows us to obtain finite numbers of microstates $W$ and therefore the notion of entropy as being the number of available states is useful (ie a finite quantity).  But it is no longer useful when one considers motion through space: there are an uncountably infinite number of directions in which a gas molecule may move.  And on closer examination of even a simple atom, there seems to be an uncountably infinite number of microstates even in the framework of quantum mechanics: although discrete orbitals and bond angles do exist, real bond angles vary more or less continuously and electron orbitals are only probable locations of electrons, not discrete limits.  

Suppose one accepts that energy levels are purely discrete.  As molecular orientation is important (and the defining characteristic between energy able to do work and heat), molecule that obeys quantum mechanics still has an uncountably infinite number of states at any time.  Defining entropy by the number of microstates is unquestionably useful, but does not appear to be an entirely accurate description of nature.

### Third consideration: combinatorial microstates

Consider the idea that entropy is determined by the number of microstates avaliable to a collection of molecules, with more microstates possible signifying a higher entropy.  The current understanding that the number of microstates is maximized upon a dissipation of head presents a problem: what about the relation of molecules in one ensemble to each other (the combinatorial microstate)?  This is of more than theoretical concern, as many subatomic particles (electrons etc.) are practically indistinguishable if they exist at a similar energy.

To see the problem, say we have 3 particles, $x, y, z$ that may each exist in one of 3 possible locations $A, B, C$.  If the particles are identical and exhibit identical thermodynamic properties, we can regard these as identical and name them $x, x, x$: these may be the results of a reaction proceeding to thermodynamic equilibrium.  How many possible combinatorial microstates are there if the particles are identical?  If they can exist in any of $A, B, C$ then there are 3 distinct possible locations each containing 1, 2, or 3 identical particles.  A simple recursive program can be written to search for all possibilities of ways that three identical particles can be arranged in three distinct spaces as follows:

```python
import pprint

def number_of_states(places, particles, final_set, count=0):
	if count == particles:
		places_tuple = tuple(tuple(i) for i in places)
		if places_tuple not in final_set:
			final_set.add(places_tuple)
		return
	for i in range(len(places)):
		places[i].append('*')
		number_of_states(places, particles, final_set, count+1)
		places[i].pop()

particles = 3
places = [[] for i in range(particles)]
final_set = set()

number_of_states(places, particles, final_set, count=0)
pprint.pprint(final_set)
print(len(final_set))
```
which results in 10 possibilites, enumerated as follows

```python
{((), (), ('*', '*', '*')),
 ((), ('*',), ('*', '*')),
 ((), ('*', '*'), ('*',)),
 ((), ('*', '*', '*'), ()),
 (('*',), (), ('*', '*')),
 (('*',), ('*',), ('*',)),
 (('*',), ('*', '*'), ()),
 (('*', '*'), (), ('*',)),
 (('*', '*'), ('*',), ()),
 (('*', '*', '*'), (), ())}
 
10
```
For small numbers, this result can quickly be confirmed by hand.  

But now imagine that the particles are distinguishable: perhaps they are thermodynamically distinct.  Now, the number of possibilities for particles $x, y, z$ is 3 places, three particles, where order matters (here meaning that xy at location A is distinct from xz at location A, but that xy at location A is not distinct from yx at location A): $3^3 = 27$ possible microstates are possible.  This can be confirmed with our previous program if we instead replace `'*'` with a separate symbol upon each addition:

```python
...
	for i in range(len(places)):
		places[i].append(str(count))
    ...
```
which yields a list of all possible microstates,

```python
{((), (), ('0', '1', '2')),
 ((), ('0',), ('1', '2')),
 ((), ('0', '1'), ('2',)),
 ((), ('0', '1', '2'), ()),
 ((), ('0', '2'), ('1',)),
 ((), ('1',), ('0', '2')),
 ((), ('1', '2'), ('0',)),
 ((), ('2',), ('0', '1')),
 (('0',), (), ('1', '2')),
 (('0',), ('1',), ('2',)),
 (('0',), ('1', '2'), ()),
 (('0',), ('2',), ('1',)),
 (('0', '1'), (), ('2',)),
 (('0', '1'), ('2',), ()),
 (('0', '1', '2'), (), ()),
 (('0', '2'), (), ('1',)),
 (('0', '2'), ('1',), ()),
 (('1',), (), ('0', '2')),
 (('1',), ('0',), ('2',)),
 (('1',), ('0', '2'), ()),
 (('1',), ('2',), ('0',)),
 (('1', '2'), (), ('0',)),
 (('1', '2'), ('0',), ()),
 (('2',), (), ('0', '1')),
 (('2',), ('0',), ('1',)),
 (('2',), ('0', '1'), ()),
 (('2',), ('1',), ('0',))}
 
27
```

Combinatorically, in the former case where particles are not distinguishable then we want to find the number of 3-element multisets of elements of the set ${A, B, C}$, corresponding to each possible location.  We seek the possible multisets because more than one location can be present in our list of where the particles are, ie $\{A, A, B\}$ is a possibility that corresponds to two particles at position $A$ and one at $B$.  The general method for finding $k$-element multisets from a set of size $n$ is

$$
\left(\binom{n}{k} \right) = \binom{n+k-1}{k}
$$

and in the case above, 

$$
\left(\binom{n}{k} \right) = \binom{3+3-1}{3} = \binom{5}{3} = 10
$$

For more information, search the stars and bars problem.  This number will always be smaller than the number of ways k distinct objects can be placed in n distinct locations for all $n>1$ and $k>1$, which is

$$
n^k
$$

because for any orientation with items in more than one location, there is more than one way to place the items while keeping the multiset identical.

This means that there are more possible microstates available to a particle ensemble where the elements are thermodynamically distinct: in other words, entropy is maximized for ensembles that contain thermodynamically distinct elements. 

For more problems resulting from indistinguishability of small particles, see the [Gibbs Paradox](https://en.wikipedia.org/wiki/Gibbs_paradox). 

### Fourth consideration: exceptions to the rule

The first law of thermodynamics is that energy cannot be created nor destroyed, and there are no phenomena that I am aware of that could be evidence to dispute this rule.  This is not the same for the second law, and two particular examples of importance will be discussed here.  It should be made clear that nearly every other relevant physical observation supports rather than fails to support the second law, but the existence of counterexamples does suggest that our understanding is presently incomplete. 

A first exception is Brownian motion, the movement of small (<0.1 mm) particles in fluids such as water.  Originally thought to be a biological phenomena (observed with pollen grains), Brown provided evidence that nonliviing matter undergoes Brownian motion too (hence the name).  Einstein used Brownian motion as evidence for the theory of the atom, and it is now understood that this motion is the result of a huge number of molecules undergoing thermal motion that collide with a larger particle, and that these collisions add and subtract and tend to push the particle along an everywhere-nondifferentiable path.  

Why does Brownian motion flout the second law?  Intuitively, because it converts heat to directed motion.  Heat causes random motion in molecules of the surrounding liquid, but the summation of the impacts of these molecules with the larger object lead to motion that is somewhat less than random (although very difficult to predict).  More concretely, Brownian motion in three dimensions is an example of an irreversible, spontaneous process that converts heat to work: irreversible because a particle undergoing Brownian motion does not revisit its initial position an arbitrary number of times in the future (ie it travels away from its initial spot over time), spontanous because no free energy is required, and it converts heat to work because work is equal to force times distance, and it takes a (very tiny) amount of force to move a tiny particle from point a to b in a fluid due to friction.  The energy source for this movement is thermal motion of molecules, meaning that heat is converted to work.  

A second exception is the arrangement of electrons (or any charged particle) on the surface of a three dimensional conductor. This is a spontanous, irreversible process that decreases the number of possible locations of the charged particles relative to what is possible if they were to fill the volume of the conductor.  By the definition of entropy as a measure of possible orientations, the process of particles moving from a state of greatest volume to a state of limited (actually zero) volume implies a decrease in entropy over time.  

Let's look more closely at why: the movement of charged particles to the surface of a conductor is spontanous because it occurs with no external work, it is irreversible because to make the particles not collect on the surface of the conductor does require work, and it decreases entropy because there are far fewer possible locations to exist in on the surface of a solid than in the interior.  As an analogy, imagine that gas molecules collected on the inside surface of a gas chamber: in this case, the molecules occupy a negligeable fraction of the volume they could possibly exist in, and thus have undergone isothermal contraction.  As isothermal expansion is entropically favorable, the opposite is the reverse. 

### Entropy and the second law, more precisely: Energy dissipation

With the considerations of attempting to avoid defining entropy in terms of microstates, or as a measure of disorder, we can try to reformulate the second law of thermodynamics to be as accurate as possible.  And here it is: Energy tends to spread out over time.  What does this mean for physical systems?  The process of energy spreading out in phase space is called dissipation. Phase space dissipation is the process by which some area of phase space is reduced to measure 0 over time in a dynamical system.  

In other words, the universe acts as a dissipative dynamical system with regards to energy, at least most of the time.  Now consider this: dissipative but not conservative (conservative meaning that energy does not spread out over time)  dynamical systems may have fractal attractors (for a good review on this topic, see [here](https://www.osapublishing.org/josaa/abstract.cfm?uri=josaa-7-6-1055). 

From this perspective, it is perhaps not difficult to see why there are so many fractal objects in nature: dissipation is necessary for self-similar phase space structures to form.  Phase space is an abstract mathematical system for describing chang eover time and is thus not directly related to physical shape, but it is important to note that many dynamical systems that are self-similar over time (in phase space) are also self-similar in structure at one time: turbulence is the classic example of this phenomenon.  

### Dissipation leads to organization

If one accepts the idea that dissipation of energy is the most accurate description of what has been oberved as entropy, then one can accept the accuracy of dissipative dynamical systems for describing physical events subjected to the second law of thermodynamics.  As seen elsewhere on [this page](https://blbadger.github.io/), dissipative phase space portraits of dynamical systems form attractors, points or lines or fractals of a lower dimension than the possible input points.  Over time, the trajectories in phase space are limited to the locations occupied by these attractors, which necessarily contain fewer points than would otherwise be possible if the trajectory were not limited to the attractor.  

All this leads to an interesting conclusion: the second law of thermodynamics implies that energy usually spreads out which leads to the formation of self-similar fractal behavior over time structures in space.  If we define entropy as dissipation of energy and the formation of fractals as decreasing the possible number of states (as behavior at very different scales is similar in some way or another), then the conclusion is that the second law of thermodynamics implies that energy spreads out, increasing entropy, which decreases the number of possible states and thus decreases disorder.  We have performed an inversion with respect to the relation of the second law of thermodynamics on order and disorder.







## Feature Visualization I: Feature Maps

### Introduction

Suppose one wished to perform a very difficult task, but did not know how to begin.  In mathematical problem solving, one of the strategies most commonly used to approach a seemingly impossible problem is to convert that problem into an easier one, and solve that.  In many respects, the field of deep learning places the same importance on the 'make it easier' principle.  Most problems faced by the field certainly fall into the category of being very difficult: object recognition, indeed, is so difficult that no one has made much progress in this problem by directly programming a computer with explicit instructions in decades.  The mathematical function mapping an input image to its corresponding output classification is therefore very difficult to find indeed.

Classical machine learning uses a combination of ideas from statistics, information theory, and computer science to try to find this and other difficult functions in a limited number of steps (usually one). Deep learning distinguishes itself by attempting to approximate functions using more than one step, such that the input is represented in a better and better way before a final classification is made.  In neural networks and similar deep learning approaches, this representation occurs in layers of nodes (usually called neurons).  

As we will be employing a model whose hidden layers consists primarily of 2-dimensional convolutions, first it may be beneficial to orient ourselves to the layout of a standard example of this type of layer.  See [this page](https://blbadger.github.io/neural-networks.html#convolutions-explained) for a recap of what a convolution is and why this operation is so useful for deep learning models dealing with images.

For a convolutional layer with a single kernal, the output is a two-dimensional grid corresponding to the value of the convolutional operation centered on each of the pixels.  Therefore it could be expected that an image of 256 pixels by 256 pixels, ie a 256x256 tensor with each pixel intensity value corresponding to one tensor element, when passed through a convolutional layer will give an output tensor of 256 by 256 (if a specific type of padding is used to deal with edge pixels).  There are normally multiple kernals used per convolutional layer, so for a layer with say 16 kernals (also called 'filters') we would have a 16x256x256 output.  Each of the 16 elements are referred to as a 'feature map', and on this page will always precede the other tensor elements.

### Feature Visualization from First Principles

The process of sequential input representation, therefore, is perhaps the defining feature of deep learning.  This representation does make for one significant difficulty, however: how does one know how the problem was solved?  We can understand how the last representation was used to make the output in a similar way to how other machine learning techniques give an output from an input, but what about how the actual input is used?  The representations may differ significantly from the input itself, making the general answer to this question very difficult.

One way to address this question is to try to figure out which elements of the input contribute more or less to the output.  This is called input attribution.

Another way is to try to understand what the representation of each layer actually respond to with regards to the input, and then try to understand how this response occurs.  Perhaps the most natural way to study this is to feed many input into the model and observe which input give a higher activation to any layer or neuron or even subset of neurons across many layers, and which images yield a lower activation.  


There is a problem with this approach, however: images as a rule do not contain only one characteristic that could become a feature, and furthermore even if some did there is little way of knowing how features would be combined together between layers being that nonlinear transformations are applied to all layers in a typical deep learning model.  

Instead we can approach the question of what each neuron, subset of neurons, or layer responds to by having the neurons in question act as if they are our model's outputs, and performing gradient ascent on the input.  We have seen [elsewhere](https://github.com/blbadger/blbadger.github.io/blob/master/input-generation.md) that an output can effectively specify the expected image on the input using gradient ascent combined with a limited number of priors that are common to nearly all natural images (pixel correlation, transformational resiliency, etc.) so it is logical to postulate that the same may be true for outputs that are actually hidden layers, albeit that maximizing the activation of a hidden layer neuron or layer does not have an intrinsic meaning w.r.t the true output.

To make this more precise, what we are searching for is an input $a'$ such that the activation of our chosen hidden layer or neuron $z^l$ (given the model configuration $\theta$ and input $a$) denoted by $z^l(a, \theta)$ is maximized,

$$
a' = \underset{a}{\mathrm{arg \; max}} \; z^l(a, \theta)
$$

Convolutional layers typically have many feature maps, each specified by a separate set of parameters. We will mostly focus on finding inputs $a'$ that maximally activate a single feature, rather than all features for a layer given that there may be hundreds of features per layer. For a two-dimensional convolutional layer with $m$ rows and $n$ columns, the total activation $z$ at layer $l$ for feature $f$ is denoted

$$
z^l_f = \sum_m \sum_n z^l_{f, m, n}
$$

In tensor notation, this would be written as the total activation of the tensor `[f, :, :]` where `:` indicates all elements of the appropriate index.

For a subset of neurons in layer $l$ and feature $f$, the total activation of all neurons in row $n$, denoted

$$
z^l_{f, m} = \sum_n z^l_{f, m, n}
$$

which is denoted as the sum of the elements of the tensor `[f, m, :]`.  The element of row `m` and column `n` is denoted of feature `f` is `[f, m, n]`.

Finding the exact value of $a'$ can be very difficult for non-convex functions like hidden layer outputs. An approximation for the input $a'$ such that when given to our model gives an approximate maximum value of $z^l_f(a', \theta)$ may be found via gradient descent.  The gradient in question used on this page is the gradient of the $L_1$ metric between a tensor with all values equal to some large constant $C$ and the tensor activation of a specific layer and feature (or a subset of this layer) $z^l_f$

$$
g = \nabla_a (C - z^l_f(a, \theta))
$$

For an example, if we were to optimize a feature comprising two neurons then the gradient would be taken as the $L^1$ distance between the activations of neurons $f_1, f_2$ and the point $C, C$, which is $(C - f_1) + (C - f_2)$.  Miminization of an $L^1$ distance between a feature's activation and some constant is one among many ways one could possibly maximize a feature's activations, but will be the primary method discussed here.

At each step of the gradient descent procedure, the input at point $a_k$ is updated to make $a_{k+1}$ as follows

$$
a_{k+1} = a_k - \epsilon * g_k
$$

with $*$ usually denoting broadcasted multiplication from scalar $\epsilon$ to tensor $g_k$.  The hope is that the sequence $a_k, a_{k+1}, a_{k+2}, ..., a_j$ converges to $a'$, which happily is normally the case for appropriately chosen values of $\epsilon$ for most layers.

We will be using Pytorch for our experiments on this page.  As explained [elsewhere](https://blbadger.github.io/input-generation.html#input-generation-with-auxiliary-outputs), Pytorch uses symbol-to-number differentiation as opposed to symbol-to-symbol differentiation which means that the automatic differentiation engine must be told which gradients are to be computed before forward propegation begins. Practically speaking, this means that getting the gradient of a model's parameters with respect to a hidden layer is difficult without special methods. 

Rather than deal with these somewhat tricky special functions, we can instead simply modify the model in question in order to make whichever layer is of interest to be our new output.  This section will focus on the InceptionV3 model, which is as follows:

![inceptionv3 architecture]({{https://blbadger.github.io}}/neural_networks/inception3_labelled.png)

(image retrieved and modified from Google [docs](https://cloud.google.com/tpu/docs/inception-v3-advanced))

and may be found on the following [github repo](https://github.com/pytorch/vision/blob/main/torchvision/models/inception.py).  Inspecting this repository will show us how the inceptionv3 model has been implemented, and that there are a few differences between the origina model and the open-source Pytorch version (most notably that the last layers do not contain softmax layers by default).

A pre-trained version of InceptionV3 may be loaded from `torch.hub` 

```python
Inception3 = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True).to(device)
```
And after deciding on which layer is of interest, the model may be modified by writing a class that takes the original InceptionV3 model as an initialization argument and then specifies what to do with the layers in question.  As this is a pre-trained model, it generally does not make sense to change the configuration of the layers themselves but we can modify the model to output any layer we wish.  Here we want to view the activations of the`Conv2d_3b_1x1` layer

```python
class NewModel(nn.Module):

	def __init__(self, model):
		super().__init__()
		self.model = model

	def forward(self, x):
		# N x 3 x 299 x 299
		x = self.model.Conv2d_1a_3x3(x)
		# N x 32 x 149 x 149
		x = self.model.Conv2d_2a_3x3(x)
		# N x 32 x 147 x 147
		x = self.model.Conv2d_2b_3x3(x)
		# N x 64 x 147 x 147
		x = self.model.maxpool1(x)
		# N x 64 x 73 x 73
		x = self.model.Conv2d_3b_1x1(x)
		return x
```
and because we are taking the original trained model as a parameter, each layer maintains its trained weight and bias.

Instantiating this model with the original InceptionV3 model object

```python
newmodel = NewModel(Inception3)
```
we can now use our `newmodel` class to find the activation of the `Conv2d_3b_1x1` layer and perform backpropegation to the input, which is necessary to find the gradient of the output of this layer with respect to the input image.

To be specific, we want to find the gradient of the output of our layer in question $O^l$ with respect to the input $x$

$$
g = \nabla_a O^l(a, \theta)
$$

```python
def layer_gradient(model, input_tensor, desired_output):
  ...
	input_tensor.requires_grad = True
	output = model(input_tensor).to(device)
	focus = output[0, 200, :, :] # the target to maximize the output
	target = torch.ones(focus.shape).to(device)*200 # make a large target of the correct dims
	loss = torch.sum(target - focus)
	loss.backward()
	gradient = input_tensor.grad
	return gradient
```

Now that we have the gradient of the output (which is our hidden layer of interest in the original InceptionV3 model) with respect to the input, we can perform gradient descent on the input in order to maximize the activations of that layer.  Note that this process is sometimes called 'gradient ascent', as we are modifying the input in order to maximize a value in the output.  But as our loss is the $L_1$ distance from a tensor with all elements assigned to be a large constant $C=200$, gradient descent is the preferred terminology on this page as this loss is indeed minimized by moving against the gradient.

For [this page](https://blbadger.github.io/input-generation.html) These restrictions may be thought of as Bayesian priors that we introduce, knowing certain characteristics that natural images possess.  The three characteristics that are implemented below are relative smoothness (enforced by Guassian convolution via `torchvision.transforms.functional.gaussian_blur`) and transformational invariance (using `torchvision.transforms.Resize()`).

We can use this procedure of gradient descent on the input combined with priors to visualize the input generated by maximizing the output of the given neuron, or layer.  If we choose module `mixed_6b` and maximize the activations of neurons in the 654th (of 738 possible) feature map of this module, we find an interesting pattern is formed.

![654 visualization]({{https://blbadger.github.io}}/neural_networks/inception_654.png)

The first thing to note is that the tensor indicies for 'row' and 'column' do indeed accurately reflect the position of the pixels affected by maximization of the output of the neuron of a given position.

It is interesting to consider how exactly the feature pattern forms from tiles of each row and column's contribution.  For each feature map in module `mixed_6b`, we have a 17x17 grid and can therefore iterate through each element of the grid, left to right and top to bottom, like reading a book.  Slicing tensors into non-rectangular subsets can be difficult, so instead the process is to compose the loss of two rectangular slices: one for all columns of each row preceding the current row, and another rectangle for the current row up to the appropriate column.

```python
def layer_gradient(model, input_tensor, desired_output, index):
	...
	input_tensor.requires_grad = True
	output = model(input_tensor).to(device)
	row, col = index // 17, index % 17
	if row > 0:
		focus = output[0, 415, :row, :] # first rectangle of prior rows
		focus2 = output[0, 415, row, :col+1] # second rectangle of the last row
		target = torch.ones(focus.shape).to(device)*200
		target2 = torch.ones(focus2.shape).to(device)*200
		loss = torch.sum(target - focus)
		loss += torch.sum(target2 - focus2) # add losses

	else:
		focus = output[0, 415, 0, :col+1]
		target = torch.ones(focus.shape).to(device)*200
		loss = torch.sum(target - focus)

	loss.backward()
	gradient = input_tensor.grad
	return gradient
```

Note that we must also set our random seed in order to make reproducable images.  This can be done by adding

```python
seed = 999
random.seed(seed)
torch.manual_seed(seed)
```

to the function that is called to make each input such that the above code is processed before every call to `layer_gradient()`.  

One important note on reproducing a set seed: the above will not lead to reproducibility (meaning identical outputs each time the code is run) using Google Colab.  For reasons that are not currently clear, image generation programs that appropriately set the necessary random seeds (as above) yield identical output images each time they are run on a local machine, but different output images each time they are run on Colab.  Preliminary observations give the impression that this issue is not due to incorrect seed setting but rather some kind of round-off error specific to Colab, but this has not been thoroughly investigated.

Running our program on a local machine to enforce reproducibility between initializations, we have

{% include youtube.html id='EJo1fUzheSU' %}

We come to an interesting observation: each neuron when optimized visibly affects only a small area that corresponds to that neuron's position in the `[row, col]` position in the feature tensor, but when we optimize multiple neurons the addition of one more affects the entire image, rather than the area that it would affect if it were optimized alone.  This can be seen by observing how the rope-like segments near the top left corner continue to change upon addition of neurons that alone only affect the bottom-right corner.

How is this possible? Neurons of any one particular layer are usually considered to be linearly independent units, and assumption that provides the basis for being able to apply the gradient of the loss to each element of a layer during training.  But if the neurons of layer `mixed_65 654` were linearly independent with respect to the input maps formed when optimizing these neuron's activations, we would not observe any change in areas unaffected by each neuron.  It is at present not clear why this pattern occurs.

For the 415th feature map of the same module, a somewhat more abstract pattern is formed.  Observe how a single neuron exhibits a much broader field of influence on the input when it is optimized compared to the previous layer: this is a common feature of interior (ie not near layer 0 or 738) versus exterior (close to 0 or 748 in this particular module).  This is likely the result of differences in the inception module's architecture for different layers that have been concatenated to make the single `mixed_6b` output.

![654 visualization]({{https://blbadger.github.io}}/neural_networks/inception_415.png)

Something interesting to note is that the neuron at position $[415, 9, 9]$ in the `mixed_6b` module is activated by a number of different patterns at once, which is notably different than the neuron at position $[354, 6, 0]$ which is maximally activated by one rope-like pattern alone.

The approach detailed above for introducing transformational invariance leads to fairly low-resolution images by design (which allows for less time to be taken while generating an image). The octave method increases resolution and clarity at the expense of more time per image generated, and may be implemented as follows:

```python
def octave(single_input, target_output, iterations, learning_rates, sigmas):
	...
	start_lr, end_lr = learning_rates
	start_sigma, end_sigma = sigmas

	for i in range(iterations):
		single_input = single_input.detach() # remove the gradient for the input (if present)
		input_grad = layer_gradient(Inception, single_input, target_output) # compute input gradient
		single_input -= (start_lr*(iterations-i)/iterations + end_lr*i/iterations) * input_grad # gradient descent step
		single_input = torchvision.transforms.functional.gaussian_blur(single_input, 3, sigma=(start_sigma*(iterations-i)/iterations + end_sigma*i/iterations)) # gaussian convolution

	return single_input
```

This function may now be called to perform gradient descent with scaling, and in the following method three octaves are applied: one without upscaling, one that scales to 340 pixels, and one that scales to 380.

```python
def generate_singleinput(model, input_tensors, output_tensors, index, count, random_input=True):
	...
	single_input = octave(single_input, target_output, 100, [0.4, 0.3], [2.4, 0.8])
 
	single_input = torchvision.transforms.Resize([340, 340])(single_input)
	single_input = octave(single_input, target_output, 100, [0.3, 0.2], [1.5, 0.4])

	single_input = torchvision.transforms.Resize([380, 380])(single_input)
	single_input = octave(single_input, target_output, 100, [0.2, 0.1], [1.1, 0.3])
```

The images produced are of higher resolution and are generally clearer than with only down-sampling used, so most of the rest of the images on this page are generated via gradient descent with octaves.

It is worth assessing how representative the images produced using octaves and Gaussian convolutions with gradient descent are to the images produced with gradient descent alone.  If the resulting images are completely different, there would be limited utility in using octaves to understand how features recognize an input.  For four arbitrarily chosen features of module Mixed 6b, plotting pure gradient descent compared to octave-based gradient descent is as follows:

![inceptionv3 comparison]({{https://blbadger.github.io}}/neural_networks/Inception3_raw_octave_comparison.png)

The images are clearly somewhat different, but the qualitative patterns are more or less unchanged. These results have been observed for features in other layers as well, providing the impetus for proceeding with using octaves and Gaussian convolutions with gradient descent to optimize the layers in images on the rest of the page.

We can observe the process of generating an image using this octave method by plotting the average activation for each feature in a layer of interest, while attempting to maximize the total activation of some given feature.  Maximizing the total activation of GoogleNet's Feature 5 of Layer 5a (denoted in red) and plotting the average activation for all features of Layer 5a,

{% include youtube.html id='WshX-WCQHno' %}

It is apparent that the process of maximizing the total activation of a given feature does not lead the others unchanged.  In particular, a feature near 350 has an average activation that is only slightly less than the activation of the target feature.  What this means is that an image that is optimized for activating feature 5 inadvertently activates this other feature as well, meaning that these are in a sense similar features.  Using this method it is possible to map how similar each feature is from each other. 

### Mapping InceptionV3 Features

In the pioneering work on feature visualization in the original Googlenet architecture (aka InceptionV1), [Olah and colleagues](https://distill.pub/2017/feature-visualization/) observed an increase in the complexity of the images resulting from maximizing the activation of successive layers in that model: the deeper into the network the authors looked, the more information could be gleaned from the input after performing gradient descent to maximize the layer's activation.  Early layers show maze-like patterns typical of early convolutional layers, which give way to more complicated patterns and textures and eventually whole objects.  Curiously, the last layers were reported to contain multiple objects jumbled together, a phenomenon this author has observed [elsewhere](https://blbadger.github.io/input-generation.html).

The InceptionV3 architecture differs somewhat from the GoogleNet that was thoroughly explored, so it is worth investigating whether or not the same phenomenon is observed for this model as well.  To orient ourselves, the module map for the InceptionV3 model may be divided into four general regions.

![inceptionv3 architecture]({{https://blbadger.github.io}}/neural_networks/inception3_zones.png)

In the first of these regions, convolutions are applied in a sequential manner with no particularly special additions.  Optimizing the first 16 features of each layer, we have

![inceptionv3 architecture]({{https://blbadger.github.io}}/neural_networks/inception3_entry_convs.png)

It is interesting to note that there are few in the input when we optimize for activation of the Conv2d 1a layer. Instead, it appears that the InceptionV3 network has learned a modified version of the CMYK color pallate, in which yellow has been substituted for a combination of yellow and blue (ie green). To this author's knowledge it has not been found previously that a deep learning model has learned a color pallate, so it is unknown whether or not this behavior is specific to Inceptionv3.  It should be noted that if optimization occurs without the octave process, similar colors are observed but a fine grid-like pattern reminiscent of CRT video appears for a minority of features.

Note that the gray filters signify a lack of gradient in that feature with respect to the input. 

The second convolutional layer begins to respond to patterns in the image, and in the following layers the features become more detailed.

Next we come to the mixed convolutional layers, which are composed of modules (sometimes called inception units) that contain convolutional layers in parallel, which become concatenated together at the end of the module to make one stack of the features combining all features from these units.  The first three mixed units have been somewhat arbitrarily classified as 'early mixed'.  Optimizing the activation of 8 features from each layer, 

![inceptionv3 architecture]({{https://blbadger.github.io}}/neural_networks/Inception3_early_mixed.png)

It is clear that more complicated patterns are found when we visualize the features that these early mixed layers respond to. 

At the middle mixed layers, we start to see more distinctive shapes appear in a minority of features, with the rest usually forming complicated patterns.  The following figure plots the first 8 features per layer denoted:

![inceptionv3 architecture]({{https://blbadger.github.io}}/neural_networks/Inception3_middle_mixed.png)

Observe the animal eyes generated by the second feature of layer Mixed 6b, or what appears to be a porcupine in the second feature of layer Mixed 6e.

In the late mixed layers, feature activation optimization often leads to recognizable shapes being formed. For example, the second feature of layer Mixed 7a is a rope harness, and the fourth feature of the same layer appears to be a snorkeling mask.  But note also how may colors are mixed together, and sometimes many shapes as well.

![inceptionv3 architecture]({{https://blbadger.github.io}}/neural_networks/Inception3_late_mixed.png)

### Feature Maps are Metric Invariant

It may be wondered how specific these feature maps are to the optimization process, and in particular whether minimization of different metrics between the feature activations $z^l_f$ and $C$ lead to different feature maps.  This is equivalent to wondering whether instead of maximizing the total activation ($L^1$ metric), we instead weight the maximization of smaller elements higher compared to maximization of larger elements of $z^l_f$ (corresponding to the $L^2$ metric) or else find use the metric with the largest absolute value, $L^\infty$.

Focusing on mapping features from InceptionV3 layer Mixed 6b, we optimize the input using the $L^2$ metric of the distance between the feature's activations and some large constant,

```python
loss = 5*torch.sqrt(torch.sum((target-focus)**2))
```
as well as the $L^\infty$ metric of that same distance, which can be implemented as follows:

```python
loss = torch.linalg.norm((target-focus), ord=np.inf)
```
For the first four features of InceptionV3's layer Mixed 6b, we can see that the same qualitative objects and patterns are formed regardles of the metric used.  Note, however, that in general the $L^1$ metric has the smallest amount of high-frequency noise in the final image and is therefore preferred.

![metrics and layer optimizations]({{https://blbadger.github.io}}/neural_networks/metric_invariance.png)

In conclusion, the feature maps presented here are mostly invariant with respect to the specific measurement used to determine how to maximize the feature activations.

### Mapping GoogleNet Features

In [another article](https://blbadger.github.io/input-generation.html), we saw how there were differences between how recognizable a generated input representative of a certain ImageNet training class was between various neural networks.  In particular, InceptionV3 generally yielded less-recognizable images than GoogleNet.  This brings about the question of whether or not the feature maps in the previous section might also be less recognizable than those for GoogleNet, and this can be easily explored.  

To recap, GoogleNet was the first published model to use the Inception architecture in which different convolutional layers are made in parallel before being joined.  GoogleNet is only about half as deep as InceptionV3, and has the following architecture (layer names modified for clarity):

![googlenet_architecture]({{https://blbadger.github.io}}/neural_networks/annotated_googlenet.png)

Initializing our model with an implementation of GoogleNet trained on ImageNet

```python
model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True, init_weights=True).to(device)
```

We have the following feature maps using the same optimization procedure denoted in the previous section. The starting convolutional layers are similar to what is found in the InceptionV3 model: solid colors and simple patterns are found.

![googlenet_architecture]({{https://blbadger.github.io}}/neural_networks/googlenet_convlayers.png)

Relatively early, more complex patterns are formed and objects such as small parts on animal faces are observed.

![googlenet_architecture]({{https://blbadger.github.io}}/neural_networks/googlenet_layer3.png)

![googlenet_architecture]({{https://blbadger.github.io}}/neural_networks/googlenet_layer4.png)

For the last two mixed convolutional layers,

![inceptionv3 layer combo]({{https://blbadger.github.io}}/neural_networks/googlenet_5a5b.png)

Compared to the final two layers of InceptionV3, we find that indeed there are more coherent images produced by GoogleNet (particularly for features of layer 5a, which contains recognizable snakes and bird faces). 

### Mapping ResNet Features

Both GoogleNet and InceptionV3 architectures are based on variants of what was originally called the Inception module, where multiple convolutional layers are applied in parallel. It may be wondered how much this architecture contributes to the feature maps that we have observed above: would a convolutional network without parallel layers also exhibit a similar tendancy for early layers to respond to simple patterns, middle layers to be activated by objects, and late layers activated by many objects together?

Another well-performing network for ImageNet-based tasks is [ResNet](https://arxiv.org/abs/1512.03385), an architecture named after its judicious use of residual connections.  These connections consist of a normal convolutional layer added elementwise to the convolutional operation input

![residual connections]({{https://blbadger.github.io}}/neural_networks/residual_connection.png)

The intuition behind this architecture is that each convolutional layer learns some function $F(x)$ that is 'referenced' to the original input $x$, meaning that $F(x)$ is not the sole source of information for the next layer as it normally is, but instead the original information in the input $x$ is retained somewhat through addition.

The original [ResNet](https://arxiv.org/abs/1512.03385) paper published a variety of closely related architectures,

![Resnet architectures]({{https://blbadger.github.io}}/neural_networks/resnet_architecture.png)

where each of the sub-modules contain residual connections.  For the 50-layer version of ResNet, 8 of the first 16 features at the final layer of each module are shown below.

![Resnet features]({{https://blbadger.github.io}}/neural_networks/resnet_features.png)

Upon examination, we see the same general features as for GoogleNet and InceptionV3: a color filter in the first convolutional layer followed by simple patterns that become more complex shapes and recognizable parts of animals in later layers. Note too that just as for both other models, features in the final layer appear to contain objects and colors that are rather jumbled together and are less coherent than in prior layers.

### Layer and Neuron Interactions

Observing the images that result from gradient descent on noise gives some coherent and sometimes quite interesting feature maps. One may next wonder how all these maps work together to produce an appropriate classification label during forward propegation, or else how they can work together to [generate an input](https://blbadger.github.io/input-generation.html) representative of a certain target output category.

A first step to understanding how neurons work together has already been shown, where different neurons of one feature have been shown to be somewhat coordinated with respect to the image that results from optimizing their activations simultaneously.  What about multiple neurons or even features, how do they interact in the context of generating an input image?

We therefore seek an input $a'$ that maximizes multiple multiple layers $l_1, l_2, ..., l_n$, which can be equated to maximizing a single value that is the sum of the activations of those layers.

$$
a' = \underset{a}{\mathrm{arg \; max}} \; \sum_{i=1}^{n} z^{l_i}(a, \theta)
$$

For only two features in separate layers $l$ and $k$, the loss may be found by finding the sum of the $L_1$ distances between each layer's activation and some large constant $C$, and therefore the gradient used to find an approximation of the target input $a'$ is

$$
g = \nabla_a (C - z^{l}(a, \theta) + C - z^{k}(a, \theta))
$$

It could be wondered if it would not be better to separate the gradients for each layer and then add them together during the gradient descent step

$$
a_{k+1} = a_k - \epsilon * g
$$

in order to ensure that both features are maximized (and not only one of these).  In practical terms, the automatic differentiation engine in Pytorch gives nearly identical outputs for these two methods, so for simplicity the addition is performed before the gradient descent step.  The following method implements the above equation for two features `focus` and `focus2` that exist in layers `output` and `output2`

```python
def double_layer_gradient(model, input_tensor, desired_output, index):
	...
	input_tensor.requires_grad = True
	output = model(input_tensor).to(device)
	output2 = newmodel2(input_tensor).to(device)

	focus = output[0][index][:][:]
	focus2 = output2[0][index][:][:]

	target = torch.ones(focus.shape).to(device)*200
	target2 = torch.ones(focus2.shape).to(device)*200

	output = model(input_tensor)
	loss = torch.sum(torch.abs(output))
	loss = (torch.sum(target - focus) + torch.sum(target2 - focus2)
	loss.backward() # back-propegate loss
	gradient = input_tensor.grad

	return gradient
```

There are a couple different ways we could specify the layers `output` and `output2`.  Pytorch uses an automatic differentiation approach that requires all gradient sources to be specified as outputs prior to the start of forward propegation.  One option to enforce this requirement is to have one model return multiple outputs, but for clarity the above method assumes multiple models are instantiated.  For features in layers `Conv2d_1z_3x3` and `Conv2d_2z_3x3` of InceptionV3, this can be done as follows:

```python
class NewModel2(nn.Module):

	def __init__(self, model):
		super().__init__()
		self.model = model

	def forward(self, x):
		# N x 3 x 299 x 299
		x = self.model.Conv2d_1a_3x3(x)
		return x
		
class NewModel(nn.Module):

	def __init__(self, model):
		super().__init__()
		self.model = model

	def forward(self, x):
		# N x 3 x 299 x 299
		x = self.model.Conv2d_1a_3x3(x)
		# N x 32 x 149 x 149
		x = self.model.Conv2d_2a_3x3(x)
		return x
```

and these two model classes may be called using a pretrained InceptionV4 model.

```python
Inception3 = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True).to(device)
newmodel = NewModel(Inception3)
newmodel2 = NewModel2(Inception3)
```

We can now observe how two features from arbitrary layers interact. For feature 199 of layer Mixed 6a combined with feature 201 of layer Mixed 6b, we have

![inceptionv3 layer combo]({{https://blbadger.github.io}}/neural_networks/inception3_layer_interaction.png)

Observe how the combination is far from linear: in certain areas of the image, eyes from layer Mixed 6b are found, wheras in other areas they are completely absent. 

For a detailed look at the optimization of many features at once when the original input $a_0$ is a natural image, see [part II](https://blbadger.github.io/deep-dream.html).
## Fractal Geometry

Images of [nonlinear dynamical systems](https://blbadger.github.io/) are typically fractals.  Here we explore the origin and meaning of this term.

### Origin and Cantor sets

The term *Fractal* was chosen by Mandelbrot (after the Latin *Fractus*) to signify irregular, fragmented objects.  These often but do not necessarily have a fractional scaling dimension, and the surface of these objects is non-differentiable.  Spectacular examples of fractals such as the Julia and Mandelbrot sets are often brought to mind when one thinks of fractals, but they are far older and richer than these examples alone.  Nearly a century before the word fractal had been invented, the foundations for this branch of math were established and are here roughly detailed.

In the late 19th century Cantor founded set theory, a study of abstract collections of objects and the relations between these collections.  Other pages in this website make use of set theory, and for a good background on the subject see "Naive set theory" by Halmos.  From set theory, we find fascinating conclusions that lay a foundation for fractal geometry, so to understand the latter it is best to examine a few key aspects of the former. 

Surprising results abound in set theory, but one of the most relevant here pertains to counting how many elements a set has.  It turns out that not all infinite sets are equal: there are as many elements of the set of positive numbers as there are rational numbers (an infinite amount of each) but there are far more elements of the set of real numbers than either of the other two sets.  The elegant diagonal proof for this is worth viewing elsewhere, as it reappears again in other forms (in Turing's computability theories, for example). 

A particularly interesting set is known as the middle thirds Cantor set $C$.  This set will be considered in some detail because it illustrates many of the most important aspects of fractals but is relatively simple, existing on a line.  $C$ is made as follows: take the closed interval on the reals $[0, 1]$ and remove from this an open interval of the middle third, that is, $[0, 1] - (1/3, 2/3)$.  

![cantor]({{https://blbadger.github.io}}/fractals/cantor000.png)

Now repeat this process for each remaining closed intervals,

![cantor]({{https://blbadger.github.io}}/fractals/cantor001.png)

and again

![cantor]({{https://blbadger.github.io}}/fractals/cantor002.png)


![cantor]({{https://blbadger.github.io}}/fractals/cantor003.png)

and so on ad infinitum.  $C$ is the set of numbers that remains after an infinite number of these steps.  This set is remarkable: after n steps of removing the inner third, $(\frac{2}{3})^n$ total length remains.  Therefore $C$ has $0$ total length: $(2/3)^n \to 0 \; as \; n \to \infty$.  If it has $0$ length, does $C$ have any points? It does indeed, just as many points as the original closed interval $[1,0]$!  The set is totally disconnected (no point touches any other point) and perfect (every point is a limit of another set of points in $C$).

This set can be drawn with Turtle, a Python programming library that was designed to help teach programming in a visual way.  It turns out to be quite capable of intricate fractal drawings as well.

```python
from turtle import *
import turtle

def cantor_set(size, recursive_steps):
	if recursive_steps > 0:
		cantor_set(size/3, recursive_steps-1)

		turtle.pu(), turtle.forward(size), turtle.pd()
		cantor_set(size/3, recursive_steps-1)

	else:
		turtle.color('red'), turtle.forward(size)
		turtle.pu(), turtle.forward(size), turtle.pd()
		turtle.color('blue'), turtle.forward(size)


for i in range(300):
	turtle.hideturtle()
	turtle.speed(0), turtle.delay(0)
	turtle.pensize(4)
	turtle.setup (width=1900, height=1080, startx=0, starty=0)
	turtle.pu(), turtle.goto(-800, 0), turtle.pd()

	cantor_set(500 * (2**(i/30)), 4 + i // 30)
	turtle_screen = turtle.getscreen()
	turtle_screen.getcanvas().postscript(file="cantor_set{0:03d}.eps".format(i))
	turtle.reset()
```

The `.eps` files generated by this program are vector images, and to make a video conversion to a fixed size .png or .jpg type file is useful.  Fortunately the Python Image Library has the wonderful ability to detect and reformat all kinds of image types!  I used the following program to convert `.eps` to `.png` files:

```python
from PIL import Image
import os

 # insert path here
path = '/home/bbadger/Desktop/' 

files = os.listdir(path)

for i in range(0, len(files)):
	string = str(i)
	while len(string) < 3:
		string = '0' + string
	try:
		eps_image = Image.open('cantor_set' + string + '.eps') # input names here
	except:
		break
	eps_image.load(scale=2)   
	eps_image.save('cantor{0:03d}.png'.format(i)) # output names here
```
After compiling these `.png` images into an mp4 using ffmpeg in bash

```bash
(base) bbadger@bbadger:~$ ffmpeg -f image2 -r 30 -pattern_type glob -i '*.png' cantor_zoom.mp4
```

the video can be viewed and edited.  Here it is as a .gif (ffmpeg can convert directly into gifs simply by changing the extension name to `cantor_zoom.gif`, but these are uncompressed and can be quite large files)

![cantor zoom]({{https://blbadger.github.io}}/fractals/cantor_zoom1.gif)

As the number of recursive calls increases, the Cantor set becomes invisible.  This should not come as a surprise, being that it is of measure $0$ as $n \to \infty$.  Thus in order to obtain a viewable map with more recursive steps, vertical lines are used to denote the position of each point in the set.  The following program accomplishes this by drawing alternating red and blue vertical lines at the start of where each set interval (at any given step) begins and so is only accurate with a relatively large value for the starting number of recursions (>4).  A black background is added for clarity, and once again the number of recursive steps increases with scale to maintain resolution.  

Starting at the 5th recursive level (code available [here](https://github.com/blbadger/fractal-geometry/blob/master/class_Cantor_zoom.py)) and adding a recursion every 30 frames, we have

![cantor zoom]({{https://blbadger.github.io}}/fractals/cantor_zoom_vertical.gif)

Now this is one particular example of a Cantor set, but there are others: one could remove middle halves instead of thirds etc.  The general definition of a Cantor set is any set that is closed, bounded, totally disconnected, and perfect.  The general Cantor set $C$ is a common occurence in nonlinear maps.  The period doubling in the logistic map forms a Cantor set, although this is not a middle thirds set as seen above 

![logistic_zoom]({{https://blbadger.github.io}}/fractals/logistic_zoom_branches.gif)

as well is in the [henon map](/henon-map.md).

### Space filling curves

What is the dimension of the Cantor set?  Finite collections of points are of dimension $0$, whereas lines are of dimension $1$.  $C$ is totally disconnected and therefore would seem to be $0$ dimensional, and yet it is an infinite collection of points that are bounded to a specific region.  Thus $C$ has characteristics of both zero and one dimensions.  Do any other curves also exhibit properties of multiple dimensions?

One can defined fractals as objects that exhibit properties of multiple dimensions, and in that respect every image on this page is an example of a curve that answers this question. But before exploring those, it may be useful to view some extreme cases: curves (1-dimensinal objects) that fill space (2 dimensions).  One of the first such curves to be discovered to do this by Peano in the late 19th century can be described in python as follows

```python
ls = [90, -90, -90, -90, 90, 90, 90, -90, 0]
def peano_curve(size, recursive_steps, ls):
	if recursive_steps > 0:
		for i in range(len(ls)):
			peano_curve(size, recursive_steps-1, [i for i in ls])
			turtle.left(ls[i])
	else:
		turtle.forward(size)
```
where `recursive_steps` goes to infinity.  To understand how this fills space, observe the base case:

![peano 1]({{https://blbadger.github.io}}/fractals/peano_curve1_1.gif)

and the first recursive step, where each line segment of the curve above is replaced with a smaller version of the whole curve:

![peano 1]({{https://blbadger.github.io}}/fractals/peano_curve1_2.gif)

After a few more recursive steps (only 5 in total!) , the present resolution is no longer able to differentiate between one line and another and we have achieved something close to a space-filling curve.

![peano 1]({{https://blbadger.github.io}}/fractals/peano_surface.gif)

This curve fills space but crosses over itself.  The following is another space filling curve from Peano that does not self-cross but is more difficult to draw.  The L -system, named after its discoverer Lindenmayer, is a very useful system for characterizing the generation of more complex recursive structures.  For a good overview of this system complete with examples, see [here](http://paulbourke.net/fractals/lsys/).  The Peano curve may be defined in the L-system by the sequences `X = 'XFYFX+F+YFXFY-F-XFYFX', Y = 'YFXFY-F-XFYFX+F+YFXFY'` where X and Y are separate recursive sequences, '+' signifies a turn left by 90 degrees, '-' a turn right 90 degrees, and 'F' signifies a movement forward.  This can be implemented in python by interpreting each L-system element separately as follows:

```python
def peano_curve(size, steps, orientation):
	X = 'XFYFX+F+YFXFY-F-XFYFX'
	Y = 'YFXFY-F-XFYFX+F+YFXFY'
	l = r = 90

	if steps == 0:
		return

	if orientation > 0:
		for i in X:
			if i == 'X':
				peano_curve(size, steps-1, orientation)
			elif i == 'Y':
				peano_curve(size, steps-1, -orientation)
			elif i == '+':
				turtle.left(90)
			elif i == '-':
				turtle.right(90)
			else:
				turtle.forward(size)

	else:
		for i in Y:
			if i == 'X':
				peano_curve(size, steps-1, -orientation)
			elif i == 'Y':
				peano_curve(size, steps-1, orientation)
			elif i == '+':
				turtle.left(90)
			elif i == '-':
				turtle.right(90)
			else:
				turtle.forward(size)
```   

The base case 

![peano 2]({{https://blbadger.github.io}}/fractals/peano_2_1.png)

and first recursion 

![peano 2]({{https://blbadger.github.io}}/fractals/peano_2_2.png)

at the fifth recursion level, 

![peano 2]({{https://blbadger.github.io}}/fractals/peano_surface2.gif)

As is the case in the first Peano curve, the true space-filling curve is the result of an infinite number of recursive steps.  This means that the curve is also infinitely long, as the length grows upon each recursive step.  Infinite length is a prerequisite for any space-filling curve.

Note that both of these curves are nowhere-differentiable: pick any point on the curve, and it is an angle (a 90 degree angle to be precise) and as angles are non-differentiable, the curve is non-differentiable.  

These curves map a line of infinite length to a surface, and the mapping is continuous.  But importantly, this mapping is not one-to-one (injective): multiple points on the starting line end up at the same spots in the final surface. In fact no injective mapping exists between one and two dimensions exists, and a geometric proof of this in the second Peano curve is as follows: observe the point at the upper right corner of the curve $p_1$ and the point directly below it $p_1$ to be $p_2$. Upon each recursion $r$, the distance between these points is divided by 3 such that 

$$
d(p_1, p_2)_{r+1} = \frac{d(p_1, p_2)_{r}}{3} \\ 
\; \\
\frac{1}{3^n} \to 0 \; as \; \; n \to \infty \\
\; \\
d(p_1, p_2) \to 0 \; as \; \; r \to \infty
$$

Now the true Peano curves are infinitely recursive, therefore this the distance between these points is 0 and therefore $p_1$ and $p_2$ map to the same point in two dimensional space, making the Peano curve a non-injective mapping.

Imagine moving along the surface created by either peano curve. An arbitrarily small movement in one direction along this surface does not necessarily lead to a small movement along the curve itself.  Indeed it can be shown that any mapping from two to one dimensions (which could be considered to be equivalent to the definition of a space filling curve) is nowhere-continuous if the mapping is one-to-one and onto.  For some interesting repercussions of this on machine learning methods such as neural networks, see [here](/neural-networks.md).

### Fractals as objects of multiple dimensions

The Koch curve may be drawn as follows

```python
ls = [60, -120, 60, 0]
def koch_curve(size, recursive_steps, ls):
	if recursive_steps > 0:
		for i in range(len(ls)):
			koch_curve(size, recursive_steps-1, [i for i in ls])
			turtle.left(ls[i])

	else:
		turtle.forward(size)
```

The curve starts as

![koch]({{https://blbadger.github.io}}/fractals/koch_snowflake1.png)

and at each recursion, each line segment is divided into parts that resemble the whole. At the 2nd,

![koch]({{https://blbadger.github.io}}/fractals/koch_snowflake2.png)

3rd,

![koch]({{https://blbadger.github.io}}/fractals/koch_snowflake3.png)

and 6th recursion:

![koch]({{https://blbadger.github.io}}/fractals/koch_snowflake6.png)

Now this curve evidently does not cover the plane like the Peano curves. But the curve does seem 'fuzzy', and that it might cover at least part of the plane. In this respect, it seems to be partway between dimensions. Is this the case?

A better understanding comes from the similarity dimension, which is equivalent to the Hausdorff dimension for the following self-similar objects. First note that Euclidean objects like a point, line, or surface have the same topological dimension as their similarity dimension: a point cannot be subdivided ($n^0 = 1$), a line of length n can be subdivided into $n^1 = n$ pieces, and a surface square of length n can be subdivided into $n^2$ pieces.  Now note that the Koch curve may be subdivided into four equal pieces, and that these pieces are $1/3$ the length of the total curve.  It's similarity dimension is therefore

$$
D = \frac{\log N}{\log (1/r)} \\
\; \\
D = \frac{\log 4}{\log 3} \approx 1.2618
$$

Now consider the following curve, known as the quadric Koch curve:

```python
ls = [90, -90, -90, 0, 90, 90, -90, 0]
def quadric_koch_curve(size, recursive_steps, ls):
	if recursive_steps > 0:
		for i in range(len(ls)):
			quadric_koch_curve(size, recursive_steps-1, ls)
			turtle.left(ls[i])

	else:
		turtle.forward(size)
```
The curve starts as follows:

![qkoch]({{https://blbadger.github.io}}/fractals/koch1.png)

in the first recursion,

![qkoch]({{https://blbadger.github.io}}/fractals/koch2.png)

and after 5 recursive levels, 

![qkoch]({{https://blbadger.github.io}}/fractals/koch5.png)

Let's calculate this curve's similarity dimension: there are 8 pieces that are smaller versions of the whole curve, and these pieces are $1/4$th the length of the whole so therefore

$$
D = \frac{\log N}{\log (1/r)} \\
\; \\
D = \frac{\log 8}{\log (4)} = 1.5 \\
$$

This curve has a slightly larger dimension than the other Koch curve, which could be interpreted as being that this curve is closer to a surface than the first Koch curve.  Visually, this results in the appearance of a rougher line, one that appears to cover more area than the first.  How long are these curves? Each recursion adds length so just like the space-filling curves, the total length is infinite.  

We can also calculate the dimension of the Cantor set.  There are 2 collections of points that are identical in structure to the entire collection, which we can call L and R.  These collections take up a third of the original interval, and so the dimension of $C$ is

$$
D = \frac{\log N}{\log (1/r)} \\
\; \\
D = \frac{\log 2}{\log (3)} \approx 0.631
$$

which is somewhere between $0$ and $1$, and this matches the observations that $C$ exhibits properties of both $0$ and $1$ dimensional objects.

Fractals are defined as shapes that have a Hausdorff dimension greater than their topological dimension.  For the shapes presented on this page, Hausdorff and scaling dimensions are equal.  Thus the curves in this section are all fractals, as are the Cantor set and both space-filling Peano curves.  

### More Selfsimilar fractals

The Sierpinski triangle (which will resemble a triforce for those of you who played Zelda) is one of the most distinctive fractals presented in the book. There are two orientations the curve takes, which means that the drawing proceeds as follows:

```python
ls = [60, 60, -60, -60, -60, -60, 60, 60, 0]
def sierpinski_curve(size, recursive_steps, ls):
	if recursive_steps == 0:
		turtle.forward(size)
	else:
		for i in range(len(ls)):
			if i % 2 == 0:
				sierpinski_curve(size, recursive_steps-1, [i for i in ls])
				turtle.left(ls[i])
			else:
				sierpinski_curve(size, recursive_steps-1, [-i for i in ls])
				turtle.left(ls[i])

```
The starting point is

![sierpinski]({{https://blbadger.github.io}}/fractals/sierpinski1.png)

and each line becomes as smaller version of the whole upon the first recursion

![sierpinski]({{https://blbadger.github.io}}/fractals/sierpinski2.png)

![sierpinski]({{https://blbadger.github.io}}/fractals/sierpinski3.png)

And after a few more recursive levels, the following curve is produced:

![sierpinski]({{https://blbadger.github.io}}/fractals/sierpinski.gif)

The following fractal is reminiscent of the appearence of [endocytosis] in cells

```python
def endocytic_curve(size, recursive_steps):
	if recursive_steps > 0:
		for angle in [60, -60, -60]:
			turtle.left(angle)
			endocytic_curve(size, recursive_steps-1)
		turtle.left(60)
	else:
		turtle.forward(size)
```

![endocytic_fractal]({{https://blbadger.github.io}}/fractals/endocytosis001.png)

![endocytic_fractal]({{https://blbadger.github.io}}/fractals/endocytosis002.png)

![endocytic_fractal]({{https://blbadger.github.io}}/fractals/endocytosis003.png)

![endocytic_fractal]({{https://blbadger.github.io}}/fractals/endocytosis004.png)

![endocytic_fractal]({{https://blbadger.github.io}}/fractals/endocytosis.gif)

and here is a modifed version of Sierpinski's triangle that makes fractal snowflakes:

```python
def snowflake_curve(size, recursive_steps):
	if recursive_steps > 0:
		for angle in [60, 60, -60, -60, -60, -60, 60, 60, 0]:
			snowflake_curve(size, recursive_steps-1)
			turtle.left(angle)

	else:
		turtle.forward(size)
```
![snowflake_fractal]({{https://blbadger.github.io}}/fractals/snowflake001.png)

![snowflake_fractal]({{https://blbadger.github.io}}/fractals/snowflake002.png)

![snowflake_fractal]({{https://blbadger.github.io}}/fractals/snowflake003.png)

![snowflake_fractal]({{https://blbadger.github.io}}/fractals/snowflake_fractal.gif)

Note that there are smaller snowflakes on the periphery of larger snowflakes: if the recursions were infinite, there would be an infinite number of smaller and smaller snowflakes on these little snowflakes. To save time, the previous image was obtained at a recursive depth of 6.

For more classic fractals and a number of very interesting ideas about fractal geometry in general, see Mandelbrot's [book](https://books.google.com/books/about/The_Fractal_Geometry_of_Nature.html?id=0R2LkE3N7-oC).  

### Fractal dimension from box counting

Say one wants to calculate the dimensions of images that are not easily interpreted as self-similar zones as described above.  Perhaps the simplest way to do this is to convert the Kolmogorov definition of dimension to a box counting one, which can be proven to have the same limit as the Kolmogorov dimension.

Let's implement the box counting algorithm.  First, importing relevant libraries
```python
# fractal_dimension.py 
# calculates the box counting dimension for thresholded images

from PIL import Image
import numpy as np 
import matplotlib.pyplot as plt
import os
```

Now for the box counting:
```python
def box_dimension(image_array, min_resolution, max_resolution):
	"""
	Takes an input array (converted from image) of 0s and 
	1s and returns the calculated box counting dimension
	over the range of box size min_resolution (int) to 
	max_resolution (int > min_resolution)
	"""
	assert max_resolution <= min(len(image_array), len(image_array[0])), 'resolution too high'

	counts_array = []
	scale_array = []
	y_size = len(image_array)
	x_size = len(image_array[0])
	for i in range(min_resolution, max_resolution, 5):
		scale_array.append(i)
		count = 0
		for j in range(0, y_size - i, i):
			for k in range(0, x_size - i, i):
				if check_presence(image_array, i, j, k):
					count += 1
		counts_array.append(count)

	# log transform scales and counts
	counts_array = [np.log(i) for i in counts_array]
	scale_array = [np.log(i) for i in scale_array]
	m, b = np.polyfit(scale_array, counts_array, 1) # fit a first degree polynomial

	return m, counts_array, scale_array
```

which calls the helper function
```python
def check_presence(image_array, i, j, k):
	"""
	Checks for the presence of 1 in a square subarray
	of length i with top left coordinates (j, k).  Returns
	a boolean indicating presence.
	"""
	for x in range(i):
		for y in range(i):
			if image_array[j+y][k+x] == 1:
				return True

	return False
```
An image must be converted to an array, which forms the input to our program.  The following converts an image to an array and thresholds it, in this case for gray pixels on a white background.
```python
path = '/home/bbadger/Desktop/sierpinski/snowflake178.png'

image_array = np.asarray(Image.open(path))
image_ls = []
for i in range(len(image_array)):
	temp = []
	for j in range(len(image_array[i])):
		# thresholding: <130 to find gray pixels
		if any(image_array[i][j] < 130):
			temp.append(1)
		else:
			temp.append(0)
	image_ls.append(temp)

m, counts_array, scale_array = box_dimension(image_ls, 1, 500)
print (f'Fractal dimension: {-m}')

plt.scatter(scale_array, counts_array)
plt.xlabel('log r')
plt.ylabel('log N')
plt.show()
plt.close()
```
Note that this program is designed for clarity rather than speed, and takes a little over a minute to run for a 1500x2100 image.  A faster version may be found [here](https://github.com/blbadger/fractal-geometry/blob/master/fractal_dimension_optimized.py), which reduces this time by a factor of two.

Let's check this program by using it to calculate the fractal dimension of an object where we can easily find the Hausdorf (scaling) dimension, such as the Sierpinski triangle.  For this object, halving the length results in a copy of the original that is 1/3 the 'volume', thus this has a Hausdorff dimension of 

$$
D = \frac{\log N}{\log (1/r)} = \frac{\log 3}{\log 2} \approx 1.585
$$

Using box sizes from 1 to 500 pixels, our program yields 

![sierpinski_dimension]({{https://blbadger.github.io}}/fractals/sierpinski_box.png)

which implies a dimension of $d \approx 1.582$, which is close to the true value.  For another example, the similarity dimension of the Koch curve is $\log 4 / \log 3 \approx 1.26$, and with box sizes ranging from 1 to 500 pixels our dimensional calculator estimates $d = 1.23$, which is again fairly accurate.  

We can also use this program to estimate the dimension of other fractals.  The ensuing log/log plot from the snowflake fractal (the last presented in the previous section) is

![snowflake_dimension]({{https://blbadger.github.io}}/fractals/box_counting.png)

and the dimension is found to be $d \approx 1.45$.

### Fractals in the natural world

When I was a kid and just learning how to plot polynomial functions, I remember being quite disappointed to find that any such function describing the objects in the natural world would have to be extremely complicated and unwieldy.  Nearly every shape in nature, from the outline of the trees around my parent's house to the pattern or surf on the seaside to the shapes of nebulae and galaxies I saw in astronomy books were all, as I found, unsuitable for attempting to recreate using the functions I knew then (polynomial and trigonometric functions). 

I did not know at the time, but there is a good reason from this.  These shapes are non-differentiable: increasing in scale does not lead to a simpler, more line-like shape.  Rather, one finds more detail the closer one looks.  These shapes are better mathematically described using fractal geometry, and why this is should be evident from the fractals that have been drawn above.  Fractals also contain more detail the closer one looks, and thus are non-differentiable.  Simple rules specify intricate shapes.

Fractals can be thought of as shapes that are irregular and do not become more regular as one increases scale, as well as shapes that have smaller parts that resemble the whole.  TA particularly noteworthy example of both of these properties exists in coastlines.  Here is a picture of the Chesapeake Bay taken from a satellite which is found in the Smithsonian museum. 

![chesapeake bay]({{https://blbadger.github.io}}/fractals/chesapeake_bay.png)

Observe how the large bay has many smaller bays that, though not identical in shape, closely resemble the whole bay.  These inlets in turn have smaller inlets and so on past the resolution limit of this image.  This makes the coastline irregular on a large scale but importantly that this irregularity does not diminish with an increase in scale.  

The length of the coastline in the image above is entirely dependent on how high the resolution of the image is: a higher resolution image will yield a larger measurement.  This is not the case for smooth objects that we like to measure, such as the desk I am writing on.  In addition, fractals are non-differentiable: calculus cannot be accurately applied to these objects because at many points they do not have a defined tangent curve. 

What does this matter, beyond the purpose of attempting to model nature in the most parsimonious fashion?  The observation that most objects in the natural world are best described as fractals is important because it implies that nearly all natural dynamical processes are nonlinear.  Being that the vast (vast) majority of all possible mathematical functions are nonlinear, it may come as little surprise that most things that change over time in the natural world are best described using nonlinear functions.  But consider what this means for scientific inquiry, both past and present.  Linear mathematical transformations are scaling and additive: they can be decomposed into parts and then reassembled.  Nonlinear transformations are not additive and therefore cannot be analyzed by decomposition into parts.

Linearity is often assumed when experimental scientific models are constructed and tested.  For an example, first consider the Schrodinger equation which forms the basis of the wave equations of quantum mechanics. This equation is linear, and if it were not then the principle of superposition would not be applicable.  

The fractal geometries seen at many scales in nature suggest that linear models are generally not accurate, as only nonlinear dynamical systems can exhibit fractal attractors.  And in observational scientific disciplines such as general relativity, for example, models are often based on nonlinear differential equations.  But there is something important to remember about such equations: they are unsolvable except under special circumstances.  This means that we can predict how celestial bodies may curve spacetime, but these predictions are necessarily imperfect.  









### Fusors: internal electrostatic containment

Fusors use electrostatic potential to confine plasma in order to accomplish nuclear fusion and are currently used for neutron production (a by-product of Deuterium-Tritium or Deuterium-Deuterium fusion) for the production of certain radioactive isotopes such as Mo-99.  Net power generation remains out of reach for the fusor or related designs (see Bussard's polywell), as is currently the case for all fusion devices.  

### Fusor under high power 

THe first fusor was the Farnsworth-Hirsch design that directed ion beams (or electron beams) at a central location in an evacuated chamber to create a stable area of plasma with high electrical potential. This is a demo model of a Hirsch-Meeks fusor, which instead uses two charged grids to form an electrostatic potential well inside the inner grid.  This model here was built only to display the plasma containment of the inner grid and does not fuse nuclei to any significant extent. 

![Fusor: high power]({{https://blbadger.github.io}}fusor_images/fusor-1-1.png)

The small vacuum chamber self-rectifies, and the fusor is running here on a 14 kV @ 70mA AC input.  

![Fusor: high power]({{https://blbadger.github.io}}fusor_images/fusor_full_2.png)

The whole device heats up rapidly under full power, and moreover the plasma burns the inner face of the borosilicate glass over time.  The inner grid becomes especially hot, and on longer runs was capable of melting the 2 part epoxy used to seal the chamber.

![Fusor: high power zoom]({{https://blbadger.github.io}}fusor_images/fusor_zoom.png)


### Fusor under low power (~400 V AC)

At a vacuum estimated at 50 microns, this fusor design ignites plasma at under 400 volts of AC input.  

![Fusor: low power]({{https://blbadger.github.io}}fusor_images/fusor-2.png)

Note the dark regions around the inner grid, also visible in the high power images above.  These are regions of high potential.  THe electron distribution function is

$$
f(u) = Ae^{\frac{-((1/2)mu^2+q\phi)}{KT_e}}
$$

where $q\phi$ is the potential energy, $KT_e$ is the thermal energy of the particles, $(1/2)mu^2$ is the electron's kinetic energy, and $A$ is the constant

$$
A = n \left(\frac{m}{2\pi KT} \right)^{1/2}
$$

$n$ is the plasma density, $K$ is Boltzmann's constant, and $m$ is the mass of the particle (in this case an electron). 

Note what this implies: as potential increases, the number of electrons decreases.  The areas of highest potential are adjacent to the grid, which is an (albeit very idealized) explanation for why the plasma at least partially avoids the grid itself. 


### Fusor vacuum chamber, power off

Fusor contain plasma with an electrostatic grid pair.  Here, the outer 'grid' is the steel vacuum coupling visible on the left and the inner grid is made of solid copper, visible front and center.  The chamber is approximately 7 inches in length and uses thick borosilicate glass (to prevent implosion from the pressure difference accross the chamber), and stainless steel couplings designed for use in high vacuum are attached via two-part epoxy to the ends of the borosilicate chamber.  The red tube on the left travels to the vacuum pump.  I experimented with a surplus high vacuum single stage pump (rated at 5 microns) as well as a standard HVAC single stage pump rated at 40 microns.  The latter was used for most of the ignitions that are imaged here. 

![vacuum chamber assembly]({{https://blbadger.github.io}}fusor_images/fusor-3.png)

Note that one should only use glass that will not shatter if exposed to a temperature gradient for any kind of fusor! Borosilicate comes under various brand names and is far more resistance to temperature gradient-induced damage than standard silica glass. 

Borosilicate is relatively transparent to UV ratiation, and as a fusor running a the above voltages can put out quite a lot of this it is necessary to add shielding that is opaque to UV.  Most clear plastics fulfill this requirement, although it is important to check the material used to be sure.  I used a polycarbonate shield which doubled as a safety barrier to mitigate any effects of chamber implosion.  The chamber never did fail, but one does not want flying shards of glass going everywhere.  Also keep in mind that x-ray radiation becomes an issue for higher potentials, as borosilicate is quite transparent to that as well. 



## Generative Adversarial Networks

### Introduction

Perhaps the primary challenge of using gradient descent on the input method for image generation (see [here](https://blbadger.github.io/input-generation.html) for more on this topic) is that the trained model in question was not tasked with generating inputs but with mapping them to outputs.  With complicated ensembles composed of many nonlinear functions like neural networks, forward and reverse functions may behave quite differently. Instead of relying on our model trained for classification, it may be a better idea to directly train the model to generate images.

One method that takes this approach is the generative adversarial network model [introduced](https://arxiv.org/abs/1406.2661) by Goodfellow and colleages.  This model used two neural networks that compete against each other (hence the term 'adversarial', which seems to have a different motivation than this term is used in the context of 'adversarial examples').  One network, the discriminator, attempts to distinguish between real samples that come from a certain dataset and the generated samples that come from another network, appropriately named the generator.  Theoretically, these two networks can compete against each other using the minimax algorithm to play a zero-sum game, although in practice GANs are implemented differently as minimax appears to be unstable in practice.

This method is of historical significance because it was a point of departure from other generative methods (Markov processes, inference networks etc.) that rely on averaging, either over the output space or the model parameter space.  Generative adversarial networks generate samples with no averaging of either.

For discriminator model parameters $\theta_d$ and generator parameters $\theta_g$

$$
f(d, g) = \mathrm{arg} \; \underset{g}{\mathrm{min}} \; \underset{d}{\mathrm{max}} \; v(\theta_d, \theta_g)
$$

Following the zero-sum game, the minimax theorem posits that one player (the discriminator) wishes to maximize $v(\theta_d, \theta_g)$ and the other (in this case the generator) wishes to minimize $-v(\theta_d, \theta_g)$.  We therefore want a value functions $v$ that grows as $d(x)$ becomes more accurate given samples $x$ taken from the data generating distribution $p(\mathrm{data})$ and shrinks (ie grows negatively) as $x$ is taken from outputs of the generator, denoted $p(\mathrm{model})$.  One such option is as follows:

$$
v(\theta_d, \theta_g) = \Bbb E_{x \sim p_{data}} \log d(x) + \Bbb E_{x \sim p_{model}} \log(1-d(x))
$$

It is worth verifying that this value function does indeed satisfy our requirements.  If we are given a set $x$ of only real examples $x \sim p_{data}$, $\Bbb E_{x \sim p_{model}} \log(1-d(x))$ can be disregarded as this is now an expectation over an empty set. A perfect discriminator would give classify all examples correctly, or $d(x) = 1$ making 

$$
\Bbb E_{x \sim p_{data}} \log d(x) = 0
$$

As $d(x) \in [0, 1]$, it is clear that $v(\theta_d, \theta_g) \to 0$ as $d(x) \to 1$ and therefore $v$ increases to 0 from some negative starting point as the accuracy of $d(x)$ increases, meaning that the discriminator has maximized $v$.

Because of the log inverse function $\log(1-d(x))$ for the second term of $v(\theta_d, \theta_g)$, the opposite is true for the generator: if we assemble a dataset $x$ of examples only from the generator's output, and if the generator was optimized at the expense of the discriminator, then the discriminator would predict the same output for the generated samples as for the real ones, or $d(x) = 1$. Therefore if the generator is optimized $d(g(x)) = 1$, 

$$
\Bbb E_{x \sim p_{model}} \log(1-d(x)) = \log(1 - 1) \\
= -\infty
$$ 

then so too is $-v$ minimized, which was what we wanted.

The goal is for $g$ to converge to $g'$ such that $d(x) = 1/2$ for each input $x$ regardless of whether it was generated or not, which occurs when the generator emits inputs that are indistinguishable (for the discriminator) from the true dataset's images. 

Formulating the generative adversarial network in the form of the zero-sum minimax game above is the theoretical basis for GAN implementation.  Unfortunately, this is a rather unstable arrangement because if we use $v$ and $-v$ as the payoff values, the generator's gradient tends to vanish when the discriminator confidently rejects all generated inputs.  During typical training runs this tends to happen, meaning that the desired state in which the discriminator neither rejects nor accepts all generated inputs is unstable.  This does not mean that one could not train a GAN using a zero-sum minimax, but that choosing a value function that avoids instability is apparently rather difficult.

Goodfellow and colleages found that it is instead better to make the loss function of the generator equivalent to the log-probability that the discriminator has made mistake when attempting to classify images emitted from the generator, with a value (loss) function of binary cross-entropy for both discriminator and generator. The training process is no longer a zero-sum minimax game or even any other kind of minimax game, but instead is performed by alternating between minimization of cross-entropy loss of $d(x)$ for the discriminator and maximization of the cross-entropy loss of $d(g(z))$ for the generator, where $z$ signifies a random variable vector in the generator's latent space.

The loss for the discrimator is binary cross-entropy between the predicted outputs, which are either 1 or 0 depending on if the discriminator thinks an image is real or fake,

$$
d \in \{ y, 1-y \} 
$$

and actual labels,

$$
q \in \{ \widehat y, 1 - \widehat y \}
$$

which is denoted as 

$$
H(d, q) = -\sum_i d_i \log q_i \\
= -y \log \widehat y - (1-y) \log(1-\widehat y) 
$$

where $P(x)$ is equal to the distribution of $x$ over a mix of real and generated input and $Q(x)$ is the distribution of correct classifications of $P(x)$.  

In contrast, the generator's ojective is to fool the discriminator and so the target distribution $q'$ becomes $q' = 1 - q \in {\widehat y, 1 - \widehat y}$, or in other words the generator uses the same binary cross-entropy applied to the discriminator but now with the labels reversed.

It is worth considering what this reformulation entails. For a single binary random variable, the Shannon self-entropy is as follows:

<!--- 
$$
H(x) = p \log (p) - (1-p) \log(1-p)
$$

Plotting this equation with $p$ on the x-axis and $H(x)$ on the y-axis, we have
![entropy]({{https://blbadger.github.io}}/misc_images/entropy.png)

Shannon entropy is largest where $p = 1-p = 1/2$, which is precisely what we are attempting 
--->

### Implementing a GAN

The process of training a generative adversarial network may be thought of as consisting of many iterations of the two steps of our approximate minimax program above: first the discriminator is taught how to differentiate between real and generated images by applying the binary cross-entropy loss to a discriminator's predictions for a set of generated samples and their labels followed by real samples and their labels,

```python
def train_generative_adversaries(dataloader, discriminator, discriminator_optimizer, generator, generator_optimizer, loss_fn, epochs):
	discriminator.train()
	generator.train()

	for e in range(epochs):
		for batch, (x, y) in enumerate(dataloader):
			discriminator_optimizer.zero_grad()
			random_output = torch.randn(16, 5)
			
			generated_samples = generator(random_output)
			discriminator_prediction = discriminator(generated_samples)
			output_labels = torch.zeros(len(y), dtype=int) # generated examples have label 0
			discriminator_loss = loss_fn(discriminator_prediction, output_labels)
			discriminator_loss.backward()

			discriminator_prediction = discriminator(x)
			output_labels = torch.ones(len(y), dtype=int) # true examples have label 1
			discriminator_loss = loss_fn(discriminator_prediction, output_labels)
			discriminator_loss.backward() # add to previous loss
			discriminator_optimizer.step()
```

and then the generator is taught how to make images that fool the discriminator by finding the loss between the discriminator's predictions for generated samples and a set of labels for real data.
```python
			...
			generated_outputs = generator(random_output)
			discriminator_outputs = discriminator(generated_outputs)
			generator_loss = loss_fn(discriminator_outputs, torch.ones(len(y), dtype=int)) 
			
			generator_optimizer.zero_grad()
			generator_loss.backward()
			generator_optimizer.step()
```

The discriminator's architecture is the same as any other network that maps $\Bbb R^n \to \{0, 1\}$.  For a small image set such as the Fashion MNIST, we could have a multilayer perceptron with input of size `28*28=784`, followed by a hidden layers and an output of size 1 as follows

```python
class FCnet(nn.Module):

	def __init__(self):
		super().__init__()
		self.input_transform = nn.Linear(28*28, 1000)
		self.d1 = nn.Linear(1000, 400)
		self.d2 = nn.Linear(400, 200)
		self.d3 = nn.Linear(200, 1)
		self.relu = nn.ReLU()
		self.sigmoid = nn.Sigmoid()
		self.dropout = nn.Dropout(0.1)

	def forward(self, input_tensor):
		out = self.input_transform(input_tensor)
		out = self.relu(out)
		out = self.dropout(out)

		out = self.d1(out)
		out = self.relu(out)
		out = self.dropout(out)

		out = self.d2(out)
		out = self.relu(out)
		out = self.dropout(out)

		out = self.d3(out)
		out = self.sigmoid(out)
		return out
```

The generator may be a different architecture altogether, but here we can use an interted form of our MLP above.  The latent space consists of 50 elements, which we will feed as 50 floating point numbers from a normal distribution.

```python
class InvertedFC(nn.Module):

	def __init__(self):
		super().__init__()
		self.input_transform = nn.Linear(1000, 28*28)
		self.d3 = nn.Linear(400, 1000)
		self.d2 = nn.Linear(200, 400)
		self.d1 = nn.Linear(50, 200)
		self.relu = nn.ReLU()
		self.tanh= nn.Tanh()

	def forward(self, input_tensor):
		out = self.d1(input_tensor)
		out = self.relu(out)
		
		out = self.d2(out)
		out = self.relu(out)
		
		out = self.d3(out)
		out = self.relu(out)
		
		out = self.input_transform(out)
		out = self.tanh(out)
		return out
```

Using a latent space input of 100 random variables assigned in a normal distribution with $\sigma=1$ and $\mu=0$ followed by hidden layers of size 256, 512, and 1024 we find that the generator is able to produce images that are similar to the fashion mnist dataset.  Here we observe a random sample of 100 latent space vectors over the training process (50 epochs)

{% include youtube.html id='7FdAfskr4is' %}

### Latent Space Exploration

Generative adversarial networks can go beyond simply producing images that look like they came from some real dataset: assuming that the relevant features of the input dataset's distribution $p_{data}(x)$ have been captured by the generator, they can also be used to observe similarities in the inputs in a (usually) lower-dimensional tensor, the latent space.  For the above video, each image is produced from 100 random numbers drawn from a normal distribution.  If we change those numbers slightly, we might find changes in the output that lead to one image being transformed into another. In particular, the hope is that some low-dimensional transformation in the latent space yields a high-dimensional transformation in th egenerator's output.

Somewhat surprisingly, this is indeed what we find: if move about in the latent space, the generated output changes fairly continuously from one clothing type to another. As an example, we can move through the 100-dimensional latent space in a 2-dimensional plane defined by simply adding or subtracting certain values from some random normal input.  We can move in different ways, and one is as follows:

$$
f(a, i, j) = a_{0:19} + i/4,\\
	a_{20:39} - j/4, \\
	a_{40:59} + i/4, \\
	a_{60:79} - j/4, \\
	a_{80:99} + j/4
$$

which may be implemented as follows:

```python
discriminator.load_state_dict(torch.load('discriminator.pth'))
generator.load_state_dict(torch.load('generator.pth'))

fixed_input = torch.randn(1, 100)
original_input = fixed_input.clone()

for i in range(10):
	for j in range(10):
		next_input = original_input.clone()
		next_input[0][0:20] += .25 * i
		next_input[0][20:40] -= .25 * j
		next_input[0][40:60] += .25 * i
		next_input[0][60:80] -= .25 * j
		next_input[0][80:100] += .25 * j
		fixed_input = torch.cat([fixed_input, next_input])

fixed_input = fixed_input[1:]
output = generator(fixed_input)
output = output.reshape(100, 28, 28).cpu().detach().numpy()
```

when we observe the elements of `output` arranged such that the y-axis corresponds to increass in `i` and the x-axis (horizontal) corresponds to increases in `j`, we have the following:

![manifold]({{https://blbadger.github.io}}/neural_networks/fmnist_manifold.png)

Does manifold learning occur for generative adversarial networks trained on other datasets?  We can apply our model to the MNIST handwritted digit dataset by loading the training data

```python
train_data = torchvision.datasets.MNIST(
	root = '.',
	train = True,
	transform = tensorify_and_normalize,
  	download = True
	)
```
Now most images in the MNIST dataset contain lots of blank space, meaning that for a typical input most tensor elements are 0s.  This make GAN learning difficult, so it is a good idea to enforce a non-zero mean on our inputs.  One option that seems to work well is to set the mean $\mu$ and standard deviation $\sigma$ to be given values by performing the following element-wise operation

$$
a_i = \frac{a_i - \mu}{\sigma}
$$

and this can be implemented using `torchvision.transforms` as follows:

```python
tensorify_and_normalize = transforms.Compose(
    [transforms.ToTensor(), transforms.Normalize((0.5), (0.5))]
)
```

In the last illustration, we moved through 100-dimensional space along a 2-dimensional hyperplane to observe the latent space manifold.  This makes it somewhat confusing to visualize, so instead we can simply begin with a 2-dimensional manifold as our latent space, with a generator architecture as follows:

```python
class InvertedFC(nn.Module):
	def __init__(self):
		super().__init__()
		self.input_transform = nn.Linear(1024, 28*28)
		self.d3 = nn.Linear(512, 1024)
		self.d2 = nn.Linear(248, 512)
		self.d1 = nn.Linear(2, 248)
		...
```

Now that the latent space is two-dimensional, we can observe the manifold more clearly in a plane. As generator latent space inputs are random normal variables ($\mu=0$, $\sigma=1$), we know most of the inputs are in near the origin. Tracing out a square from (-2, -2) to (2, 2)

```python
discriminator.load_state_dict(torch.load('discriminator.pth'))
generator.load_state_dict(torch.load('generator.pth'))
fixed_input = torch.tensor([[0.,  0.]]).to(device)
original_input = fixed_input.clone()

for i in range(16):
  for j in range(16):
    next_input = original_input.clone()
    next_input[0][0] = 2 - (1/4) * (i) + original_input[0][0]
    next_input[0][1] = -2 + (1/4) * (j) + original_input[0][1]
    fixed_input = torch.cat([fixed_input, next_input])

fixed_input = fixed_input[1:]
```
we see that indeed nearly every digit is found in this region of the latent space, as expected.

![manifold]({{https://blbadger.github.io}}/neural_networks/mnist_2latent_fig.png)

We can perform the same procedure for the Fashion MNIST dataset by training a GAN with a latent space of size 2. 

![manifold]({{https://blbadger.github.io}}/neural_networks/fmnist_manifold2.png)

Do generative adversarial networks tend to prefer a stereotypical generator configuration $\theta_s$ on the latent space over other possible configurations? To be concrete, do GANs of one particular architecture when trained repeatedly on the same dataset tend to generate the same images for a given coordinate $a_1, a_2, ..., a_n$ in the latent space?  Visualization is often difficult for a high-dimensional latent space, but is perfectly approachable in two dimensions. We can therefore design a GAN with a two-dimensional latent space as has been done above and observe whether or not the same generated images are found at any given coordinate for multiple training runs.  The answer is no, for any coordinate $a_n$ in latent space we do not find a similar generated image from one training run to the next. 

### Continuity and GAN stability

It is interesting to observe how relatively difficult it is to train a GAN, especially compared to the process of training an image classification model alone.

### Unstable Convolutional GANs

For large images, fully connected network GANs become less practical due to the exponential number of trainable parameters in both generator and discriminator.  Convolutional neural networks generally perform very well at object recognition tasks, and so it is natural to wonder whether they would also make effective generative networks too.

Convolutional neural networks have been historically viewed as difficult to use as discriminator/generator pairs in the GAN model.  Empirically this has been attributed to their tendancy to lead to instabilities while training: either the discriminator may become much more effective than the generator such that all generated inputs are confidently rejected, or else the generator may be able to fool the discriminator early in the training program, which reduces the objective gradient for the discriminator and thus prevents effective learning.

Some of the difficult stems from the nature of the convolution, which as defined in the context of deep learning signifies a mathematical function on tensors of real numbers that is strictly non-invertable as it is non-injective.  To see why this is, take the simple example of a convolution on a two-element array of one dimension, with a kernal of $\gamma= [\gamma_1, \gamma_2]$ and no padding:

$$
f([a, b], \gamma) = a \gamma_1 + b \gamma_2 = c
$$

Is there any way, if one knows $c$ and $\gamma$ to compute $[a, b]$? There is not, as different values of $a, b$ would give equivalent values of $c$ and thus this function is not injective.  More precisely, any linear combination of $a + b = c$ will suffice.

This is important because it means that there is no way to unambiguously invert a convolutional discriminator architecture.  We could of course use a convolutional net for a discriminator and a fully connected architecture for the generator, but doing so risks the instabilities mentioned above.  

In spite of these challenges, we can go ahead and implement a convolutional GAN to see how it performs.  For the discriminator, we can use the same architecure used elsewhere on this page for classifying flower types, with two notable changes: firstly, dropout is introduced to the fully connected layers and secondly we now store the indices identified by the max pooling steps (which signifies the indices of the elements that contributes their values to the subsequent pooling layer). Max pooling by itself non-injective and thus non-invertible function, and using the indicies of the discriminator is one way to allow make max pooling invertible.

```python
class MediumNetwork(nn.Module):

	def __init__(self):
		super(MediumNetwork, self).__init__()
		self.entry_conv = Conv2d(3, 16, 3, padding=(1, 1))
		self.conv16 = Conv2d(16, 16, 3, padding=(1, 1))
		self.conv32 = Conv2d(16, 32, 3, padding=(1, 1))

		self.max_pooling = nn.MaxPool2d(2, return_indices=True)
		self.flatten = nn.Flatten()
		self.relu = nn.ReLU()
		self.d1 = nn.Linear(2048, 512)
		self.d2 = nn.Linear(512, 50)
		self.d3 = nn.Linear(50, 1)
		self.sigmoid = nn.Sigmoid()
		self.dropout = nn.Dropout(0.1)
		self.index1, self.index2, self.index3, self.index4 = [], [], [], [] # save indicies for max unpooling in generator
		
	def forward(self, model_input):
		out = self.relu(self.entry_conv(model_input))
		out, self.index1 = self.max_pooling(out)
		out = self.relu(self.conv16(out))
		out, self.index2 = self.max_pooling(out)
		out = self.relu(self.conv16(out))
		out, self.index3 = self.max_pooling(out)
		out = self.relu(self.conv32(out))
		out, self.index4 = self.max_pooling(out)
		output = torch.flatten(out, 1, 3)

		output = self.d1(output)
		output = self.relu(output)
		output = self.dropout(output)

		output = self.d2(output)
		output = self.relu(output)
		output = self.dropout(output)

		final_output = self.d3(output)
		final_output = self.sigmoid(final_output)
		return final_output
```

Now we can make essentially the same architecture in reverse, but starting with a latent space of size 50. The model below uses the max pooling indicies obtained by the discriminator at each step of the training process, which is a somewhat dubious choice as doing so has the potential to bring about memorization of the training set.  

```python
class InvertedMediumNet(nn.Module):

	def __init__(self, minibatch_size):
		super(InvertedMediumNet, self).__init__()
		self.entry_conv = Conv2d(16, 3, 3, padding=(1, 1))
		self.conv16 = Conv2d(16, 16, 3, padding=(1, 1))
		self.conv32 = Conv2d(32, 16, 3, padding=(1, 1))

		self.max_pooling = nn.MaxUnpool2d(2)
		self.minibatch_size
		self.relu = nn.ReLU()
		self.tanh = nn.Tanh()
		self.d1 = nn.Linear(512, 2048)
		self.d2 = nn.Linear(50, 512)

	def forward(self, final_output):
		output = self.d2(final_output)
		output = self.relu(output)
		output = self.d1(output)
		output = self.relu(output)

		out = torch.reshape(output, (self.minibatch_size, 32, 8, 8)) # reshape for convolutions
		out = self.max_pooling(out, discriminator.index4)
		out = self.relu(self.conv32(out))
		out = self.max_pooling(out, discriminator.index3)
		out = self.relu(self.conv16(out))
		out = self.max_pooling(out, discriminator.index2)
		out = self.relu(self.conv16(out))
		out = self.max_pooling(out, discriminator.index1)
		out = self.tanh(self.entry_conv(out))
		return out
```

In practice, however, the use of an input's max pooling indicies appears to not result in memorization a sigmoid unit discriminator output is combined with binary cross-entropy. If a softmax output layer is used instead, weak memorization early in the training process has been observed.  If memorization is suspected to become a problem, it is not difficult to avoid the issue of transfer of max pooling indicies by either fixing them in place or else using the pooling indicies of a the discriminator applied to a generated example rather than a true input.

The training proceeds the same way as the other GAN examples, except we need to initialize the generator's max unpooling indicies.  Below the indicies from the discrimantor applied to the input tensor `x` are used for initialization.

```python
def train_colorgan_adversaries(dataloader, discriminator, discriminator_optimizer, generator, generator_optimizer, loss_fn):
	fixed_input = torch.randn(minibatch_size, 50) # latent space of 50
	for batch, (x, y) in enumerate(dataloader):
		count += 1
		_ = discriminator(x) # initialize the index arrays
		random_output = torch.randn(minibatch_size, 50).to(device)
		generated_samples = generator(random_output)
		...
```

This method is at least somewhat successful: comparing six training input images to six generated inputs from our flower identification dataset, we see there is some general resemblance between the generated data and the original.

![manifold]({{https://blbadger.github.io}}/neural_networks/custom_flowergan.png)

But unfortunately this architecture tends to be unstable while training, and in particular the generator seems to be often incapable of producing images that challenge the discriminator's ability to discern them from the real inputs.  

### Semi Stable Convolutional GANs

Difficulties with generative adversarial networks based on deep convolutional networks were well documented in the early days of research into GANs.  One approach to working around this problem is that taken by [Radford and colleagues](https://arxiv.org/abs/1511.06434).  They detail a model architecture in which both generator and discriminator are composed entirely of convolutional layers as opposed to a mixture of convolutional and fully connected hidden layers, batch normalization is used for both generator and discriminator hidden layers, and unpooling is replaced with fractional convolutional layers.  The architecture published in the paper above is now referred to as 'DCGAN', id deep convolutional GAN.

Using these principles, we can make a DCGAN-like discriminator that takes 3x128x128 color images as inputs and returns a sigmoid transformation with image $y\in(0, 1)$ which corresponds to whether or not a given image is one from our training dataset. 

```python
class StableDiscriminator(nn.Module):

	def __init__(self):
		super(StableDiscriminator, self).__init__()
		# switch second index to 3 for color
		self.conv1 = nn.Conv2d(3, 64, 4, stride=2, padding=1) # 3x128x128 image input
		self.conv2 = nn.Conv2d(64, 128, 4, stride=2, padding=1)
		self.conv3 = nn.Conv2d(128, 256, 4, stride=2, padding=1)
		self.conv4 = nn.Conv2d(256, 512, 4, stride=2, padding=1)
		self.conv5 = nn.Conv2d(512, 1024, 4, stride=2, padding=1)
		self.conv6 = nn.Conv2d(1024, 1, 4, stride=1, padding=0)

		self.leakyrelu = nn.LeakyReLU(negative_slope=0.2)
		self.batchnorm2 = nn.BatchNorm2d(128)
		self.batchnorm3 = nn.BatchNorm2d(256)
		self.batchnorm4 = nn.BatchNorm2d(512)
		self.batchnorm5 = nn.BatchNorm2d(1024)
		self.sigmoid = nn.Sigmoid()

	def forward(self, input):
		out = self.conv1(input)
		out = self.leakyrelu(out)
		
		out = self.conv2(out)
		out = self.leakyrelu(out)
		out = self.batchnorm2(out)
		
		out = self.conv3(out)
		out = self.leakyrelu(out)
		out = self.batchnorm3(out)
		
		out = self.conv4(out)
		out = self.leakyrelu(out)
		out = self.batchnorm4(out)
		
		out = self.conv5(out)
		out = self.leakyrelu(out)
		out = self.batchnorm5(out)
		
		out = self.conv6(out)
		out = self.sigmoid(out)
		return out
```

For the generator again we follow the original DCGAN layer dimensions fairly closely, but deviate in a couple key areas to allow for a 100-dimensional latent space input to give a 3x128x128 generated image output.  Note that in the original paper, the exact choice of transformation between the input layer and the first convolutional layer is somewhat ambiguous: either a fully connected layer (with no activation function) or else a direct reshape and projection are potential implementations. We take the latter approach (also taken [here](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)) as the former tends to lead to the problem of diminishing gradients that convolutional netowrks with deep fully connected hidden layers seem to face.

```python
class StableGenerator(nn.Module):

	def __init__(self, minibatch_size):
		super(StableGenerator, self).__init__()
		self.input_transform = nn.ConvTranspose2d(100, 1024, 4, 1, padding=0) # expects an input of shape 1x100
		self.conv1 = nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1) 
		self.conv2 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1)
		self.conv3 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)
		self.conv4 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)
		self.conv5 = nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1) # end with shape minibatch_sizex3x128x128

		self.relu = nn.ReLU()
		self.tanh = nn.Tanh()
		self.minibatch_size = minibatch_size
		self.batchnorm1 = nn.BatchNorm2d(512)
		self.batchnorm2 = nn.BatchNorm2d(256)
		self.batchnorm3 = nn.BatchNorm2d(128)
		self.batchnorm4 = nn.BatchNorm2d(64)

	def forward(self, input):
		input = input.reshape(minibatch_size, 100, 1, 1)
		transformed_input = self.input_transform(input)
		out = self.conv1(transformed_input)
		out = self.relu(out)
		out = self.batchnorm1(out)
		
		out = self.conv2(out)
		out = self.relu(out)
		out = self.batchnorm2(out)
		
		out = self.conv3(out)
		out = self.relu(out)
		out = self.batchnorm3(out)
		
		out = self.conv4(out)
		out = self.relu(out)
		out = self.batchnorm4(out)
		
		out = self.conv5(out)
		out = self.tanh(out)
		return out
```

Lastly, our original flower image dataset contained a wide array of images (some of which did not contain any flowers at all), making generative learning difficult.  A small (n=249) subset of rose and tulip flower images were selected for training using the DCGAN -style model.  This small dataset brings its own challenges, as deep learning in general tends to be easier with larger sample sizes.  

The results are fairly realistic-looking, and some generated images have a marked similarity to watercolor images of roses or tulips.  

![manifold]({{https://blbadger.github.io}}/neural_networks/stablegan_flowers.png)

Spikes, ie sudden increases, are found uppon observing the generator's loss function over time. This suggests that our architecture and design choices are not as stable as one would wish. Indeed, observing a subset of the generator's outputs during learning shows that there are periods in which the model appears to 'forget' how to make accurate images of flowers and has to re-learn how to do so multiple times during the dataset.  These correspond to times at which the discriminator is able to confidently reject all outputs by the generator, and appear in the following video when flower-like images dissipate into abstract patterns.

{% include youtube.html id='RXykSUv0GZ4' %}

In the literature this is termed catastrophic forgetting, and is an area of current research in deep learning. Note that this phenomenon is also observed when an exact replica of the DCGAN architecture is used (with 64x64 input and generated image sizes, one less convolutional layer, no bias parameter in the fractional convolutional layers, and slightly different stride and padding parameters) and so is not the result of modifications made: instead it may be wondered if that the 'forgetting' is due to the very small sample size.

To test this idea, we can observe the stability of this architecture (modified slightly for 64x64 inputs) on a much larger dataset, say the 126,000 LSUN Churches dataset.  Changing the resolution requires some small modifications to both generator and discriminator architectures if we use the DCGan approach, in this case making the resulting model more similar to the original one proposed by Radford and colleagues.  This models is capable of rather quickly producing realistic-looking images of churches (albeit at low resolution) but it too suffers from instability during training: observe that at the end of the video below there is a lattice artefact introduced into each image and training progress ceases.  This is because the discriminator's loss for all images has zeroed out such that the generator no longer recieves a coherent error function.

{% include youtube.html id='DAC7PqRTFx4' %}

Somewhat worryingly, the disappearance of the discriminator's loss gradient is a regular occurrence for GANs of all types.  The author has found this disappearance to be a common problem for everything from small, fully connected GANs applied to un-normalized MNIST monocolor datasets to larger cNN-based GANs applied to large datasets such as LSUN.  This observation motivates the following question: why is the GAN objective function so unstable?

The first thing to note is that the reformulation of a zero-sum game introduced by Goodfellow and colleagues that has made GANs so effective and widespread was originally motivated by the observation that it prevented the disappearance of the generator's loss gradient early in training, an outcome is typical of a zero-sum game implementation of a GAN.  One may consider the derivation to be providing a Bayesian prior on the model specifying that input generation is more difficult than discrimination, which is why we make the loss function of the generator equivalent to the log-probability that the discriminator has made mistake rather than the loss function of the discriminator equivalent to the log-probability that the generator has not produced convincing samples.

However, once the generator begins to produce realistic samples then we may view the process of discrimination just as if not more difficult than generation.  This motivates the use of both the Goodfellow reformulation as well as the alternative above...

Beyond this, however, we find other challenges to the GAN

### More challenges of GANs

A fully connected architecture seemed to be effective for small (28x28) monocolor image generation using the MNIST datasets.  Can a similar architecture applied to our small flower dataset yield realistic images? We can try a very large gut fairly shallow model: here the discriminator has the following architecture:

```
+------------------------+------------+
|        Modules         | Parameters |
+------------------------+------------+
| input_transform.weight | 100663296  |
|  input_transform.bias  |    8192    |
|       d1.weight        |  8388608   |
|        d1.bias         |    2048    |
|       d2.weight        |  1048576   |
|        d2.bias         |    512     |
|       d3.weight        |    512     |
|        d3.bias         |     1      |
+------------------------+------------+
```

and the generator mirrors this but with a latent space of 100, meaning that the entire network is over 230 million parameters which nears the GPU memory limit on colab for 32-digit float parameters.

This network and a half-sized version (half the first layer's neurons) both make realistic images of flowers, but curiously explore a very small space: the following generator was trained using the same tulip and rose flower dataset as above, but only roses are represented among the outputs and even then only a small subset of possible roses (three to be specific) are generated.

![large fcgan]({{https://blbadger.github.io}}/neural_networks/bagan_generated_flowers.png)

These images look quite realistic, but the truth is that they are far from what we want from a GAN: the are both overfit and underfit at the same time.  Certain images in the dataset have been approximately copied, but the distribution over all flowers has clearly not been captured.  This failure is known as 'mode collapse' which references the generator's distribution over the inputs, $p_{model} (x)$, that assignes very high probability to only a few inputs $x$ (ie may modes $x_1, x_2, x_3,... x_n$ have collapsed into a few modes $x_1, x_2, x_3$ in the case above).  

It is more common for GANs to exhibit a weaker version of mode collapse, in which more than a few but not all the distribution of inputs of the dataset, $x\sim p_{data}$, are represented.  GANs typically develop sharp images that are visually realistic-looking (and can achieve low Frechet Inception Distance (FID) scores) but often 'overlook' certain inputs in doing so.  This is not penalized by the typical measure of a generator network (best visual samples or FID score etc) but becomes a more serious issue for conditional image generation.  Before exploring conditional generation, however, it is worth looking into how GANs may make higher-resolution images.

### Increasing generator resolution

We have so far focused on the generation of low-resolution images, specifically those not exceeding 3x128x128.  It is natural to wonder whether GANs may be used to generate higher-resolution inputs as well.  Given that deep learning models applied to tasks of image classification often are effective accross a range of input resolutions, it may be assumed that the same is true of the DCGan architecture used in the last section.

Using the architectural choices of the DCGan for a higher-resolution image generation, we have the following generator (the discriminator mirrors this architecture) which takes a 1000-dimensional latent space vector and generates a color 512x512 output.

```python
class StableGenerator(nn.Module):

	def __init__(self, minibatch_size):
		super(StableGenerator, self).__init__()
		self.input_transform = nn.ConvTranspose2d(1000, 2048, 4, 1, padding=0) # expects an input of shape 1x1000
		self.fc_transform = nn.Linear(100, 1024*4*4) # alternative as described in paper
		self.conv1 = nn.ConvTranspose2d(2048, 1024, 4, stride=2, padding=1) 
		self.conv2 = nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1)
		self.conv3 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1)
		self.conv4 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)
		self.conv5 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)
		self.conv6 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)
		# switch second index to 3 for color images
		self.conv7 = nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1) # end with shape minibatch_sizex3x512x512
		...

```

But when we apply this model to a dataset of 4k high-resolution images of landscapes, we find that the generator makes high-resolution but nonsensical images where a specific pattern or texture is repeated over and over.

![large fcgan]({{https://blbadger.github.io}}/neural_networks/dcgan_512_landscapes.png)

This problem has been observed in previous work, and is not too surprising when we consider all the ways that a generator could try to fool a discriminator.  Instead of applying a large discriminator and generator pair to high-resolution images, we can instead use a stack of two models: first a low-resolution image is synthesizes by one generative adversarial network pair, and then this low-resolution map is converted to a high-resolution one with another GAN.

### Directing GANs with latent space conditioning

Thus far we have been considering what is termred 'unconditional' image synthesis: given some dataset, the probability of a synthesized example $x_g$ of that dataset being similar to any one example $x_i$ is equal assuming that the GAN has not experienced mode collapse (which is unfortunately a rare scenario).  It may be wondered how one can direct the synthesis system such that one type of input may be more likely to be generated




## The genetic information problem

### Background

Organisms can be viewed as the combination of the environment and heredity.  In this work, we will consider mostly only heredity. The hereditery material of living organisms is (mostly) a polymer called deoxyribonucleic acid, usually referred to as DNA.  The DNA forms very long strands in many cells, with some estimates putting the total length at over six feet per total DNA (called the genome) in each of ours.  

A surprise came when the genomes of various animals and plants were first investigated: body complexity is not correlated with genome length.  To give a particularly dramatic example of this phenomenon, it has been observed that the single-cell, half-millimeter-sized *Amoeba proteus* has a genome length of 290 billion (short billion, ie $10^9$) base pairs with is two orders of magnitude longer than our own (~3 billion base pairs). 

When the human genome project was completed, the number of sections of DNA that encode proteins (genes) was estimated at around 22 thousand, which is not an inconsiderable number but far fewer than expected at the time.  It was found that the vast majority of the human genome does not encode proteins but instead is a combination of repeat sequences left over from retroviruses, regulatory regions, and extended introns.

It could be hypothesized that the large amount of non-coding DNA is essential for the development of a complex body plan by providing extra regulatory regions.  But even this is doubtful: the pufferfish *Tetraodontidae rubripes* contains a compact genome an eighth of the size of ours, and being a chordate retains an intricate body.

How then does the relatively small genome specify an incredibly detailed body of, for example, a vertebrate?  In Edelman's book Topobiology, a problem is laid out as follows: how can a one-dimensional molecule encode all the information necessary for a three-dimensional body plan for complicated organisms like mammals, given the incredible level of detail of the body plan?

Note that although the genome does have a three dimensional conformation that seems to be important to cell biology, any information stored in such shapes is lost between generations, that is, when egg and sperm nuclei fuse.  This means that information contained in the complex three dimensional structure of the genome is lost, implying that it is the genetic code itself which provides most information.

### An illustration: the memory of a 1D, 2D, and 3D arrays

To gain more perspective on what exactly the problem of storing information in a one dimensional chain like DNA, consider the memory requirements necessary to store 1- or 2- versus 3-dimensional arrays as a proxy for information content in each of these objects.  We can do this using Python and Numpy.

```python
# import third party libraries
import numpy as np 
```

Now for our one dimensional array of size 10000 ($10^4$), populated with floating point numbers (64 bits each) with every entry set to 1, which represents our genome.  Calling the method `nbytes` on this array yeilds its size,

```python
x_array = np.ones((10000))
print (x_array.nbytes)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
80000
```
or 80 kilobytes.  This is equivalent to about 5000 words, or the size of a chapter in a book.  This is not a trivial amount of information, but now let's look at the memory requirements of a two dimensional array of size ($(10^4)^2$) once again populated with floating point 1s:

```python
xy_array = np.ones((10000,10000))
print (xy_array.nbytes)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
800000000
```
or 800 megabytes, around the size of the human genome.  This makes sense, as if we square the 80 kilobytes we get 6.4 gigabytes, which is within an order of magnitude to what the program tells us about the memory here (the computed value being smaller means that the program stores the information cleverly to minimize its size).  

Now lets try with a three dimensional array of size ($(10^4)^3$), representing our three dimensional body plan.  

```python
xyz_array = np.ones((10000, 10000, 10000))
print (xyz_array.nbytes)
```

Our program throws an exception

```python
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Traceback (most recent call last):
...
MemoryError: Unable to allocate 7.28 TiB for an array with shape (10000, 10000, 10000) and data type float64
[Finished in 0.2s with exit code 1]
```
but at have our answer: the three dimensional array is a whopping 7.28 terabytes.  This is a thousand times the size of the memory of most personal computers so it is little wonder that we get an error!  

The increase from 80 kilobytes to 7.28 terabytes demonstrates that there is far more informational content in a three dimensional object than in an object of one.


Now let's look at scaling in order to see by how much dimensional genome should increase in length when an organism increases in complexity.  Say that we are working with a coral, *Acropora millepora* that has a genome of around 400 million base pairs.  Here is a picture of an acropora (not *millepora* sp.) from my aquarium:

![coral image]({{https://blbadger.github.io}}/bio_images/acropora.png)

Like all cnidarians, this coral has a simple blind-ended digestive system, a small web of neurons, and little mesoderm.  It is a colonial polyp, meaning that each of the small rings you see are an individual animal that is fused to the rest of the coral.  Now imagine that its body were over the course of evolution to acquire more adaptive features like ten times as many neural-like cells and ten times the number of cnidocytes, the cells that have tiny harpoon-like stingers that trap prey and deter preditors.  As an approximation, say that the three dimensional plan becomes ten times as information intensive.  How much would the one dimensional strand of DNA have to increase to accomodate this new information? $ 10 ^ 3 = 1000$, in other words we would expect a new genome size of around 400 billion base pairs.  This is three times the size of the largest genome currently found for any animal (*Protopterus aethiopicus*, the lungfish), and is over a hundred times the size of our genome! 

But as recounted in the introduction with dramatic examples, there is little correlation between genome size and body complexity.  This means that there is no scaling at all, but how is this possible?

To sum up the problem in a sentence: the human genome is ~750 megabytes (2 bits per nucelotide, 3 billion nucleotides, and 8 bits per byte yields 750 million bytes), which is less information than is contained in one three dimensional reconstruction of a single neuron with an electron microscope.

### What genomes specify

How can such a small amount of information encode something so complex?  The simple answer is that it cannot, at least not in the way encoding is usually meant.  

Consider the other pages of [this website](/index.md).  There, we see many examples of small and relatively simple equations give rise to extremly intricate and infinitely detailed patterns.  An important thing to remember about these systems are that they are dynamical: they specify a change over time, and that they are all nonlinear (piecewise linear equations can also form comlicated patterns, but for our purposes we can classify piecewise linear equations as nonlinear).  

If simple nonlinear dynamical equations can give rise to complex maps, perhaps a relatively simple genetic code could specify a complex body by acting as instructions not for the body directly but instead for parameters that over time specify a body.  In this way, the genetic code could be described as a specification for a nonlinear dynamical system of equations rather than a simple blueprint that can be read.

Why does this help the genome specify a detailed three dimensional body from a single dimension?  Suppose we wanted to store the information necessary to create a complex structure like a zoomed video of a [Julia set](/julia-sets.md).  The ones I made were usually around 80 megabytes for 10 seconds worth of video.  It would take 100 seconds to make a movie that is 800 megabytes in size (without compression), just larger than the human genome.  And this is only for one among a multitute of possible locations to increase scale and still recieve more information!  A very detailed version of a Julia set would take a prodigous amount of memory to store, wouldn't it?

No, because the dynamical equation specifying the specific Julia set gives us all the information we need.  This is because dynamical systems are deterministic: one input yields one output, no chance involved.  All we need is to store this equation

$$
z_{next} = z_{current}^2 + a
$$

and then have a system that computes each iteration.  In text, this is only 25 bytes, but gives us the incredibly information-rich maps of the set.  In this sense, an extremely complicated structure may be stored in with a miniscule amount of information.


### Iterated Information

Considering the transmission of binary electrical signal over a noisy background, Shannon defined binary informational entropy as follows:

$$
H = -\left( p \log_2(p) + q \log_2(q) \right)
$$

where $p$ is the probability of transmitting a certain signal (perhaps '1') and $q = 1-p$, or the probability of not recieving the other signal, and $H$ is the information entropy in bits (binary digits) per signal.  'Entropy' here has little to nothing to do with the entropy of physical substances, but rather is a measure of total informational amount: the more entropy a transmission contains, the more information it transfers.  Plotting this equation with $H$ on the y-axis and $p$ on the x-axis, 

![Informational entropy]({{https://blbadger.github.io}}/misc_images/entropy.png)

Informational entropy is maximized when $p = q = 0.5$, which at first may seem strange.  After all, a simple random distribution of 0s and 1s, or heads and tails if one wished to flip coins, would also yield $p = q = 0.5$.  Why does approaching a random distribution give more information?  

One way to see this is to consider what would happen if $p = 1, q = 0$: now every signal received is a $1$, and so there is minimal new information (as we could already predict that the signal would be $1$ at any given time).  On the other hand, a signal consisting more unpredictable sequence of 1s and 0s intuitively yields more informational content. A completely unpredictable distribution of 1s and 0s would be (without prior knowledge) indistinguisheable from noise.

Say one were attempting to communicate with a friend by sending messages back and forth over an unconventional transmission line.  This transmission line changes such that each time a message is sent, the probability of a $1$ being recieved at any position of the message is equal to the entropy (in bits) of the message recieved. Tracking the entropy of each message over time can be accomlished using the dynamical system:

$$
x_{n+1} = - \left( x_n \log_2 (x_n) + (1-x_n) \log_2 (1-x_n) \right)
$$

which when starting near $x_0 = 0.299$ gives the following graph:

![Informational entropy]({{https://blbadger.github.io}}/misc_images/entropy_discrete.png)

which itself looks like noise!  The entropy moves around unpredictably, not settling on any value over time.

Now real electrical transfer is usually lossy, meaning that whatever sequence of 0s and 1s we send will likely not arrive so cleanly, but instead as for example  $0,1,0,1,1...$ may become $0.12, \; 0.99,\; 0.01,\; 0.87,\; 0.62...$.  In this case we are not exactly sure what the signal was intended to be, meaning that some information is lost.

Repeating the same process of tracking entropy over time as messages are sent and received, we have

$$
x_{n+1} = - a\left( x_n \log_2 (x_n) + (1-x_n) \log_2 (1-x_n) \right)
$$

where $a$ is defined as a constant of loss: $a=1$ signifies no signal loss, $a=0$ means all sigal is lost.  Making an orbit map at different values of $a$, 

```python
#! python3 
# Import third-party libraries
import numpy as np 
import matplotlib.pyplot as plt 

def information_map(x, a):
	return - a * (x*np.log2(x) + (1-x)*np.log2(1-x))

a_ls = [0.5]
x_ls = [0.1]

steps = 1000000

for i in range(steps):
	a_ls.append(a_ls[-1] + 0.5/steps)
	x_ls.append(information_map(x_ls[-1], a_ls[-1]))

plt.plot(a_ls, x_ls, ',', color='black', alpha=0.1, markersize=0.1)
plt.axis('on')
plt.xlabel('Loss factor')
plt.ylabel('Informational entropy')
```

there is

![Informational entropy]({{https://blbadger.github.io}}/misc_images/entropy_with_loss.png)

This is a period-doubling behavior analagous to the [logistic map](https://blbadger.github.io/logistic-map.html).

### Instructional information

How can we reconcile the observations above of very low-information instructions leading to complicated, even unpredictable outputs that by Shannon's definition have a very large informational content?  

Much of computer science can be thought of as belonging either to the study of information transfer, or the study of computation.  Shannon's definition of information is based on communication from a source to a reciever, which is clearly applicable to the real world with respect to transfer of information. But our observations above are of a computational nature, and this motivates a different definition of information (but one that closely corresponds with Shannon's).

Definition: instructional (computational) information amount is the number of steps specified before the procedure repeats itself endlessly or halts.  

This is most clearly understood in the context of the theory of general recursive functions, which we can substitute with Turing machines.  First note that a limited number of Turing machine input specifications can yield a very large number of possible instructions (before looping or halting).  A further discussion of steps taken given a Turing machine input may be [found elsewhere](https://blbadger.github.io/solvable-periodicity.html).  But for the sake of illustration, the maximum number of steps possible given a Turing machine with two symbols and n possible states, denoted $s(2, n)$ is (as documented [here](https://webusers.imj-prg.fr/~pascal.michel/ha.html)):

$$
s(2, 2) = 6 \\
s(2, 3) = 21 \\
s(2, 4) = 107 \\
s(2, 5) = 47,176,870 \\
s(2, 6) > 7.4  10^{36534} 
$$

This number grows at an incredible rate: its precise growth is uncomputable due to the fact that the halting problem itself is undecidable. 

This definition of information is similar to Shannon's with respect to the amount of information, termed informational entropy in Shannon's parlance.  For instructional information, a truly random input would have an infinite number of instructions without ever halting or looping and this mirrors how the maximum Shannon informational entropy is achieved for a random-like input.  This is an important quality to retain, as it is clear that storage of real data that is noisy, or stochastic, is much more information-heavy than storage of non-noisy data.

Using the number of instructions as a definition for informational content, we can clearly see that a function with very few inputs can result in an extremely large informational output, which was what we wanted.  

### The process of protein folding

To the eye of someone used to thinking of the genome as a collection of genes that are 'read' during protein transcription, the idea that the genome does not directly encode information yielding a three dimensional product may be counterintuitive.  After all, this seems to be how proteins are made: the genetic code is read out almost like a tape by RNA polymerase, which then is read out (again sequentially) by the ribosome.  The three-nucleotide genetic code for each amino acid has been deciphered and seems to point us to the idea that the genome is indeed a source of information that is stored as blueprints, or as a direct encoding.  Is there any evidence for the idea that information in genetic are stored as instead instructions for nonlinear dynamical systems?  

Let us consider the example of proteins (strings of amino acids) being encoded by genes more closely.  Ignoring alternative splicing events, the output of one gene is one strand of mRNA, and this yields one strand of amino acids.  But now remember that the strand of amino acids must fold to make a protein, and by doing so it changes from one dimension to three.  It is in this step that we can see the [usual traits of nonlinear dynamical systems](/index.md) come in to play, and these features deserve enumeration.

First, protein folding is a many-step process and is slow on molecular timescales.  This means that there is an element of time that cannot be ignored.  As dynamical systems are those that change over time, we can define protein folding as a dynamical system.  Protein folding usually proceeds such that nonpolar amino acids collect in an interior whereas polar and charged amino acids end up on the exterior, and this takes between microseconds to seconds depending on the protein [ref](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2323684/), far longer than the picosecond time scales in which molecular bond angles change.  

This is not surpising, given that a protein with a modest number of 100 amino acids has $3^{100}$ most likely configurations (3 chemically probable orientations for 100 side chains) [ref](https://www.pnas.org/content/pnas/89/1/20.full.pdf).  But viewing a [Ramachandran plot](https://en.wikipedia.org/wiki/Ramachandran_plot#/media/File:Ramachandran_plot_general_100K.jpg) of bond angles of peptides is sufficient to show us that these three orientations are not all-encompassing, nor discrete: there are far more actual bond configurations than three per amino acid.  This means that a more accurate number of orientations for our 100 amino acids is far larger than $3^{100}$.

It is apparent that proteins do not visit each and every configuration while folding, for if they did then folding would take $10^{27}$ years, an observation known as Levinthal's paradox. It is clear that a folding protein does not visit every possible orientation, but instead only a subset of these before settling on a folded state.  The orientations that are visited are influenced by every amino acid: the folding is in this way nonlinear as it is not additive, meaning that the effects on the final protein of a change in one amino acid cannot be determined independantly of the other amino acids present.  As it is dynamical, protein folding may be described as aperiodic, ie non-repeating, unless the folded protein is exactly the same as the unfolded amino acid chain. 

Second, protein folding is sensitive to initial conditions. A chemically significant (nonpolar to ionic, for example) change in amino acid quality may be enough to drastically change the protein's final shape, an example of which is the E6V (charged glutamic acid to nonpolar valine) mutation of Hemoglobin. 

But note that this sensitivity does not seem to extend to the initial bond angles for a given amino acid chain, only to the amino acids found in the chain itself (otherwise thermal motion to identical chains would be expected to yield different folded conformations).  In this way protein folding resembles the formation of a strange attractor like the [Clifford attractor](clifford-boundary.md) where many starting points yield one final outcome, but these starting points are bounded by others that result in a far different outcome.

Finally, protein folding is difficult to predict: given a sequence of DNA, one can be fairly certain as to what mRNA and thus what amino acid chain will be produced (ignoring splicing) if that sequence of DNA has never been seen before.  But a similar task for predicting a protein's folded structure based on its amino acid chain starting point has proven extremely challenging for all but the smallest amino acid sequences.

The reason for this is often attributed to the vast number of possible orientations an amino acid may exist in, but it is important to note that there are a vast number of possible sequences of DNA for each gene (if there are 100 nucleotides, there are $4^{100}$ possibilities) but that we can predict the amino acid sequences from all these outcomes regardless.  If we instead view this problem from the lense of nonlinear dynamics, a different explanation comes about: sensitivity to initial conditions implies aperiodicity, and aperiodic dynamical systems are [intrinsically difficult to predict in the long term](/logistic-map.md). 

### Implications for cellular and organismal biology

We have seen how protein folding displays traits of change over time, aperiodicity, sensitivity to initial conditions and difficulties in long-term prediction.  These are traits of nonlinear dynamical systems, and the latter three are features of chaotic dynamical systems.  What about at a larger scale: can we see any features one would expect to find in a nonlinear dynamical system at the scale of the tissue or organism?

Self-similarity is a common feature of nonlinear dynamical systems, and for our purposes we can define this as object where a small part of the shape resembles the whole. This can be exactly geometric similarity as observed in many images of [dynamical boundaries](/index.md) or else approximate (also known as statistical) self-similarity, where some qualitative aspect of a portion of an object resembles some qualitative aspect of the whole, which manifests itself as a fractal dimension.  For a good introduction to fractals see [here](https://www.youtube.com/watch?v=gB9n2gHsHN40), and note that in this context a fractal dimension is an indicator of approximate self-similarity and that the phrase 'self-similarity' in this video is limited to perfect geometrical self-similarity. 

Most tissues in the human body contain some degree of self-simimilarity.  This can be most clearly seen in the branching patterns of bronchi in lungs and the arteries, arterioles, and capillaries of the circulatory system.  But in many organs, the shapes of cells themselves often reflects the shape of the tissue in which they live, which reflects the organ. For instance, neurons are branched and elongated cells that inhabit a nervous system that is itself branched and elongated at a much larger scale.  

Temporal self-similarities in living things and and the implications of nonlinear dynamics in homeostasis and development are considered elsewhere. 

### Reproduction and information

On other pages of this site, self-similar fractals resulting from simple instructions are found. Many such maps contain smaller versions of themselves ad infinitum, meaning that one can see at any arbitrary scale the whole figure.  Except in cases of trivial self-similarity (like that existing for a line), such objects are of infinite information.  THe images of fractals on this and other sites are only finite representations of the true figures.

Self-similar fractals may seem far removed from biology, but consider the ability of living organisms to reproduce.  With the acceptance of cell theory in the 1800s (the idea that multicellular organisms such as humans are composed of many smaller organisms) brought about a difficulty for understanding reproduction.  How can the information necessary to make an entire person reside in a single cell?  One theory (predating acceptance of cell theory) was the existance of a homonculus, a small person resided in a gamete. But in this small person must exist another gamete containing an even smaller person and so on. 

The homunculus was discarded after closer examination revealed no such person inside gamete cells.  But reproduction does bring a difficult question with regards to information.  Consider the simpler case of an asexually reproducing single cell: the genetic information contained inside one individual is also enough information for other individuals as well.  The apprently limitless capacity for reproduction (as long as enviromental conditions are optimal) means that a finite amount of information contained in the genetic material of one cell has the potential to specify any arbitrary number of cells.  This applies even when the original cell only divides a certain number of times before scenescence (as for budding yeast, for example) because as long as each cell divides once, exponential growth occurs.





















## Deep Learning Server

On this page, I will detail the process of building a high-performance compute server node with four V100 GPUs for less than the cost of a single RTX 4090, but with around triple the GPU memory and compute for training models (assuming 16/32 bit mixed precision training). The server will be shown to be very effective for other tasks outside the realm of machine learning and particularly excels at those requiring high numerical precision, as this machine has an impressive twenty-six times the the 64-bit FLOPs of a 4090.

### Background

When one thinks of state-of-the-art deep learning models, one might imagine enormous data centers with thousands of GPUs training models with billions or trillions of parameters. While large computing clusters are indeed necessary for training the largest foundational models, they are not at all necessary to do interesting work in the field. For the past couple of years, I have been using the following rig for experimentation and model development (as well as three body simulation and polynomial root fractal generation and other assorted projects presented on my [blog](https://blbadger.github.io)): 

![Desktop]({{https://blbadger.github.io}}/server_setup/desktop.jpg)

which consists of i7-12700F with 48GB RAM, an RTX 3060 with 12 GB vRAM and (more recently added) a GTX 1060 with 6GB vRAM on a Gigabyte B660m motherboard. This is very for smaller experiments, and I have used it for the majority of the deep learning work I have written about in academic papers or on this blog.

But this system is certainly not ideal for training larger models or more extensive experimentation. This is because the GPU compute one can bring to bear (the 3060) has relatively low memory bandwidth (360 GBps) and CUDA core count (3584) such that it is not particularly fast at training the models that it can fit in the 12 GB vRAM, even accounting for the FP16 support and asynchronous memory copy features that are present in Nvidia's Ampere architecture. The 1060 was really only added for small-scale experiment work and software development purposes, as it is significantly slower (around two thirds the speed to be precise) than the 3060 and therefore would slow down training significantly were it to be used in parallel. This is a notable difference from other applications such as crypto mining, where the cluster runs at a sum speed of all individual GPUs. For deep learning the cluster will typically increase in speed with more parallelization, but will also be bottlenecked by slower GPUs if there is a large enough difference in their compute (especially for training). I typically perform training runs of no more than a few days, on models that are under 1 billion parameters (for foundational models) due to these limitations. To test methods and ideas that appear promising on this relatively modest amount of compute, I wanted to try upgrading. 

The most straightforward way to make a significant upgrade to this rig in terms of GPU memory and compute (with some semblance of affordability) would be to add some used RTX 3090s and use PCIE extenders and a beefier power source or two. I came very close to choosing this route, but decided against it for a number of reasons: firstly because the B660 mobo has 5 PCIE slots but four of those are 1x PCIE 3.0, meaning 1 GBps data transfer for all GPUs except one. This is not really a problem for inference, but will slow down the gradient synchronization step for very large models during distributed training: for these two GPUs with a relatively small model (30m parameters), distributed data parallel training proceeds nearly four times as slowly as the same training algorithm with no all-gather operation communicating gradients. This means that a new motherboard would be necessary to avoid a severe bottleneck in distributed training for 3090s. Secondly, 3090s run quite hot (peaking up to 500 watts per gpu) and my PC resides in a small room, and thirdly because mixed precision training (where weights are stored in FP16 and gradients and norms in FP32) is not optimal on these GPUs due to their lack of full FP16 acceleration.

When deciding on the right GPU depends on many factors (inference or training workloads? large transformers or smaller cNNs?), a good resource to check when deciding on a GPU for deep learning is on Tim Dettmer's [blog](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/). A very handy plot of the theoretical mixed precision training compute for each type of modern GPU is presented, but with some more research I noticed something curious: by far the best-value GPU by current market price (ie on ebay) is the 16 GB V100 SXM2. This is mostly because the SXM2 socket is relatively rare, but we will find a good system for this hardware without too much problem.

Because of this I decided to go with the V100, which is somewhat similar to the 3090 with nearly identical memory bandwidth (900.1 vs 936 GB/s), albeit double the Tensor core count with half the CUDA cores. The main functional difference is that the V100 is a data center card rather than a consumer one and is more suitable for extended workloads, and does not suffer from the memory overheating problems the 3090 is known for. It does have an older architecture (Volta) than the 3090 (Ampere) and therefore cannot take advantage of a number of improvements in Ampere such as asynchronous memory copy, but it has other advantages (full FP16 performance and more memory in the 32 GB V100). Using the SXM2 socket happily allows for much faster data transfer between GPUs than the more expensive PCI-e version, so it is a no-brainer for training.

### Motivation

A similar general principle for procuring parts is found here that also was observed for [high voltage engineering projects](https://blbadger.github.io/#high-voltage): obsolete (by industry standards) industrial equipment is often the cheapest way of accomplishing engineering tasks provided that one has the know-how to work with the equipment out of its intended use niche. On this page I will show you how to do exactly that in the context of a deep learning server.

Most deep learning servers with decent GPUs cost many thousands of dollars to buy used, much less new. The main exceptions to this are the Gigabyte T181-G20 and T180-G20, and this is because these servers are built to fit in and be powered by Open Compute Project 1OU racks. These racks are extremely rare and expensive, making even new T181s and T180s relatively inexpensive. Happily, however, these servers run perfectly well outside the OCP rack if supplied with external power from a sufficiently powerful source (12V with at least 80 amps to each of three power sockets). How this may be done will be described later.

The model I chose is a Gigabyte T180-G20, which is very similar to the T181-G20 except that it supports Intel Xeon E5 2600 v3 and v4 generation CPUs, whereas the T-181 supports Intel Xeon Scalable CPUs (which are effectively the next generation of Intel server CPU after the v4 E5s) and has more memory DIMMS (24 versus 16). For more information on the difference between these servers as well as the other OCP rack entrant from Gigabyte, see [this documentation](https://www.gigabyte.com/FileUpload/TW/MicroSite/354/dl/RACKLUTION-OP_Brochure.pdf). If you were to expect your workloads to utilize a substantial amount of CPU compute, the T181-G20 would probably be worth looking at as the top-of-the-line 2600 series (the 2699) v4 Xeon is similar in workload performance to today's i5 processor, whereas the top-of-the-line Xeon scalable gen 2 CPU (Platinum 8280, excluding the 400W TDP 9282) is more similar to today's i7. That said, there are two processors per server and I have found that a pair of 2680 v4 Xeons (a mid-range Broadwell with about 2/3s the capability of the 2699) to be more than sufficient for most purposes. Memory and CPU->GPU transfers are also somewhat slower for the T180 as it has PCI-e Gen3 rather than the Gen4 in the T181. 

After a half years use, it seems accurate to say that for the vast majority of tasks I have given the server the older CPUs have not been at all noticeable. For the few applicable CPU tasks that do benefit from faster hardware (multinomial sampling for example), I am considering swapping in a pair of e5 2699s in the future. If I were getting another server I would seriously consider the T181-G20, but would be inclined to prefer the T180-G20 once again. Between the costlier server, CPUs, and ram for the T181 compared to the T180, you would be able to pick up the V100 GPUs for free with the difference.

Because the T180-G20 supports older CPUs and less maximum memory than the T181s, they are a good deal cheaper and can be had for under a thousand dollars new. Not bad for a machine that supports up to 750 TFLOPs for FMA (fused multiply-add) matrix operations with up to six V100 GPUs (four sxm2 and two PCI-e), 192 GB vRAM with the same configuration, and 1.024 TB DDR4 RAM. In my initial configuration only the four SXM2 sockets are occupied, each with 16GB V100s to total 500 TFLOPs for tensor multiply-add operations with 64GB vRAM. These SXM2 sockets are interconnected via 300 GBps NVlink, making these four GPUs behave for all purposes as one large GPU performance-wise. I chose the 16GB rather than the 32GB V100 as they are nearly a factor of 10 cheaper at present, although the price of the 32gb versions is falling quickly. If you are interested in this GPU, I suggest reading the [Volta architecture](https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf) paper from nvidia for more information on the machine's capabilities.

### Hardware Installation

The T180-G20 comes with heatsinks for the GPUs (four) and CPUs (two). These heatsinks contain all the screws necessary pre-installed as well as thermal paste pre-applied, which is a very nice touch but is probably standard in the high performance compute industry. After removing the heatsinks, we find the server itself. In the following picture, the server is oriented where the power sockets are on the left and the I/O ports (and air inlets) are to the right. Note the hardware installation instructions happily present on the top of the case, and the opened HDD/SSD drive tray in the bottom right. The server even comes with those little storage drive screws which fixes the drive in place as the tray is inserted.

![server]({{https://blbadger.github.io}}/server_setup/server_lid.jpg)

This server takes two CPUs which are interconnected and act for most purposes as a single unit, although it can run with only one. I thought that the Intel Xeon E5 2680 V4 would be a good balance between TDP (120 Watts each) and power (3.3 GHz turbo, with 28 threads, 40 PCI-e lanes, and 35 MB caches each). It is remarkable that a CPU of these attributes can be bought for under 20 dollars: to buy a consumer CPU with anywhere near the thread count or PCI-e lanes one would have to pay perhaps a hundred times that amount. This CPU has far more pins than the i7 12700K, and the lid is somewhat reminiscent of an AMD Ryzen. It tends to make removing thermal paste messy, as you can see:

![server]({{https://blbadger.github.io}}/server_setup/xeon_pins.jpg)

and here it is ready to be clipped in:

![server]({{https://blbadger.github.io}}/server_setup/cpu_seat.jpg)

In terms of core count two CPUs are extremely overkill for most of the types of workloads I am expecting to run on this machine, that is, CUDA kernals for deep learning and dynamical systems. As we will see later, only one CPU is necessary for excellent performance (and really only a quarter of the cores of one CPU are required). A more powerful CPU does come in handy when one is attempting to perform linear algebraic operations outside the set of matrix multiply, add, convolve, etc: for example, finding the singular value decomposition of a matrix is typically a CPU-intensive process, and there it helps to have all the threads you can get. I will be performing this kind of operation non-infrequently, hence the more powerful CPUs. In terms of functional performance, the 2x Xeon 2680s are about as fast for tokenization as my (much newer) i7-12700K but together support nearly 10x the memory and PCI-e lanes. The i7 does have better performance than a single Xeon for both single- and multi-threaded jobs, but as there are two Xeons the server's speed on CPU bound tasks is fairly similar for most tasks.

The CPU heatsinks are interesting: only half the base plate contains fins, a flat heatpipe covers the base, and the rest of the heatsink is apparently a copper alloy. In the following image you can also see one memory stick installed: this is a 16GB RDIMM RAM module for testing (more were added later). As with most servers, only RDIMM or LRDIMM modules may be used. 

![server]({{https://blbadger.github.io}}/server_setup/cpu_heatsink.jpg)

With CPU heatsinks installed, I installed one GPU for testing purposes. In the image below, the four SXM2 sockets are on the left, CPUs on the right, and PCI-e sockets are in the center right. Note the connection to the SXM2 board from one PCI-e socket, leaving the other two free for high-speed NIC switches to other server nodes. The PCI-e connection from CPU to GPUs is limited to 16 GBps (32 GBps bidirectional), but data transfer between GPUs on the SXM2 board is a juicy 300 GBps.

![server]({{https://blbadger.github.io}}/server_setup/server_internals.jpg)

The V100 arrived nice and clean with a mirror finish on the lid. The large grey 'TR21...' modules are voltage regulators, and if you have seen an SXM2 P100 this will look very familiar except for the GV100 chip. 

![server]({{https://blbadger.github.io}}/server_setup/gpu_lid.jpg)

The hardware installation guide warns you that there is a very fine tolerance window for the screws that fasten the GPU to SXM2 board: less than 5%! This is because there are tiny springs used to modulate torque. It is recommended to use a precision torque screwdriver for installation, but I winged it with a small-bore screwdriver and lots of patience. To be honest, I would probably just get a precision screwdriver if I were to do this again: I had to go back and re-tighten both heatsink and GPU-board connections multiple times to eliminate various gremlins (a too-warm GPU, GPU that was not recognized at all, strange memory hangs resulting in process kills etc). To be frank, the SXM2 connection is not nearly as robust as a modern CPU connection, but this is a small price to pay for huge bandwidth I suppose.

![server]({{https://blbadger.github.io}}/server_setup/gpu_presink.jpg)

The GPU heatsink comes pre-loaded with thermal paste, nice!

![server]({{https://blbadger.github.io}}/server_setup/gpu_heatshink.jpg)

The heatsink has a nice thick base plate and large fins, and is around six times the size of the CPU heatsink. 

![server]({{https://blbadger.github.io}}/server_setup/gpu_heatsink.jpg)

There do not appear to be any heat pipes in the GPU's heatsink, just a hefty base plate (the center of which is copper) and closely spaced fins.

![server]({{https://blbadger.github.io}}/server_setup/through_gpu.jpg)

The heatsink also has a small tolerance window, but with springs it is not quite as small. 
Here is a side view of the GPU and heatsink after installation. Interestingly the voltage regulators do not contact the heatsink.

![server]({{https://blbadger.github.io}}/server_setup/gpu_side.jpg)

With that, the first GPU is installed. In the image on the right (below), air moves from right to left.

![server]({{https://blbadger.github.io}}/server_setup/gpu_heatsink_install.png)


### Power Supply Units

Now we get to a tricky part: powering a 1OU OCP server outside its native habitat (an OCP rack) using power supply units (PSUs) designed for different servers. Whilst most of the safety features of PSUs are retained when doing this (over-draw leads to current trip and shutdown etc.) this is obviously not the intended use of these components and therefore the manufacturers cannot be expected to have planned for it. If you are planning on assembling your own OCP server, proceed at your own risk.

With a total TDP of around 1500 watts for the chips alone, perhaps the simplest power supply would be a 2000W dell server PSU. Unfortunately these require 240V AC inputs, and I only have 120V outlets. Happily however other dell server PSUs are designed to be run in parallel as they would be in their native environment, so instead we can just use two 1100W PSUs in parallel (with the current sharing pin connected to avoid burning out one while the other idles). 1100W dell PSUs are very inexpensive on ebay, so I got a couple and started breaking them out, ie making them run outside their intended environment. I started with dell z1100p PSUs, not to be confused with l1100e supplies that have very different pinouts.

![psu]({{https://blbadger.github.io}}/server_setup/dell_psu.jpg)

One can buy breakout boards for this purpose, but I thought it would be more fun to solder the connections myself. This turned out to be more difficult than I had anticipated, and it turns out that you have to first draw one pin to a GND, supply power and wait a few seconds, and then connect two 'switch' pins together to complete the power on. One of the strangest sequences I have seen for a power supply, but at least it works.

![psu]({{https://blbadger.github.io}}/server_setup/psu_test.jpg)

Power must be supplied to each of the sockets on the right of the server in the photo below (actually one can also bypass the sockets and install a cable into the cages next to the sockets, but I thought this would be more of a pain to do). Each socket has a +12V (left) and GND (right) connection, and the server expects 80 amps per socket.

To get the power from the PSU to sockets I used a combination of 4 AWG battery cable and 1/8" thick by 1" wide solid copper bus bar used to insert into the sockets, and for collecting the inputs for the +12V and GND. The 4 AWG cable turned out to be overkill, and I would choose 6 or 8 AWG if doing this over. To make things simple, standard convention is followed where red is HIGH +12V (nominal) and black is LOW, 0V.

![server]({{https://blbadger.github.io}}/server_setup/server_prepower.jpg)

Sawing the bus bar into chunks and drilling for connections allows for one bus bar in each power socket.

![server]({{https://blbadger.github.io}}/server_setup/bus_terminals.jpg)

Connecting things together with the appropriate hap-hazardness that indicates a test, we have a successful power connection.

![server]({{https://blbadger.github.io}}/server_setup/test_psu.jpg)

While performing these tests, I noticed that this PSU tended to modulate its fans in response to current draw (which is good) but that it tended to be rather warm when the system itself was powered down (bad, indicates parasitic current draw). Because of this (and because I accidentally stripped a pin during a de-solder process of one of the PSUs) I switched my original plan to instead use a similar PSU but with breakout boards. 

The new PSUs are two Dell l1100e-s1 modules with adjustable breakout boards from ebay. There is some voltage drop from the breakout board output to the server power socket, but both PSUs are recruited during heavy workloads even without further coordination. This can be seen during experiments where the wattage pulled is greater than a single PSU's rating if we use only a single PSU: doing so results in a reset of the PSU and system crash, and unfortunately fried one of my RAM stick in the process. With both PSUs connected there is no such failure. 

I also connected the current share pins (which is pin S7) of the two PSUs in an effort to get them to coordinate better under high load. This connection can be with very thin wire as it carries virtually no load (<500mW), so I used insulated AWG 20 wire (black in the picture below) inserted directly into the breakout board pin opening. I also used proper copper ring lugs to make the high-current connections to and from the sockets.

![server]({{https://blbadger.github.io}}/server_setup/gpu_cshare.jpg)

This effort was moderately successful, but there is still a sizeable voltage drop under high load (all four GPUs at ~300W + 200W CPUs + 200W other = ~1600W) which can lead to GPU under-volting and GPU bus drop if this amount of current is drawn for extended periods of time. It was not uncommon to see terminal voltages around 11.2V in this configuration under high load.

![server]({{https://blbadger.github.io}}/server_setup/full_psus.jpg)

Unfortunately, under high load (all four GPUs running) over extended periods of time I found that the bannana plug breakout board connections were getting much too hot such that one of the plug's plastic sleeves ended up melting within the first few dozen experiments. The bannana plug in question was loose and probably would not have melted the plastic if it was tight, but the plugs tend to get loose over time spontaneously. It turns out that the common bannana plugs such as these are rated for only 15A, and at 1kW we would expect both PSU's plugs to operate at 42A if they share the current load perfectly, so it is not surprising that the bannana plugs were overheating.

The highest-rated plug on these breakout boards is the XT60 plug in the upper right: this is rated for 60A at 30V DC, meaning that a continuous draw of around 45A at 12V should be pretty safe. I got some XT60 male plugs with leads and attached those to the existing terminals as follows:

![server]({{https://blbadger.github.io}}/server_setup/xt60_connection.jpg)

and sure enough the plugs get somewhat warm to the touch but not hot under load. The XT60 plug setup also prevents the voltage drops that I was seeing when the bannana plugs were used, and the voltage rarely drops under 11.85V under load. Here is the voltage at the terminal under around 1.1kW:

![server]({{https://blbadger.github.io}}/server_setup/xt60_final.jpg)

The XT60 plug also allows us to run the GPUs at full power (~1600W total for the server) for extended periods of time, although the plug itself and the PSUs get rather warm if no external airflow is provided. Under full load there is somewhat more of a voltage drop at the terminal, with minimums around 11.60V, but no power loss or GPU bus drops.  To deal with the warm XT60 plugs and PSUs, I added small 12V blower fans that you can see in the following image. The temporary power connections were also replaced with more permanant ring terminals, and the two z1100p PSUs make a nice heatsink and support.

![server]({{https://blbadger.github.io}}/server_setup/cooled_psu.jpg)

Due to the power and heat and noise reduction for a slightly larger performance degradation, I tend to limit the power to close to 200W per GPU, which degrades performance only slightly. The plugs and PSUs tend to be much cooler with this limit, especially with the blowers running. The eventual plan is to add a third PSU for redundancy so that any one unit can fail without killing the server's processes, and with the added benefit of reducing the power drawn from each PSU and through each XT60 for increased longevity.

UPDATE: The third and final PSU has been added. This turned out to be necessary for stable training runs where all four GPUs tended to ramp up and down very quickly (as happens during DDP training under certain conditions). When that happens the sharp increases in GPU power draw (which appears to occur in the microsecond timescale) are too much for even two Dell l1100e PSUs, and the supply under-volts (~11.4 V at the server terminals) and the GPUs drop off the SXM bus, even with limited clock speeds and lower power levels. The addition of a third PSU happily prevents this issue even when the V100 clock speeds are not limited and the power levels are not lowered from the default TDP of 300W. I have also added small aluminum heatsinks to the XT60 plugs, which seems to keep them somewhat cooler.

![server]({{https://blbadger.github.io}}/server_setup/final_psu.png)

Since writing the above, I experienced a slow degradation in voltage at the server power terminals such that GPUs began to drop off the bus once again. After some investigation, this turned out to be due to poor connections between the xt60 wires and the copper ring terminals: it turns out that simply wrapping these wires around a lug results in poor connection and thus significant heat and corrosion over time, particularly on the hot (+) terminal. With the same PSUs, simply replacing this connection with two ring terminals (one for each +/0 xt60 wire, one for each +/0 bus wire) bolted together results in much higher full-load server voltage (12.06V at the terminal), with virtualy no fluctuation during training and no more GPUs dropping off the bus. If you plan on DIYing a power supply like this, pay extra attention to *all* your high-current connections as a single bad one will cause a significant voltage drop. 

It turns out that much of the heat observed on the xt60 plug was also due to these poor connections. After installing the double-ring bolt connections, the xt60 plugs no longer feel warm to the touch even after extended runs. It appears that most of the heat felt on these plugs actually came from resistance in the downstream connection, as with more testing the xt60 itself handles 60 amps continuous at 12V without getting hot. 

### Test

The I/O ports are suprisingly comprehensive for a server: one VGA, three RJ45s, and two USB ports. 

![server]({{https://blbadger.github.io}}/server_setup/server_io.png)

This makes it easy to connect a monitor, keyboard, ethernet connection, and bootable USB (to install ubuntu-server). There is no reason to connect a mouse as ubuntu-server has no GUI be default (although it is easy to install one). As I am going to `ssh` into this server for almost everything, I did not install a Desktop GUI.

After powering on, it POSTs! Much of the hardware in this HPC server is managed by Penguin's software, making a linux OS even more fitting.

![server]({{https://blbadger.github.io}}/server_setup/server_post.jpg)

Heading to the BIOS, we find that both CPUs are recognized, the memory is performing at its maximum speed (2400 MHz) and that the server was built in 2017.

![server]({{https://blbadger.github.io}}/server_setup/server_bios.jpg)

After installing ubuntu-server, we can check the internals. All 56 threads are active, and not terribly busy which is great.

![server]({{https://blbadger.github.io}}/server_setup/server_htop.jpg)

After checking that the GPU hardware with `sudo lshw -C display` and finding our V100, installing the proper NVIDIA drivers and rebooting allows interfacing with the GPU. Et voila, our V100 is found and is idling (although in performance mode, interesting).

![server]({{https://blbadger.github.io}}/server_setup/server_nvidia-smi.jpg)

There are various CPU performance modes available in the BIOS, and I went with maximum performance as the TDP for each CPU is not very high.

### Installing the rest of the hardware

With the test completed, I went ahead and installed the rest of the GPUs and memory sticks. Unfortunately the rest of the GPUs were not pre-cleaned, so I had to wipe some thermal paste off the chip lids.

![server]({{https://blbadger.github.io}}/server_setup/more_gpus.jpg)

I seated the GPUs first before installing the heatsinks. Note that you should never attempt to run a GPU without a heatsink! It will rapidly overheat, although it may turn off automatically if you are lucky.

![server]({{https://blbadger.github.io}}/server_setup/all_gpus.jpg)

And here is the server with all the GPUs, memory, and heatsinks installed! 

![server]({{https://blbadger.github.io}}/server_setup/full_gpus.jpg)

I had to re-install two of the GPUs a couple times in order to get them to be recognized by the system, and whether this was due to dusty SXM2 pins, incorrectly torqued screws, or just old hardware it is difficult to tell. Happily it is easy to see if a GPU is connected using the `sudo lshw -C display` command, and the nvidia toolkit finds the GPUs as well. I had installed fairly recent CUDA driver (535.172.04) and API (12.2) versions, and you can see that here. Note that the GPUs are by default in maximum performance mode (P0) even while idling: this is to be typical of SXM-socketed nvidia GPUs, and indeed the SXM2 V100 cannot be set to any other mode. Note too that watts per GPU at idle has roughly doubled from what it was when a single GPU was installed to ~40W: this is unsurprising, given that the NVLink connections between GPUs cannot completely power down when not in use (typically at least two of the eight link channels are active at all times). Could be worse: A100s typically idle at aroun 65W each!

![server]({{https://blbadger.github.io}}/server_setup/full_nvidiasmi.jpg)

We have an impressive 500 teraflops of matmult performance for ~650$ worth of GPUs. A little appreciated fact is that watt-for-watt the V100's performance is similar to the A100, which has a TDP of 400W (30% more than the V100), while the A100 is typically perhaps 45% faster for real workloads. Both wattage estimates are averaged over time, as it is uncommon for the V100 to near 350W and the A100 to approach 475W at peak boost.

### Stress Testing with Deep learning training

Now that the power supply had been sorted out (or so I thought) I went ahead and stress tested the server for workloads typical of what I intended this machine to experience in order to make sure that the GPUs and other hardware elements were performing optimally. 

Launching my `distributed_mixer_trainer.py` module via `torch.distributed.launch` for DDP training via

```bash
-m torch.distributed.launch \
  --nproc_per_node=4 \
  --nnodes=1 \
  distributed_mixer_trainer.py 
```

I was in for a surprise: using up to three GPUs (`--nproc_per_node=3`) performed as expected, but adding the fourth caused the GPUs to drop off the bus, or in other words to crash so completely that they cannot be found via `nvidia-smi -q`. The error messages were extremely unhelpful, and essentially conveyed that there was an unknown failure with the GPUs themselves. 

After some reflection I wondered whether this could be due to the power draw being too much for my power supply: even though the supply should be good for 2100W (175 Amps at the terminal), this server is actually designed to be supplied by 200 Amps per terminal and thus is being somewhat underpowered. But if the TDP of the server is 1500W, why would this matter? It turns out that V100s (like A100s and other high-performance datacenter GPUs) are capable of drawing much more current than their maximum rating for short periods of time during boos: I have seen a 16GB V100 SMX2 on this system (rated at 300W) draw over 350W. Even this should not be a problem for our server PSUs, but a rapid increase in load (for a PSU not designed for this) might be: in tenths of a second, it is not uncommon to see a V100 go from 150W to 350W. If you multiply this by four, you get nearly 1KW in rapid load increases, which might lead to undervolting. When I tested the power socket voltage during this ramp-up, it indeed did drop to nearly 11V.

A straightforward way to prevent this issue would be to simply limit the amount of current the GPUs may draw. This may be done via `nvidia-smi -pl` followed by an integer corresponding to the number of watts per GPU desired. After enforcing this power limit, we indeed see that the average power does decrease to approximate the limit but that performance is very lightly affected, such that going from 300W -> 200W results in a ~15% performance decline for the V100. This is even less that what was observed for [consumer graphics cards](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/), but I ended up having the same GPU bus drop problem as before under full load. 

Close inspection with `watch -n0.1 nvidia-smi` revealed the cause: the average current was lower but the peak current was far above the specified 200 watts per GPU, and even exceeded 300 watts! Clearly the V100s viewed the power limit as more guidelines rather than actual rules, so I resorted to down-clocking the processor to avoid boost as follows:

```bash
bbadger@servbadge:~/experiments/generative-models/mixer_lm$ sudo nvidia-smi -pm 1
bbadger@servbadge:~/experiments/generative-models/mixer_lm$ sudo nvidia-smi -ac 877,1005
```
where the first command induces persistance and the second specifies the application clock speed in format `<memory_clock,processor_clock`. The exact processor and memory clocks that are allowed are somewhat arbitrary depending on the GPU, and you will have to use `nvidia-smi -q -d SUPPORTED_CLOCKS` to view the clock limits that are supported. Newer GPUs have an `-lgc` flag that can be used to specify a logic versus application clock, but this flag is not present for the V100. 

After applying the clock limit, we have GPUs that are now observing our intended power limit, and training proceeds successfully.

![server]({{https://blbadger.github.io}}/server_setup/training_gpu_power.png)

Note that the GPU in position 2 is running significantly hotter than the others: this is only under load, and was due to an incorrectly torqued heatsink. After tightening, the GPU is more in line with the others and no longer ramps up the fan speed (more on that later).

![server]({{https://blbadger.github.io}}/server_setup/idle_gpu_power.png)

It is remarkable that removing the V100 boost clock speed (1530 MHz) and reducing the base from 1290 MHz to 1005 MHz (along with our earlier power limits) leads to such a small change in performance: enforcing this for a training run with only two GPUs during a test leads to a ~13% decline in training speed.

That said, once these tests were completed I connected the PSU current share pins (see the PSU section for more information) and after doing so and switching to the XT60 plug output on the breakout board, the GPUs were able to run at full power (up to ~350W each). I consider the ~600W total saved to be worth a 13% performance drop, and continue to run the GPUs as 200W each.

I experienced difficult-to-pin-down training process hangs and freezes which manifested as `nv_queue` processes taking nearly all a CPU's compute followed by interrupt requests (`irq/57-nvidia`) that also hangs, leading to stalled GPU processes with high vRAM use. Close inspection reveals that these are associated with `nccl` communication problems, which means the GPUs are not communicating properly with each other or the CPU. I was able to solve this problem by simply tightening the screws that affix the GPUs to the SXM2 socket.

As a final note, `python3 -m torch.distributed.launch` is a legacy DDP launcher, and I prefer `torchrun` as it is easier and slightly more performant to work with for this server.

### Performance

Now we can test the performance. Happily it is very good! For deep learning, each V100 is generally between two and four times faster than my RTX 3060. The exact difference depends somewhat on the task at hand, and appears to mostly be the result of the difference in memory between these GPUs: the 3060 uses GDDR6 (fast clock, low bandwidth) and the V100 uses HBM2 (slow clock, large bandwidth). Thus for models with small weight matrices the 3060 is relatively better-suited, but for larger models the V100's HBM2 becomes far superior. In my tests on a 2048-model dimensional [language mixer](https://blbadger.github.io/smaller-lms.html), a mixed precision training run with the 3060 took 880 seconds, whereas a single V100 took 286. This ~3.1x speedup seems typical of medium-sized models for mixed precision training, and aligns with what is expected from theoretical values from Dettmer's [blog post](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/).

We also get to enjoy the fruits of our SXM2 socket labor: because the inter-GPU bandwidth is a whopping 300GB/s, there is very little per-GPU performance decrease when parallelizing a workload using distributed data parallel for a medium-small model with ~300m parameters, with some current and clock limiting for power reduction: with one GPU a similar training run took 361 seconds, with two 180 seconds, with three 121 seconds, and with all 4 GPUs 92 seconds. This sort of thing is generally not possible NVLink: it is common to see speedups of between 3.5x and 3.6x for four GPUs that are connected by PCIE 16x lanes to the CPU. Here the very high GPU communication bandwidth allows us to hit a speedup of 3.92x for four GPUs! The same speedups are observed even when the GPU application clock is not limited to 1005 MHz, although in each case there is an approximately ~14% reduction in time at the cost of around 600 watts. This means that with all four GPUs at full power, training is completed in 79 seconds. This is a speedup of 11.1x compared to the same training run on the 3060!

![server]({{https://blbadger.github.io}}/server_setup/full_gpu_power.png)

To substantiate the claims made earlier that the CPU core number is quite overkill for training deep learning models, observe the single-Xeon 2680 (with 28 threads) CPU utilization for a CPU-intensive task such as fast tokenization,

![server]({{https://blbadger.github.io}}/server_setup/cpu_tokenization.png)

or a similarly intensive task of performing forward- and back-propegation on a medium-sized model.

![server]({{https://blbadger.github.io}}/server_setup/cpu_training.png)

In both cases the majority of thread are heavily utilized. Now for the thread utilization for four-V100 DDP training:

![server]({{https://blbadger.github.io}}/server_setup/all_gpu_training.png)

the cores utilized count is small because the DDP defaults to a single thread per GPU, with a few threads saved for data loading. Increasing the number of threads per GPU in my experience does not result in better performance and indeed often leads to detrimental effects. Because of this, we can estimate that a 28-thread CPU could support nearly two dozen GPUs if the PCIE lane count were high enough. The 4x V100 SXM2 board requires two 16x PCIE lanes, so 20x V100s would require 160 PCIE lanes. That said, having a large number of cores can come in very handy for CPU-bound tasks such as tokenization or more complicated linear algebraic tasks, and a better CPU certainly makes cpu tokenization quicker. Because of this I run the server with both CPUs installed rather than just one.

### HPC performance

This server is not only used for deep learning, however, and is also applied to more traditional high-performance computing tasks such as integrating partial differential equations. In particular, it is used for [three body problem](https://blbadger.github.io/3-body-problem-2.html) research which entails integrating differential equations for Newtonian mechanics over tens of thousands of steps, for millions of starting conditions. This is a very different challenge than deep learning training, and I assumed that the V100 would provide some benefit over the 3060 but not perhaps as much as for deep learning.

This is because deep learning training involves loading data from memory to registers, performing matrix multiplication and addition, and then sending the result to memory. There is some difference between a forward pass were only output vectors are computed (which is more likely to be limited by the rate of memory transfer to registers) and a gradient back-propegation where Jacobians must be found (more often limited by compute speed in addition to memory transfer rates), but the fundamental steps are fairly similar. This is why nvidia's introduction of the tensor core that can multiply 4x4 matrices in one step is so useful for deep learning, and indeed the introduction of this led to a much larger increase in relative performance in the V100 versus its predecessor (P100) than any subsequent generation yet.

But differential equation integration for many starting values is very different: here there is little or no requirement that each value stored in an array in memory must be added or multiplied to other values, or that different parts of memory must be re-loaded to registers. We can hope to approach the expected limit of floating point operations due to this more limited memory movement, and a quick search finds that a 3060 has a floating-point TFLOP expected value of 12.74, where as the V100 improves this slightly to 15.7 but has a lower processor and memory clock speed that makes small memory transfers relatively less efficient. The V100 is far better at double-precision computation however, boasting 7.8 versus 0.2 TFLOPs for the 3060. 

It will not be entirely unexpected, therefore, that the V100 is far superior for 64-bit numerical integration of the three body problem: for 1 million starting configurations (each simulations for 50 thousand steps of Newton's method) a 3060 completes the integration in 464 seconds whereas a single V100 requires only 59 seconds (a 7.9x speedup), and a distributed version of the integration algorithm completes in a mere 15.2 seconds. This is an effective speedup of 31x for the server, not bad!

It comes as a complete surpise, however, that the V100 experiences an identical speedup for 32-bit (float) numerical integration, with the four GPUs requiring only 7.04 seconds to complete the 50k steps that the 3060 took 212 seconds for (using an CUDA kernal that is already highly optimized). This is completely unexpected if we take the FP32 TFLOP estimates for these cards at face value, where one would predict only a 1.2x speedup. Why then is the V100's integration so fast?

The answer is difficult to know exactly without knowledge of how each device handles each computation, but there are two probable explanations. One is that memory in each type of cache that exists in these two devices. The 3060 has 28 streaming multiplexes each with 128KB L1 cache for a total of 3.584MB, and a 3MB L2 cache. The V100 on the other hand has 80 streaming multiplexes each with 128KB for a total of ~10MB L1 cache with a 6MB L2 cache, and therefore can load much more information from global memory at a given time.

The other possibility is that there were unannounced performance degradations in the floating point precision computation for the 3060, or equivalently that the official value is over-estimated. This is likely because the expected speed increase for a 3060 going from 64-bit double to 32-bit single precision is a factor of 12.74/0.2 = 64x, but we see only anapproximately 2x performance increase for that device when this change is made. I expect both memory capacity and TFLOP estimation to be the reason for the V100 performance increase.

### Noise

This was one of the things I though hardest about before going the T180/T181 route over a bunch of used 3090s in a PC. The world's best server is useless in a home setting if it has the acoustic properties of a turbojet engine, unless one were to make special accommodations such as walling off the server in concrete. This sort of thing did not appeal to me, and while the server was going to be operating in a basement room and could be noisier than the average PC it could not be overly loud.

The reputation of 1U servers (the more common measurement that is most similar to the T180-G20's 1OU form factor) is that they are simply too loud for home use and that they indeed sound like jet engines. This much was even claimed by George Hotz while talking about the motivations for Tinygrad's Tinybox, but I can confirm that it is a bit of a misunderstanding. The potential for a high-performance compute 1OU server such as the T180 for making noise is indeed very high: when first booting up, for example, all 40 fans ramp up to their maximum 25000 RPM and the server sounds much like a jet engine during takeoff, such that one needs hearing protection to work with it in a small room (testing or setting up ssh, for example). The fans to modulate after a couple minutes, and the noise becomes much more managable and is what one would expect for a blade server: equivalent to a somewhat noisy Desktop PC, just with a timbre more akin to a turbine. 

What matters after those first thirty or so seconds is that even heavy loads on all four V100s does not lead to the fans reaching anywhere near their maximum RPM provided the ambient temperature is near room temp (72 F). This means that once the BMC is initialized, the fans should not be expected to reach the speed they started at again. All without adjusting the server's preset fan curves, and with the GPUs never reaching internal temperatures higher than 60 degrees. 

I would not want to run an unmodified 1OU HPC server like the T180-G20 in a living room, but in a basement or attic or garage it is virtually unnoticeable from living space. If you have ever heard a fan-cooled ebay grow light before, it sounds pretty much identically to that during normal operation just with minor fluctuations as GPUs ramp up and down. 

That said, modifying a 1OU server like the T180-G20 for quiet operation would not be very hard: the only reason this server is noisy is because the fans spin at such high RPM (which is necessary for sufficient air movement as they are very small). Swapping for a much larger but lower RPM fan would remove most noise. There are only two air intakes (one on the front with the IO ports, one on the top for accessory cooling) so simply hooking up a large blower motor (think 1/5 hp floor blower fan) to the front and sealing the top air intake would provide more than enough airflow to allow one to remove the small fans entirely. 

### Conclusion

To conclude, you can build a 4x SXM2 V100 server that is very good at all sorts of things for well under two grand if you are willing to be resourceful and hack a power supply together. I reckon that this system is overkill for some types of model inference, but for large-context LLM inference where compute is limiting, or else training models, or especially anything dealing with high-precision floating point operations it seems to be one of the best deals to be had. 

That said, this server performs very well for smaller inference tasks: an 8-bit quantized Llama 3 (8b) runs at ~71 tokens per second while only taking around 75 watts per GPU for unbatched inputs, and a 5.5-bit quantized Llama 3 70b (for a model size of 50 GB) runs at ~14 tokens per second with around 125 watts per GPU. Due to the high CUDA and tensor core count, increasing the context to 2k tokens results in a barely noticeable drop in generation time (~13 tokens per second for 70b llama). To be frank, these are not particularly good tests of this server as the GPUs experience very low Tensor and CUDA core utilization even for long-context inputs, typically less than 30% of all CUDA cores are active during such inference runs.

If you have more cash you could build three of these with 32GB V100s and hook up some mellanox switches (which connect to the free PCIE lanes and ports in the middle of the T180-G20 for up to 128 GB/s internode communication), which would allow for fairly fast training of models up to around 500 billion parameters via bitwise optimized FSDP with low rank adapters.


### Gradients are sensitive to minibatch composition

This idea runs somewhat against the current grain of intuition by many researchers, so reading the preceding paragraphs was probably not sufficient to convince an active member in the deep learning field.  But happily the concept may be shown experimentally and with clarity.  Take a relatively small network being trained to approximate some defined (and non-stochastic) function.  This practically any non-trivial function, and here we will focus on the example detailed [on this page](https://blbadger.github.io/neural-networks3.html). In this particular case, a network is trained to regress an output to approximate the function

$$
y = 10d
$$

where $y$ is the output and $d$ is one of 9 inputs.  The task is simple: the model must learn that $d$ is what determines the output, and must also learn to decipher the numerical input of $d$, or in other words the network needs to learn how to read numbers that are given in character form.  A modest network of 3.5 million parameters across 3 hidden layers is capable of performing this task extremely accurately. 

In the last section, the landscape of $h$ was considered.  Here we will focus on the gradient of $h$, as stochastic gradient descent is not affected by the values of $h$ but only their rate of change as $\theta$ chages. We can observe the gradient of the objective function $J(O(a; \theta))$ with respect to certain trainable parameters, say two parameters in vector form $x = (x_1, x_2)$.  The gradient is signified by $\nabla_x J(O(a; \theta))$ and resulting vector is two dimensional and may be plotted in the plane, as $x$ is equivalent to the projection of the gradient $\nabla_\theta J(O(a; \theta))$ onto our two parameters.  But we are interested in more than just the gradient of the parameters: we also want to visualize the landscape of the possible gradients nearby, that is, the gradients of $\nabla_x J(O(a; \theta), y)$ if we were to change the parameter $x$ slightly, as this is how learning takes place during SGD.  The gradient landscape may be plotted by assigning gradients to points on a 2-dimensional grid of possible values for the parameters $(x_1 + \epsilon_n, x_2 + \epsilon_n)\; n \in \Bbb Z$ that are near the model's true parameters $x$.  In the following plot, the $\nabla_x J(O(a; \theta), y)$ vector is
located at the center circle, and the surrounding vectors are the gradients $\nabla_{x'} J(O(a; \theta), y)$ with $x'$ signifying $x_1+\epsilon_n, x_2 + \epsilon_n$ 

![gradients]({{https://blbadger.github.io}}/neural_networks/gradient_quiver.png)

Because our model is learning to approximate a deterministic function applied to each input, the classical view of stochastic gradient descent suggests that different subsets of our input set will give approximately the same gradient vectors for any given parameters, as the information content in each example is identical (the same rule is being applied to generate an output). In contrast, our idea is that we should see significant differences in the gradient vectors depending on the exact composition of our inputs, regardless of whether or not their informational content is identical w.r.t. the loss function.

Choosing an epoch that exhibits a decrease in the cost function $J(O(a; \theta), y)$ (corresponding to 6 seconds into [this video](https://www.youtube.com/watch?v=KgCuK6v_MgI)) allows us to investigate the sensitivity (or lack thereof) of the model's gradients to input $a$ during the learning process. As above the gradient's projection onto $(x_1, x_2)$ is plotted but now we observe the first two bias parameters in two hidden layers.  The model used on this page has three hidden layers, indexed from 0, and we will observe the gradient vectors on the second and third layer.

One can readily see that for 50 different minibatches $a_1, a_2,...,a_{50} \in a$ (each of size 64) of the same training set, there are quite different (sometimes opposite) vectors of $\nabla_x J(O(a_n; \theta), y)$ 

![gradients]({{https://blbadger.github.io}}/neural_networks/gradients_epoch10_eval.gif)

In contrast, at the start of training the vectors of $\nabla_x J(O(a; \theta), y)$ tend to yield gradients on $x$ that are (somewhat weak) approximations of each other.

![gradients]({{https://blbadger.github.io}}/neural_networks/gradients_start_eval.gif)

Regularization is the process of reducing the test error without necessarily reducing training error, and is thus important for overfitting.  One nearly ubiquitous regularization strategy is dropout, which is where individual neurons are stochastically de-activated during training in order to force the model to learn a family of closely related functions rather than only one.  It might be assumed that dropout prevents this difference in $\nabla_x J(O(a; \theta), y)$ between minibatches during training, but it does not: we still have very different gradient landscapes depending on the input minibatch. Note too how dropout leads to unstable gradient landscapes, where adjacent gradient projections are unpredictably different from one another.

![gradients]({{https://blbadger.github.io}}/neural_networks/gradients_epoch10.gif)

but once again this behavior is not as apparent at the start of training

![gradients]({{https://blbadger.github.io}}/neural_networks/gradients_start.gif)

Another technique used for regularization is batch normalization.  This method is motivated by an intrinsic problem associated with deep learning: the process of finding the gradient of the cost function $J$ with respect to parameters $x$ with respect to the cost function $\nabla_x J(O(a; \theta), y)$ may be achieved using backpropegation, but the gradient descent update of $x$, specifically $x - \epsilon\nabla_x J(O(a; \theta), y)$, assumes that no other parameters have been changed.  In a one-layer (inputs are connected directly to outputs) network this is not much of a problem because the contribution of $x_n$ (ie the weights) to the output's activations are additive. This is due to how most deep learning models are set up: in a typical case of a fully connected layer $h$ following layer $h_{-1}$ given the weight vector for that neuron $w$ and bias scalar $b$

$$
h = w^Th_{-1} + b
$$

where $w^Th_{-1}$ is a vector dot product, a linear transformation that adds all $w_nh_{-1,n}$ elements.  The gradient is computed and updated in (linear) vector space, so if a small enough $\epsilon$ is used then gradient descent should decrease $J$, assuming that computational round-off is not an issue.

But with more layers, the changes to network components becomes exponential with respect to the activations at $h$. To see why this is, note that for a four-layer network with biases set to 0 and weight vectors all equal to $w$

$$
h = w^T(w^T(w^T(w^Th_{-4})))
$$

Now updates to these weight vectors, $w - \epsilon\nabla_w J(O(a; \theta), y)$ are no longer linear with respect to the activation $h$.  In other words, depending on the values of the components of the model a small increase in one layer may lead to a large change in other layers' activations, which goes against the assumption of linearity implicit in the gradient calculation and update procedure.

Batch normalization attemps to deal with this problem by re-parametrizing each layer to have activations $h'$ such that they have a defined standard deviation of 1 and a mean of 0, which is accomplished by using the layer's activation mean $\mu$ and standard deviation $\sigma$ values that are calculated per minibatch during training.  The idea is that if the weights of each layer form distributions of unit variance around a mean of 0, the effect of exponential growth in activations (and also gradients) is minimized.

But curiously, batch normalization also stipulates that back-propegation proceed through these values $\sigma, \mu$ such that they are effectively changed during training in addition to changing the model parameter. Precisely, this is done by learning new parameters $\gamma, \beta$ that transform a layer's re-paremetrized activations $h'$ defined by the function

$$
h'' = \gamma h' + \beta
$$

which means that the mean is multiplied by $\gamma$ before being added by $\beta$, and the standard deviation is multiplied by $\gamma$. This procedure is necessary to increase the ability of batch normalized models to approximate a wide enough array of functions, but it in some sense defeats the intended purpose of ameliorating the exponential effect, as the transformed layer $h''$ has a mean and standard deviation can drift from the origin and unit value substantially. Why then is batch normalization an effective regularizer?

Let's investigate by applying batch normalization to our model and observing the effect on the gradint landscape during training. When 1-dimensional batch normalization is applied to each hidden layer of our model above, we find at 10 epochs that $\nabla_x J(O(\theta; a), y)$ exhibits relatively unstable gradient vectors in the middle layer.  As we saw for dropout and non-regularized gradients, different minibatches have very different gradient landscapes.

![gradients]({{https://blbadger.github.io}}/neural_networks/gradients_epoch10_batchnorm.gif)

Thus we come to the interesting observation that batch normalization leads to a similar loss of stability in the gradient landscape that is seen for dropout. which in this author's opinion is a probable reason for its success as a regularizer (given dropout's demonstrated success in this area).  This helps explain why it was found that batch normalization and dropout are often able to substitute for each other in large models: it turns out that they have similar effects on the gradient landscape of hidden layers, although batch normalization in this case seems to be a more moderate inducement of this loss of stability.

Note that for each of the above plots, the model's parameters $\theta$ did not change between evaluation of different minibatches $a_n$, of in symbols there is an invariant between $\nabla_x J(O(a_n; \theta), y) \; \forall n$.  This means that the direction of stochastic gradient descent does indeed depend on the exact composition of the minibatch $a_n$.

To summarize, we find that the gradient with respect to four parameters can change drastically depending on the training examples that make of the given minibatch $a_n$.  As the network parameters are updated between minibatches, both the identity of the inputs per minibatch and the order in which the same inputs are used to update a network determine the path of stochastic gradient descent. This is why the identity of the input $a$ is so important, even for a fixed dataset with no randomness.
## Sine-Cosine grid map

The differential system

$$ 
\cfrac{dx}{dt} = a \cdot cos(y) \\
\cfrac{dy}{dt} = b \cdot sin(x) \tag{1}\label{eq1}
$$

may be viewed using its vector map as follows:

![t=0.05 map]({{https://blbadger.github.io}}/grid_map/cossin_vectors.png)

Trajectories of particles obeying \eqref{eq1} may be observed with Euler's method,

$$
x_{n+1} = x_n + \cfrac{dx}{dt} \Delta t \\
y_{n+1} = y_n + \cfrac{dy}{dt} \Delta t  \tag{2} \label{eq2}
$$

\eqref{eq1} is an unbounded nonlinear two dimensional system.  It is extremely sensitive to initial conditions for certain values of $\Delta t$.  For example, take $\Delta t = 0.8$ and the starting $(x, y)$ coordinates to be $(1, 0)$. The following map is produced:

![t=0.8 map]({{https://blbadger.github.io}}/grid_map/cossin_0.8t.png)

If the starting $x$ coordinate is shifted by a factor of one billionth (to 1.000000001), a completely different map is produced:

![t=0.8 shifted map]({{https://blbadger.github.io}}/grid_map/cossin_0.8t_shifted.png)

Animating the trajectory of both of these maps with $x_{01} = 1$ in red and $x_{02} = 1.000000001$ in blue, we have 

![t=0.8 shifted map]({{https://blbadger.github.io}}/grid_map/grid_vid.gif)

Euler's formula is used to (not very accurately) estimate the trajectory of unsolvable differential equations.  Here it is employed with deliberately large values of delta_t in order to demonstrate a mapping that is not quite continuous but not a classic recurrence (discrete) mapping either.

This idea becomes clearer when the vector map is added to the trajectory.  Observe how the particles are influenced by the vectors, as is the case for a continuous trajectory, 

![t=0.05 map]({{https://blbadger.github.io}}/grid_map/cossin_quivers.png)

and that on close inspection there are gaps between successive iterations, as for a discrete recursive map
![t=0.05 map]({{https://blbadger.github.io}}/grid_map/cossin_quivers_zoom.png)

Systems of ordinary differential equations have one independent variable: time.  This leads to all trajectories being unique.  How is this grid map possible given that each vertex point seems to lead to multiple trajectories?  The answer is that the trajectories get very close to each other but do not touch.  At a smaller scale, an intersection of grids is revealed to not be an intersection at all.

![t=0.05 map]({{https://blbadger.github.io}}/grid_map/grid_map_intersection.png)

\eqref{eq1} is an example of a chaotic mathematical system as it is deterministic but deeply unpredictable: small changes to the starting value of a chaotic system will lead to large changes in the output.  These are also called aperiodic systems, because they never revisit previously visited points.

### An aperiodic, unbounded map

The grid map is an example of an aperiodic but unbounded trajectory.  Aperiodic trajectories must cross each other if bounded, meaning that if one connects the iterations of a discontinuous map over time the connections must cross one another (for why this is, see [here](https://blbadger.github.io/continuity-poincare.html)).  But as the grid map is unbounded, a trajectory does not necessarily have to cross itself in this manner in order to be aperiodic. For larger $\Delta t$ values detailed below, the trajectory does indeed self-cross.

The grid map displays sensitivity to initial values typical of aperiodic maps, and although not bounded the trajectories head towards infinity very slowly. 

### The grid map is indistinguisheable from a random walk Brownian trajectory for some $\Delta t$

Imagine a ball with elastic collisions to sparse particles that flow in the vector map pattern, or else a ball moving smoothly that is only influenced by the vectors at discrete time intervals. Observe what happens with increases in the time step size:

$\Delta t = 0.05$
![t=0.05 map]({{https://blbadger.github.io}}/grid_map/cossin_0.05t.png)

$\Delta t = 0.5$
![t=0.5 map]({{https://blbadger.github.io}}/grid_map/cossin_0.5t.png)

$\Delta t = 13$
![t=13 map]({{https://blbadger.github.io}}/grid_map/cossin_13t.png)

$\Delta t = 15$
![t=15 map]({{https://blbadger.github.io}}/grid_map/cossin_15t.png)

$\Delta t = 18$
![t=18 map]({{https://blbadger.github.io}}/grid_map/cossin_18t.png)

Which has a trajectory that is formed as follows
![t=18 map]({{https://blbadger.github.io}}/grid_map/grid_18.gif)

and still remains extremely sensitive to inital values ($x_0 = 1$ in red, $x_0 = 1.000000001$ in blue).

![t=18 map]({{https://blbadger.github.io}}/grid_map/grid_18_comp.png)

With increases in $\Delta t$, the map's fractal dimension increases. It is impossible for 2-dimensional continuous differential equations to produce a strange (fractal) attractor, but it is possible for a 2D discrete system to do so.  For more on this topic, see [here](https://blbadger.github.io/continuity-poincare.html). 

At $\Delta t = 18$, the trajectory is indistinguisheable from random walk, which is often modelled mathematically by a system called a ([Wiener process](https://en.wikipedia.org/wiki/Wiener_process)).  This is not peculiar to the equation system \eqref{eq1} but is a feature of many nonlinear systems (see the logistic attractor or Clifford attractor pages) that are iterated discontinuously.  

Why is this important?  It means that real observations that are normally ascribed to a stochastic (usually linear) model are equally ascribable to deterministic nonlinear models.  And this is important because once we have perfomed an inversion with respect to what can be ascribed to stochastic versus deterministic events, we can invert the reasoning on what is insignificant data ('noise') versus what is significant ('signal').  What one normally thinks of as signal may actually be far less important to understanding an underlying physical process than what is considered noise.



## The Henon map

Michel Hnon sought to recapitulate the geometry of the Lorenz attractor in two dimensions.  This requires stretching and folding of space, achieved with the following [discrete system](https://projecteuclid.org/euclid.cmp/1103900150), which is now referred to as the Henon map:

$$
x_{n+1} = 1-ax_n^2 + y_n \\
y_{n+1} = bx_n 
\tag{1} \label{eq1}
$$

When

$$a = 1.4 \\
b = 0.3 \\
x_0, y_0 = 0, 0
$$

the result may be plotted using python as follows:

```python
#python3
import numpy as np 
import matplotlib.pyplot as plt 
plt.style.use('dark_background')

def henon_attractor(x, y, a=1.4, b=0.3):
	'''Computes the next step in the Henon 
	map for arguments x, y with kwargs a and
	b as constants.
	'''
	x_next = 1 - a * x ** 2 + y
	y_next = b * x
	return x_next, y_next
	
# number of iterations and array initialization
steps = 100000
X = np.zeros(steps + 1)
Y = np.zeros(steps + 1)

# starting point
X[0], Y[0] = 0, 0

# add points to array
for i in range(steps):
	x_next, y_next = henon_attractor(X[i], Y[i])
	X[i+1] = x_next
	Y[i+1] = y_next
	
# plot figure
plt.plot(X, Y, '^', color='white', alpha = 0.8, markersize=0.3)
plt.axis('off')
plt.show()
plt.close()
```

After many iterations, the following map is produced:

![map]({{https://blbadger.github.io}}/logistic_map/henon_map.png)

How does the equation produce the map above?  We can plot each point one by one to find out.  To do this, the program above can be modified as follows to make many images of the map of successive iterations of \eqref{eq1}, which can then be compiled into a movie (see [here](/julia-sets.md) for an explanation on how to compile images using ffmpeg).

```python
...

for i in range(steps):
	x_dot, y_dot = henon_attractor(X[i], Y[i])
	X[i+1] = x_dot 
	Y[i+1] = y_dot 
	plt.xlim(-1.5, 1.5)
	plt.ylim(-0.5, 0.5)

	plt.plot(X, Y, '^', color='white', alpha = 0.8, markersize=0.3)
	plt.axis('off')
	plt.savefig('{}.png'.format(i), dpi=300)
	plt.close()
```

For the first thousand iterations:

![map]({{https://blbadger.github.io}}/henon_map/henon_dev.gif)

Successive iterations jump around unpredictably but are attracted to a distinctive curved shape. 

### The Henon map is a strange (fractal) attractor

For certain starting values $x_0, y_0$, \eqref{eq1} with a=1.4 and b=0.3 does not head towards infinity but is instead attracted to the region shown above.  This shape is called an attractor because regardless of where $x_0, y_0$ is placed, if subsequent iterations do not diverge then they are drawn to the shape above.  

Let's examine this attractor.  If we increase magnification on the top line in the center, we find that it is not a line at all!  With successive increases in magnification (and more iterations of \eqref{eq1}), we can see that each top line is actually many lines close together, in a self-similar pattern.  This is indicative of a fractal shape called the Cantor set.

![map]({{https://blbadger.github.io}}/henon_map/henon_zoom1.png)

![map]({{https://blbadger.github.io}}/henon_map/henon_zoom2.png)

![map]({{https://blbadger.github.io}}/henon_map/henon_zoom3.png)

![map]({{https://blbadger.github.io}}/henon_map/henon_zoom4.png)

In general terms, the Henon map is a fractal because it looks similar at widely different scales.  Zooming in near the point (x, y) = (0.3114164... ,  0.234185....), we have

{% include youtube.html id='jG_9x6gMleI' %}

### The boundary of the basin of attraction for the Henon map 

Some experimentation can convince us that not all starting points head towards the attractor upon successive iterations of \eqref{eq1} with $a=1.4$ and $b=0.3$: instead, some head towards positive or negative infinity!  The collection of points that do not diverge (head towards infinity) for a given dynamical system is called the basin of attraction.  Basins of attraction may be fractal or else smooth as shown by [Yorke](https://projecteuclid.org/download/pdf_1/euclid.cmp/1104248191).  Does the Henon map (with $a=1.4, b=0.3$) have a smooth or fractal basin?

To find out, let's import the necessary libraires and define a function `henon_boundary` as follows

```python
#! python3
# import third-party libraries
import numpy as np 
import matplotlib.pyplot as plt 
plt.style.use('dark_background')
import copy

def henon_boundary(max_iterations, a, b):
	''' A function to show the basin of attraction for
	the Henon map.  Takes in the desired number of maximum
	iterations and the a and b values for the Henon equation,
	returns an array of the number of iterations until divergence
	'''
```
Now we can initialize the size of the image (in pixels) we want to make by specifying values for variables `x_range` and `y_range`. A list of points for each variable is made over this range, and x- and y- arrays (`array[0]` and `array[1]`) are formed using the `np.meshgrid` class.  This array will store the values of each point as \eqref{eq1} is iterated.  Next we need an array to store the number of iterations until divergence, which can be accomplished (not particularly efficiently) by making an array of 0s in the same shape as the `array` and then adding the maximum iteration number to each array element.

```python
	x_range = 2000
	y_range = 2000

	x_list = np.arange(-5, 5, 10/x_range)
	y_list = np.arange(5, -5, -10/y_range)
	array = np.meshgrid(x_list, y_list)

	x2 = np.zeros(x_range)
	y2 = np.zeros(y_range)
	iterations_until_divergence = np.meshgrid(x2, y2)

	for i in iterations_until_divergence:
		for j in i:
			j += max_iterations
```
In an effort to prevent explosion of values to infinity, we will run into the possibility that some values can diverge more than once.  To prevent this, we can make a boolean array `not_alread_diverged` in which every element is set to `True` (because nothing has diverged yet).

```python
	# make an array with all elements set to 'True'
	not_already_diverged = array[0] < 1000
```

Now we iterate over the `array` to find when each position diverges towards infinity (if it does).  Because iteration of \eqref{eq1} is a two-step process, the x-array is copied such that it is not modified before being used to make the new y-array.  A boolean array `diverging` is made, signifying whether or not the distance of any point has become farther than 10 units from the origin, which I use as a proxy for divergence.  By using bitwise and, we make a new array `diverging_now` that checks whether divergence has already happened or not, and assigns `True` only to the diverging values that have not. The indicies of `iterations_until_divergence` that are currently diverging are assigned to the iteration number `k`, and the `not_already_diverged` array is updated. Finally, diverging elements of x or y arrays are then assigned as 0 to prevent them from exploding to infinity (as long as the origin does not head towards infinity, that is).

```python
	for k in range(max_iterations):
		array_copied = copy.deepcopy(array[0]) # copy array to prevent premature modification of x array

		# henon map applied to array 
		array[0] = 1 - a * array[0]**2 + array[1]
		array[1] = b * array_copied

		# note which array elements are diverging but have not already diverged 
		r = (array[0]**2 + array[1]**2)**0.5
		diverging = r > 10 
		diverging_now = diverging & not_already_diverged
		iterations_until_divergence[0][diverging_now] = k
		not_already_diverged = np.invert(diverging_now) & not_already_diverged

		# prevent explosion to infinity
		array[0][diverging] = 0
		array[1][diverging] = 0

	return iterations_until_divergence[0]
```

And now this may be plotted.  To overlay the henon map with the attractor basin, the basin map must be scaled appropriately using the kwarg `extent`.

```python
plt.plot(X, Y, ',', color='white', alpha = 0.8, markersize=0.3)
plt.imshow(henon_boundary(70, a=0.2, b=-0.909 - t/6000), extent=[-5, 5, -5, 5], cmap='twilight_shifted', alpha=1)
plt.axis('off')
plt.savefig(Henon_boundary.png', dpi=300)
plt.close()
```

Let's see what happens to the basin of attraction and the attractor itself when $a$ is increased from $1$ to $1.48$ (constant  $b=0.3$):

![map]({{https://blbadger.github.io}}/henon_map/henon_boundary_1_to_1.48.gif)

The attractor is visible as long as it remains in the basin of attraction.  This intuitively makes sense: there is nothing special about the original points compared to subsequent iterations.  If points in an attractor were drawn to a region that then blew up to infinity, the attractor would be no more no matter where the starting point was located. Focusing on the transition from smooth to fractal form in the basin of attraction, we can see this coincides with the disappearence of the attractor itself:

![map]({{https://blbadger.github.io}}/henon_map/henon_boundary_1.41_to_1.45.gif)

This abrupt change between smooth and fractal attractor basin shape is called basin metamorphosis.

### A semicontinuous iteration of the Henon map reveals period doubling 

This map \eqref{eq1} is discrete, but may be iterated using Euler's method as if we wanted to approximate a continuous equation:

$$
\cfrac{dx}{dt} = 1-ax^2 + y \\
\cfrac{dy}{dt} = bx_n \\
x_{n+1} \approx x_n + \cfrac{dx}{dt} \cdot \Delta t \\
y_{n+1} \approx y_n + \cfrac{dy}{dt} \cdot \Delta t \\
\tag{3}
$$

With larger-than-accurate values of $\Delta t$, we have a not-quite-continuous map that can be made as follows:

```python
# import third-party libraries
import numpy as np 
import matplotlib.pyplot as plt 
plt.style.use('dark_background')

def henon_attractor(x, y, a=.1, b=0.03):
	'''Computes the next step in the Henon 
	map for arguments x, y with kwargs a and
	b as constants.
	'''
	dx = 1 - a * x ** 2 + y
	dy = b * x
	return dx, dy
	
# number of iterations and step size
steps = 5000000
delta_t = 0.0047

X = np.zeros(steps + 1)
Y = np.zeros(steps + 1)

# starting point
X[0], Y[0] = 1, 1

# compute using Euler's formula
for i in range(steps):
	x_dot, y_dot = henon_attractor(X[i], Y[i])
	X[i+1] = X[i] + x_dot * delta_t
	Y[i+1] = Y[i] + y_dot * delta_t

# display plot
plt.plot(X, Y, ',', color='white', alpha = 0.1, markersize=0.1)
plt.axis('on')
plt.show()
```

If iterate (3) with $a=0.1, b = 0.03, \Delta t = 0.047 $, the following map is produced:
![t=0.05 map]({{https://blbadger.github.io}}/logistic_map/henon_logistic.jpg)

It looks like the orbit plot for the [logistic map](https://blbadger.github.io/logistic-map.html)! As this system is being iterated semicontinuously, we can observe the vectorfield that the motion of the points:
![t=0.05 map]({{https://blbadger.github.io}}/logistic_map/henon_logistic_quiver2.png)

Subsequent iterations after the first bifurcation lead to the point bouncing from left portion to right portion in a stable period.  In the region of chaotic motion of the point, the vectors are ordered.
![map]({{https://blbadger.github.io}}/logistic_map/henon_logistic_quiver_zoom2.png)

Why is this?  The Henon map has one nonlinearity: an $x^2$.  Nonlinear maps may transition from order (with finite periodicity) to chaos (a period of infinity for most points) with changes in parameter values. The transition from order to chaos for many systems occurs via period doubling leading to infinite periodicity in finite time, resulting in a logistic-like map.  

Renaming $\Delta t$ to $d$ for clarity, we have

$$
x_{n+1} = x_n + (1-ax_n^2 + y) \Delta t \\
x_{n+1} = x_n + d - adx_n^2 + dy \\
x_{n+1} = -adx_n^2 + x_n + d(1+y)
$$

Notice the similarity to the quadratic equation

$$
x_{n+1} = x_n^2 + c
$$

If an orbit map of the quadratic equation (see [this page](https://blbadger.github.io/logistic-map.html) for explanation) where the horizontal axis corresponds to $x_n$ iterations and the vertical axis to $c$ values, multiplied by negative 1 (the actual range is 0 to -2):

![map]({{https://blbadger.github.io}}/henon_map/quadratic_orbit_map.png)

The orbit map for the quadratic equation displays the same periodicity to aperiodicity pattern as the logistic map with period doubling and a chaotic region.  It looks nearly identical to this semicontinuous Henon orbit map!  Could these orbits actually be the same, only in different notation?  

Given the two equations

$$
f(x) = x^2 + c \\
g(x) = -adx^2 + x + d + dy
$$

and a linear transformation, 

$$
h(x) = mx + b
$$

and being that linear transformations do not change the topological properties of a set (they are homeomorphic transformations), if it can be shown that 

$$
h^{-1} \circ f \circ h = g
$$

or equivalently that

$$
h^{-1}(f(h(x))) = g(x) \\
f(h(x)) = h(g(x))
$$

then $f(x)$ is dynamically equivalent to $g(x)$ because these are topological [conjugates](https://en.wikipedia.org/wiki/Topological_conjugacy) of one another.  

Expanding these expressions and simplifying, we have

$$
(mx+b)^2 + c = m^2x^2 + 2mbx + b^2 + c \\
m(-adx^2 + x + d + dy) + b \implies \\
mx^2+2bx + \frac{b^2}{m} + \frac{c}{m} = -adx^2 + x + d + dy + \frac{b}{m}
$$

now by a change of variables,

$$
m = -ad \implies 2bx + \frac{b^2}{-ad} + \frac{c}{-ad} = x + d + dy + \frac{b}{-ad} \\
b = 1/2 \implies \frac{c}{-ad} = d + dy + \frac{1}{-4ad} \\
$$

and therefore

$$
c = -ad \left( d+dy-\frac{1}{4ad} \right) = -ad^2(1+y) + 1/4
$$

results in $f(h(x)) = h(g(x))$, which can be checked by substituting the values obtained for $m, \; b, \; c$ and simplifying. This being the case, these two expressions are conjugates of each other, meaning that it is no surprise that they are capable of displaying nearly identical dynamics.

### Pendulum map from the Henon attractor

This is not the only similarity the Henon map has to another dynamical system: \eqref{eq1} can also result in a map that displays the waves of the [semicontinuous pendulum map](/pendulum-map.md).  The $a, b$ values yielding the spiral patterns were found [here](https://mathworld.wolfram.com/HenonMap.html).

Setting $a=0.2, b=-0.99994$ and $x_0, y_0 = -1.5, 1.5$ we have

![map]({{https://blbadger.github.io}}/henon_map/henon_spiral.png)

The semicontinuous pendulum waves form as a spiral trajectory unwinds with increasing $\Delta t$.  Does this Henon map form the same way?  Let's find out by plotting \eqref{eq1} going from $b \approx -0.9 \to b \approx -1.05$, including the attractor basin.

![map]({{https://blbadger.github.io}}/henon_map/henon_b0.9_to_1.1.gif)


Similarly, at $a=0.2, b=0.999448$ and $x_0, y_0 = 0, 0$, there are two pendulum-like maps

![map]({{https://blbadger.github.io}}/henon_map/henon_spiral2.png)

which form as spirals unwind before the attractor basin collapses from $b=0.95 \to b\approx 1.05$:

![map]({{https://blbadger.github.io}}/henon_map/henon_double_b0.95_to_1.1.gif)

Thus the waves of the henon map form in a similar fashion to those seen in the pendulum phase space.  But there is a significant difference between these two maps: the Henon spiral does not settle on a periodic orbit (as is the case for the pendulum map for certain parameter values) but continues to head towards a point attractor as long as 0 > b > -1.  

Note that unlike the case for $a=1.4, b=0.3$, the basin of attraction is a fractal while a stable attractor remains.  The fractal edge of the basin of attraction extends outward when the attractor remains (as for the spiral maps) but extends inward into the attractor space in the region of $a=1.4, b=0.3$.

To observe the behavior of stable and unstable points for the Henon map iterated in reverse, see [this page](https://blbadger.github.io/aperiodic-inverted.html).

### Fractal zoom on a henon map divergence

At $a=0.2$ and $b=-1.1$, points head towards infinity nearly everywhere.  One point that does not diverge is where the next iteration is equivalent to the current, or where $x_{n+1} = x_n$ and $y_{n+1} = y_n$.  This can be found as follows:

$$
x_n = 1 - ax_n^2 + y_n \\
y_n = bx_n \implies \\
0 = 1-ax_n^2 + (b-1)x_n + 1 \\
$$

which by the quadratic formula yields 

$$
x_n = \frac{(b-1) \pm \sqrt{(b-1)^2 + 4a}}{2a} \\
y_n = bx_n
$$

When $a = 0.2, b = -1.1$ is substituted into this equation system, we can evaluate two non-diverging points at $(x, y) \approx (0.456, -0.502)$ and $(x, y) \approx (-10.956, 12.052)$.  Both coordinates are unstable: only the (irrational) values of

$$
x = \frac{(-2.1) \pm \sqrt{(-2.1)^2 + 0.8}}{0.4} \\
y = -1.1x
$$

will remain in place for an arbitrary number of iterations.  Approximations, no matter how accurate, will diverge over time. This is important because there are no perfect finite representations of irrational numbers, meaning that any form of the radical above that can be stored in finite memory will eventually diverge to infinity given enough iterations of \eqref{eq1}.  

The former coordinate lies at the center of the pinwheel, meaning that regions nearby converge more slowly than regions elsewhere and is therefore semistable.  The latter point is unstable, such that iterations arbitrarily close rapidly diverge.  To get an idea of just how unstable this point is, for $(x, y) \approx (-10.956, 12.052)$ at 64 bit precision (meaning that x is defined as -10.956356105256663), divergence occurs after a mere ~28 iterations.  In contrast, it takes over five hundred iterations for $(x, y) \approx (0.456, -0.502)$ at 64 bit precision to diverge.

Let's zoom in on the pinwheel-like pattern of slower divergence around $(x, y) \approx (0.456, -0.502)$ to get an appreciation of its structure!  The first step is to pick a point and then adjust the array the graph is produced on accordingly.

```python
def henon_map(max_iterations, a, b, x_range, y_range):
	# offset slightly from true value
	xl = -5/(2**(t/15)) + 0.4564
	xr = 5/(2**(t/15)) + 0.4564
	yl = 5/(2**(t/15)) - 0.50202
	yr = -5/(2**(t/15)) - 0.50202

	x_list = np.arange(xl, xr, (xr - xl)/x_range)
	y_list = np.arange(yl, yr, -(yl - yr)/y_range)
	array = np.meshgrid(x_list, y_list)
```
Now we can plot the images produced (adjusting the `extent` variable if the correct scale labels are desired) in a for loop.  I ran into a difficulty here that I could not completely debug: the `diverging` array in the `henon_map()` function occasionally experienced an off-by-one error in indexing: if the `array[0]` dimensions are 2000x2000, the `diverging` array would become 2001x2001 or 2002x2002 etc. The root of this problem is a round off error in the array size calculation (here `10 / x_range`). Although the workaround above is effective, the simplest and most efficient way of addressing this is to simply take the correct number of indicies of the `x_list` and `y_list` arrays when making the two dimensional `array`. 

```python
...
def henon_boundary(max_iterations, a, b):
	x_range = 2000
	y_range = 2000

	x_list = np.arange(-5, 5, 10/x_range)
	y_list = np.arange(5, -5, -10/y_range)
	array = np.meshgrid(x_list[:2000], y_list[:2000])
...
```

When $a=0.2, b=-1.1$, increasing the scale by a factor of $2^{20}$ around the point $(x, y) = (0.4564, -0.50202)$, we have

![map]({{https://blbadger.github.io}}/henon_map/henon_boundary_zoom.gif)



## Homeostasis

### Background

Homeostasis is a fundamental feature of living organisms, and is defined by [Britannica](https://www.britannica.com/science/homeostasis) to be 

"Any self-regulating process by which biological systems tend to maintain stability while adjusting to conditions that are optimal for survival"

although the conditions to which a biological system is subjected to are usually actually sub-optimal.  The next sentence is:

"If homeostasis is successful, life continues; if unsuccessful, disaster or death ensues." 

The idea here is that if a living organism is moved from one environment to another, it must change its physiology such that some internal internal parameters are unchanged, or else the organism dies.  Exactly which parameters are important depends on the organism: warm-blooded endothermic animals cannot tolerate more than a few degrees of core temperature change, whereas ectotherms can tolerate a wide range of temperatures while still maintaining homeostasis and thefore stability (ie little change) in other parameters.  

Dramatic demonstrations of homeostasis have been made to audiences: in one of the most famous, the physician Blagden entered an oven with a dog and a piece of meat, and emerged forty minutes later with the dog and a cooked steak.  Blagden and his dog cooled themselves via evaporation of water, and were able to maintain homeostasis whereas the meat could not, and therefore it was irrevocably changed.

The connection between homeostasis and life is nearly always present: organisms that fail to undergo homeostasis die.  The exceptions are the dormant stages of life that commonly occur upon dessication or freezing or germination.  

Continuing our definition:

"The stability attained is actually a dynamic equilibrium, in which continuous change occurs yet relatively uniform conditions prevail."

Dynamic means changing over time, therefore homeostasis exists in a dynamical system.  The continuous changes that result in relatively uniform internal conditions can be thought of as the path, or trajectory, of the dynamical system.  Here the change in parameters of temperature, oxygen consumption etc. may be plotted over time in the same way that a phase space may be used to observe the changes in kinetic energy as position changes for a [pendulum](/pendulum-map.md).

### Homeostasis as an attractor in a nonlinear dynamical system

What type of dynamical system would lead to continuous changes that preserve similar conditions over time, even when outside influences nudge them towards instability?  Any organism that ages does not re-enter precisely the same condition as it was in before, otherwise we could remove time from the system and the organism would last perpetually without some adverse outside influence.  But for nearly all organisms, this is not the case and ageing occurs, which can eventually disrupt homeostasis once and for all. 

This means that homeostasis is not just in any dynamical system: it is an [aperiodic dynamical system](/index.md), or one that does not precisely revisit a previous state as time passes.  As only nonlinear (or piecewise linear, which one can think of as being discontinuously nonlinear) dynamical systems exhibit attractors that are not points (with measure > 0) and are periodic, homeostasis can be viewed as an attractor in a nonlinear dynamical system.

Another line of reasoning reaches the same conclusion.  Negative feedback is feedback that is used to decrease the distance from a desired state.  The classic example here is a home thermostat: if the temperature is too low, a heater is activated until it is within an acceptable margin.  If the temperature is too high, cooling is activated and both heating and cooling are negative feedback. 

Homeostasis can be equated to the combination of negative feedback events.  Feedback can be stable, where Negative feedback can be linear, in which case it yields a stable or unstable outcome regardless of the inputs, or else it can be nonlinear in which case some inputs can yield stability and some yield instability (see Pierce 1961).   The many instances of homeostatic feedback that takes a small and relatively harmless input and yields an undesirable output (for example take any allergy or the temperature fall during hypothermia, etc.) means that the feedback is not stable regardless of the inputs.  Thus the feedback must be nonlinear, meaning that homeostasis exists as an attractive state in a nonlinear system. 


### Implications for biological research

Why does it matter if homeostasis is a nonlinear dynamical attractor?  It matters because nonlinear systems are not additive, and the scientific method of experimentation via variable isolation and manipulation assumes linearity (specifically additivity).  This means that nonlinear systems are poorly understood by the methods which are normally used for biological research.

To see why experimentation assumes linearity, imagine you want to understand how a mechanical watch works.  You open it up and see a little wheel spinning to and fro, many gears moving at various speeds, and a spring.  What does the spring do?  Remove it and all the gears stop: therefore the spring is necessary to cause the gears (and ultimately the hands) to move.  In mathematical logic terms, 'if $m$ then $s$' or formally $ m \supset s $ means the same thing as '$s$ is necessary for $m$' and so for our watch, if it is moving ($m$) then it must have a spring ($s$).  Note that $\supset$ signifies 'implies', not 'is a superset of' as for set theory.

In order to learn this experimentally, the spring alone must be affected.  If removing the spring also causes all the gears to fall out then the conclusion above is no longer necessarily true.  It could be that having gears is necessary for movement but not having a spring, or $g \supset m$.  Changing only the spring is an isolation of variables, and it is necessary for an experiment to yield any information.  But now imagine that the watch is nonlinear: the parts are not additive, meaning that they are not separable.  This means that whenever someone tries to remove the spring, the gears fall out.  Then the conclusion is that the spring is necessary to stop the watch hands from spinning freely could be made.  This is not accurate because it is actually the gears that prevent the hands from spinning freely. By failing to isolate the spring variable from the gear variable, a conclusion based on experimentation is not helpful.

Consider that an analagous process occurs in any homeostatic organism.  Say we want to understand the function of protein X in a cell, so we induce some change to reduce it's number in the cell by a fifth.  Without knowning which protein this is, a molecular biologist could reasonably predict that nothing will happen.  Indeed, the majority of proteins in the cell are non-essential meaning that if all of one of these is removed, homeostasis remains.  Does this mean that none of these proteins do anything in the cell?  No, because the variables are not separable: removing one changes the rest, in this case perhaps causing other proteins to compensate for the loss of one. 

The current reproducibility crisis of scientific fields investigating living organisms (biomedicine, psychology etc.) is likely a consequence of attempts to apply a method that assumes linearity to systems that are not linear.  Nonlinearity means that isolation of variables is not possible, and therefore experimentation leading to meaningful prediction is not possible for an arbitrary nonlinear system.


### Approximations

Does nonlinearity always matter? In other words, can we ever study a nonlinear system by pretending that it is linear and using the scientific method of variable isolation and experimentation?  Sometimes yes, when any particular influence is large enough.  This is the case for the [three body problem](/3-body-problem.md), a nonlinear system of three astronomical bodies that are subject to the gravitational attraction of each other.  If one of the bodies is much less massive than the others, the three body problem can be approximated by the much simpler two body problem. 

In a similar way, one can draw conclusions about a living system when one or a few factors act to negate homeostasis in some way.  For example, removal of oxygen prevents respiration and kills nearly all animal cells because homeostasis is lost.  We can with reasonable confidence state that oxygen is necessary for respiration because these events cease completely when oxygen is removed.  We can approximate the system as a linear one with respect to oxygen removal because any attempt in a cell to cope with a lack of oxygen (and there are many such attempts) has little effect.

It should be noted that this is only true for large effects that destroy homeostasis in some way, and even then approximations should be treated with care for anything other than extremely short-term prediction. Of course, with homeostasis lost there is likely no long term to think of.  





























 <head>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  
 <!--
 <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171312398-1"></script>
 <script>
   window.dataLayer = window.dataLayer || [];
   function gtag(){dataLayer.push(arguments);}
   gtag('js', new Date());

   gtag('config', 'UA-171312398-1');
 </script>
 -->

 <meta name="Description" CONTENT="Author: Benjamin Badger, Category: Informational">
 <meta name="google-site-verification" content="UtBQXaaKqY6KYEk1SldtSO5XVEy9SmoUfqJ5as0603Y" />
 </head>

## Dynamics
The mathematical approach to change over time. Most dynamical systems are nonlinear and generally unsolvable, and though deterministic are often unpredictable.  

### [Logistic map](/logistic-map.md)

![logistic map image]({{https://blbadger.github.io}}logistic_map/logistic_trace0428_cropped.png)


### [Clifford attractor](/clifford-attractor.md)

![clifford vectors image]({{https://blbadger.github.io}}clifford_attractor/semi_clifford_cover.png)


### [Grid map](/grid-map.md)

![Grid map image]({{https://blbadger.github.io}}misc_images/grid_map_cover.gif)

### [Pendulum phase space](/pendulum-map.md)

![pendulum]({{https://blbadger.github.io}}misc_images/pendulum_cover.png)

## Boundaries 
Trajectories of any dynamical equation may stay bounded or else diverge towards infinity.  The borders between bounded and unbounded trajectories can take on spectacular fractal geometries.  

### [Polynomial roots I](/polynomial-roots.md)

![roots]({{https://blbadger.github.io}}misc_images/newton_cover.png)


### [Polynomial roots II](/polynomial-roots2.md)

![convergence]({{https://blbadger.github.io}}misc_images/newton_boundary_cover.png)


### [Julia sets](/julia-sets.md)

![julia set1]({{https://blbadger.github.io}}misc_images/julia_cover.png)

### [Mandelbrot set](/mandelbrot-set.md)

![disappearing complex mandelbrot]({{https://blbadger.github.io}}fractals/mandelbrot_complex_disappeared.gif)


### [Henon map](/henon-map.md)

![map]({{https://blbadger.github.io}}misc_images/henon_cover.png)


### [Clifford map](/clifford-boundary.md)

![clifford]({{https://blbadger.github.io}}misc_images/clifford_cover.png)


### [Logistic map](/logistic-boundary.md)

![logistic map image]({{https://blbadger.github.io}}misc_images/logistic_bound_cover.png)


## Foundations

### [Primes are unpredictable](/unpredictable-primes.md) 

$$
\lnot \exists n, m : (g_n, g_{n+1}, g_{n+2}, ... , g_{n + m - 1}) \\
= (g_{n+m}, g_{n+m+1}, g_{n+m+2}, ..., g_{n + 2m - 1}) \\
= (g_{n+2m}, g_{n+2m+1}, g_{n+2m+2}, ..., g_{n + 3m - 1}) \\
\; \; \vdots
$$

### [Aperiodicity implies sensitivity to initial conditions](/chaotic-sensitivity.md)

$$
f(x) : f^n(x(0)) \neq f^{n+k}(x(0)) \forall k \implies \\
\forall x_1, x_2 : \lvert x_1 - x_2 \rvert < \epsilon, \; \\
\exists n \; : \lvert f^n(x_1) - f^n(x_2) \rvert > \epsilon
$$

### [Aperiodic maps, irrational numbers, and solvable problems](/aperiodic-irrationals.md)

$$  
\Bbb R - \Bbb Q \sim \{f(x) : f^n(x(0)) \neq f^k(x(0))\} \\
\text{given} \; n, k \in \Bbb N \; \text{and} \; k \neq n \\
$$

### [Irrational numbers on the real line](/irrational-dimension.md)

$$
\Bbb R \neq \{ ... x \in \Bbb Q, \; y \in \Bbb I, \; z \in \Bbb Q ... \}
$$

### [Discontinuous aperiodic maps](/most-discontinuous.md)

$$
\{ f_{continuous} \} \sim \Bbb R \\
\{ f \} \sim 2^{\Bbb R}
$$

### [Poincar-Bendixson and dimension](/continuity-poincare.md)

$$
D=2 \implies \\
\forall f\in \{f_c\} \; \exists n, k: f^n(x) = f^k(x) \; if \; n \neq k
$$

### [Computability and Periodicity I: the Church-Turing thesis](/solvable-periodicity.md)

$$
\\
\{i_0 \to O_0, i_1 \to O_1, i_2 \to O_2 ...\}
$$

### [Computability and Periodicity II](/uncomputable-aperiodics.md)

$$
x_{n+1} = 4x_n(1-x_n) \implies \\
x_n = \sin^2(\pi 2^n \theta) 
$$

### [Nonlinearity and dimension](/nonlinear-dimension.md)

![mapping]({{https://blbadger.github.io}}misc_images/curve_mapping.png)


### [Reversibility and periodicity](/aperiodic-inverted.md)

$$
x_{n+1} = rx_n(1-x_n) \\
\; \\
x_{n} = \frac{r \pm \sqrt{r^2-4rx_{n+1}}}{2r}
$$

### [Additive transformations](/additivity-order.md)

![random fractal]({{https://blbadger.github.io}}/misc_images/additivity_cover.png)


### [Fractal Geometry](/fractal-geometry.md)

![snowflake]({{https://blbadger.github.io}}/misc_images/fractal_cover.png)


## Physics
As for any natural science, an attempt to explain observations and predict future ones using hypothetical statements called theories.  Unlike the case for axiomatic mathematics, such theories are never proven because some future observation may be more accurately accounted for by a different theory.  As many different theories can accurately describe or predict any given set of observations, it is customary to favor the simplest as a result of Occam's razor.  

### [Three Body Problem I](/3-body-problem.md)

![3 body image]({{https://blbadger.github.io}}/3_body_problem/3_body_cover.png)

### [Three Body Problem II: Parallelized Computation with CUDA](/3-body-problem-2.md)

![3 body image]({{https://blbadger.github.io}}/3_body_problem/cuda_vs_torch.png)

### [Three Body Problem III: Distributed Multi-GPU simulations](/3-body-problem-3.md)

![3 body image]({{https://blbadger.github.io}}/3_body_problem/distributed_threebody_cover.png)

### [Entropy](/entropy.md)

![malachite]({{https://blbadger.github.io}}misc_images/malachite.png)

### [Quantum Mechanics](/quantum-mechanics.md)

$$
P_{12} \neq P_1 + P_2 \\
P_{12} = P_1 + P_2 + 2\sqrt{P_1P_2}cos \delta
$$

## Biology
The study of life, observations of which display many of the features of nonlinear mathematical systems: an attractive state resistant to perturbation, lack of exact repeats, and simple instructions giving rise to intricate shapes and movements.  

### [Genetic Information Problem](/genetic-info-problem.md)

![coral image]({{https://blbadger.github.io}}misc_images/acropora.png)


### [Homeostasis](/homeostasis.md)

![lake image]({{https://blbadger.github.io}}misc_images/lake.png)


## Deep Learning

Machine learning with layered representations.  Originally inspired by efforts to model the animalian nervous system, much work today is of somewhat dubious biological relevance but is extraordinarily potent for a wide range of applications.  For some of these pages and more as academic papers, see [here](https://arxiv.org/search/?searchtype=author&query=Badger%2C+B+L).

### [Image Classification](/neural-networks.md) 

![neural network architecture](/neural_networks/neural_network.png)


### [Input Attribution and Adversarial Examples](/input-adversarials.md)

![neural network architecture]({{https://blbadger.github.io}}/neural_networks/attributions_highres_cropped.gif)


### [Input Generation I: Classifiers](/input-generation.md)

![generated badger](/neural_networks/two_generated_badgers.png)


### [Input Generation II: Vectorization and Latent Space Embedding](/latent_output.md)

![wordnet recovered from imagenet](/neural_networks/nearest_neighbors_animal_embedding.png)


### [Input Generation III: Input Representations](/input-representation.md)

![resnet googlenet transformation](/neural_networks/resnet_vectorized_to_be_googlenet_goldfinch.png)


### [Input Representation I: Depth and Representation Accuracy](/depth-generality.md)

![layer autoencoding](/neural_networks/representation_cover.png)


### [Input Representation II: Vision Transformers](/vision-transformers.md)

![vision transformer layer representations](/neural_networks/vit_cover.png)

### [Language Representation I: Spatial Information](/language-representations.md)

![vision transformer layer representations](/deep-learning/gpt2_features_viz.png)

### [Language Representation II: Sense and Nonsense](/language-representations-inputs.md)

$$
\mathtt{This \; is \; a \; prompt \; sentence} \\ 
\mathtt{channelAvailability \; is \; a \; prompt \; sentence} \\ 
\mathtt{channelAvailability \; millenn \; a \; prompt \; sentence} \\
\dots \\
\mathtt{redessenal \; millenn-+-+DragonMagazine}
$$

### [Language Representation III: Noisy Communication on a Discrete Channel](/language-discreteness.md)

$$
a_g([:, :, :2202]) = \mathtt{Mario \; the \; Idea \; versus \; Mario \; the \; Man} \\
a_g([:, :, :2201]) = \mathtt{largerpectedino missionville printed satisfiedward}
$$

### [Language Representation IV: Inter-token communication and Masked Mixers](/llm-invertibility.md)

![clm_flow](/deep-learning/clm_cover.png)

### [Language Features](/language-model-features.md)

$$
O_f = [:, \; :, \; 2000-2004] \\
a_g = \mathtt{called \; called \; called \; called \; called} \\
\mathtt{ItemItemItemItemItem} \\
\mathtt{urauraurauraura} \\
\mathtt{vecvecvecvecvec} \\
\mathtt{emeemeemeemeeme} \\
$$

### [Feature Visualization I](/feature-visualization.md)

![features 2](/neural_networks/featuremap_cover2.png)


### [Feature Visualization II: Deep Dream](/deep-dream.md)

![features](/neural_networks/deep_dream_cover.png)


### [Feature Visualization III: Transformers and Mixers](/transformer-features.md)

![features](/deep-learning/transformer_feature_cover.png)

### [Language Mixer](/smaller-lms.md)

![features](/deep-learning/llm_mixer.png)

### [Autoencoders](/autoencoder-representation.md)

![autoencoding of landscapes](/deep-learning/autoencoder_cover.png)


### [Diffusion Inversion](/diffusion-inversion.md)

![features](/neural_networks/diffusion_cover.png)


### [Generative Adversarial Networks](/generative-adversarials.md)

![network architecture](/neural_networks/mnist_2latent_fig.png)


### [Normalization and Gradient Stability](/gradient-landscapes.md)

![network architecture](misc_images/gradient_quivercover.png)


### [Small Language Models for Abstract Sequences](/neural-networks3.md)

![network architecture](/neural_networks/nn_including_embeddings.png)


### [Interpreting Sequence Models](/nn_interpretations.md)

![deep learning attributions](/misc_images/attributions_cover.png)


### [Training Memory](/neural-networks2.md)

### [Limitations of Neural Networks](/nn-limitations.md)

![discontinous proof]({{https://blbadger.github.io}}/neural_networks/discontinous_proof.png)


## Small Projects

### [Game puzzles](/puzzle-projects.md)

![puzzles]({{https://blbadger.github.io}}/assets/images/games.png)

### [Programs to compute things](/computing-programs.md)

$$
\; \\
\begin{vmatrix}
a_{00} & a_{01} & a_{02} & \cdots & a_{0n} \\
a_{10} & a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n0} & a_{n1} & a_{n2} & \cdots & a_{nn} \\
\end{vmatrix}
\; \\
$$

$$ 
\; \\
5!_{10} = 12\mathbf{0} \to 1 \\
20!_{10} = 243290200817664\mathbf{0000} \to 4 \\
n!_k \to ?
\; \\
$$

## Low Voltage
In many ways less stressful than high voltage engineering, still exciting and rewarding.

### [Deep Learning Server](/gpu-server.md)

![DL server]({{https://blbadger.github.io}}server_setup/server_coverphoto.jpg)
 	
## High Voltage 
High voltage engineering projects: follow the links for more on arcs and plasma.

### [Tesla coil](/tesla-coils.md)

![tesla coil arcs]({{https://blbadger.github.io}}tesla_images/newtesla.jpg)


### [Fusor](/fusor.md)

![fusor image]({{https://blbadger.github.io}}fusor_images/fusor-1-1.png)


### [About The Author](/about-me.md)



## Input Attribution and Adversarial Examples

### Classification of Fashion MNIST images

The [fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) is a set of 28x28 monocolor images of articles of 10 types of clothing, labelled accordingly.  Because these images are much smaller than the 256x256 pixel biological images above, the architectures used above must be modified (or else the input images must be reformatted to 256x256).  The reason for this is because max pooling (or convolutions with no padding) lead to reductions in subsequent layer size, eventually resulting in a 0-dimensional layer.  Thus the last four max pooling layers were removed from the deep network, and the last two from the AlexNet clone ([code](https://github.com/blbadger/neural-network/blob/master/fmnist_bench.py) for these networks).  

The deep network with no other modifications than noted above performs very well on the task of classifying the fashion MNIST dataset, and >91 % accuracy rate on test datasets achieved with no hyperparameter tuning. 

![fashion MNIST]({{https://blbadger.github.io}}/neural_networks/Fashion_mnist.png)

AlexNet achieves a ~72% accuracy rate on this dataset with no tuning or other modifications, although it trains much slower than the deep network as it has many more parameters (>10,000,000) than the deep network (~180,000).

We may observe a model's attribution on the inputs from this dataset as well in order to understand how a trained model arrives at its conclusion. Here we have our standard model architecture and we compute the gradientxinput

$$
\nabla_a O(a; \theta) * a
$$

where the input $a$ is a tensor of the input image (28x28), the output of the model with parameters $\theta$ and input $a$ is $O(a; \theta)$, and $*$ denotes Hadamard (element-wise) multiplication.  Here we implement gradientxinput using pytorch as follows:

```python
def gradientxinput(model, input_tensor, output_dim):
	...
	input_tensor.requires_grad = True
	output = model.forward(input_tensor)
	output = output.reshape(1, output_dim).max()

	# backpropegate output gradient to input
	output.backward(retain_graph=True)
	gradientxinput = torch.abs(input_tensor.grad) * input_tensor
	return gradientxinput
```

Note that the figures below also have a max normalization step before returning the gradientxinput tensor.  The `gradientxinput` object returned is a `torch.Tensor()` object and happily may be viewed directly using `matplotlib.pyplot.imshow()`. 

For an image of a sandal, we observe the following attribution:

![fashion MNIST gradientxinput]({{https://blbadger.github.io}}/neural_networks/fmnist_gradxinput.png)

which focuses on certain points where the sandal top meets the sole.  How does a deep learning model such as our convolutional network learn which regions of the input to focus on in order to minimize the cost function?  At the start of training, there is a mostly random gradientxinput attribution for each image

![fashion MNIST gradientxinput]({{https://blbadger.github.io}}/neural_networks/fmnist_attribution_grid0001.png)

but at the end of training, certain stereotypical features of a given image category receive a larger attribution than others: for example, the elbows and collars of coats tend to exhibit a higher attribution than the rest of the garment.

![fashion MNIST gradientxinput]({{https://blbadger.github.io}}/neural_networks/fmnist_attribution_grid0505.png)

It is especially illuminating to observe how attribution changes after each minibatch gradient update.  Here we go from the start of the start to the end of the training as show in the preceding images, plotting attributions on a subset of test set images after each minibatch (size 16) update.

{% include youtube.html id='7SCd5YVYejc' %}

### Flower Type Identification

For some more colorful image classifications, lets turn to Alexander's flower [photoset](https://www.kaggle.com/alxmamaev/flowers-recognition), containing labeled images of sunflowers, tulips, dandelions, dasies, and roses.  The deep network reaches a 61 % test classification score on this dataset, which increases to 91 % for binary discrimination between some flower types. Examples of this model classifying images of roses or dandelions,

![flower classes]({{https://blbadger.github.io}}/neural_networks/Figure_flowers2.png)

sunflowers or tulips,

![flower classes]({{https://blbadger.github.io}}/neural_networks/Figure_flowers1.png)

and tulips or roses

![flower classes]({{https://blbadger.github.io}}/neural_networks/Figure_flowers_tulips_roses2.png)

We can investigate the learning process by using gradientxinput attribution. Before the start of training, we see that there is relatively random attribution placed on various pixels of test set flower images

![flower saliency]({{https://blbadger.github.io}}/neural_networks/flower_attributions0000.png)

but after 25 epochs, certain features are focused upon

![flower saliency]({{https://blbadger.github.io}}/neural_networks/flower_attributions1200.png)

Plotting attribution after every minibatch update to the gradient, we have

{% include youtube.html id='lVcNSD0viX0' %}

The deep learning models generally perform worse on flower type discrimination when they are not given color images, which makes sense being that flowers are usually quite colorful.  Before the start of training, we have a stochastic attribution: note how the model places relatively high attribution on the sky in the bottom three images (especially the bottom right)

![flower saliency]({{https://blbadger.github.io}}/neural_networks/colorflower_attributions0000.png)

In contrast, after 25 epochs of training the model has learned to place more attribution on the tulip flower body, the edge of the rose petals, and the seeds of the sunflower and dandelion.  Note that the bottom center tulip has questionable attribution: the edge of the leaves may be used to discriminate between plant types, but it is not likely that flower pot itself is a useful feature to focus on.

![flower saliency]({{https://blbadger.github.io}}/neural_networks/colorflower_attributions1202.png)

Plotting attribution after every minibatch update to the gradient, we have

{% include youtube.html id='mz_Qo1fcmgc' %}

### Adversarial Examples

Considering the attribution patterns placed on various input images, it may seem that a deep learning object recognition process is similar to a human-like decision making process when identifying images: focus on the features that differ between images and learn which features correspond to what image. But there are significant differences between natural and deep learning-based object recognition, and one of the most dramatic of these differences is the presence of what has been termed 'adversarial examples', first observed by Szegedy and colleagues in their [paper](https://arxiv.org/abs/1312.6199) on this subject.

To those of you who have read [this page](https://blbadger.github.io/nn-limitations.html) on the subject, the presence of adversarial examples should come as no surprise: as a model becomes able to discriminate between more and more input images it better and better approximates a one-to-one mapping between a multidimensional input (the image) and a one-dimensional output (the cost function).  To summarize the argument on that page, there are no continuous one-to-one (bijective) mappings possible from two or more dimensions to one, we would expect to see discontinuities in a function approximating a bijective map between many and one dimension.  This is precisely what occurs when an image tensor input (which for a 28x28 image is 784-dimensional) is mapped by a deep learning model to a loss value by $J(O(a; \theta), y)$.

How might we go about finding an adversarial example?  One option is to compute the gradient $g$ of the loss function of the output $J(O)$ with respect to the input $a$ of the model with parameters $\theta$ with a true classification $y$,

$$
g = \nabla_a J(O(a; \theta), y)
$$

but instead of taking a small step against the gradient (as would be the case if we were computing $\nabla_{\theta} J(O(a; \theta))$ and then taking a small step in the opposite direction during stochastic gradient descent), we first find the direction along each input tensor element that $g$ projects onto with $\mathrm{sign}(g)$ and then take a small step in this direction.

$$
a' = a + \epsilon * \mathrm{sign}(g)
$$

where the $\mathrm{sign}()$ function the real-valued elements of a tensor $a$ to either 1 or -1, or more precisely this function 

$$
f: \Bbb R \to \{ -1, 1 \} 
$$ 

depending on the sign of each element $a_n \in a$. This is known as the fast gradient sign method, and has been reported to yield adversarial examples for practically any CIFAR image dataset input when applied to a trained AlexNet architecture.

What this procedure accomplishes is to change the input by a small amount (determined by the size of $\epsilon$) in the direction that makes the cost function $J$ increase the most, which intuitively is effectively the same as making the input a slightly different in precisely the direction per pixel that makes the neural network less accurate.  

To implement this, first we need to calculate $g$, which may be accomplished as follows:

```python
def loss_gradient(model, input_tensor, true_output, output_dim):
	... # see source code for the full method with documentation
	true_output = true_output.reshape(1)
	input_tensor.requires_grad = True
	output = model.forward(input_tensor)
	loss = loss_fn(output, true_output) # objective function applied (negative log likelihood)

	# only scalars may be assigned a gradient
	output = output.reshape(1, output_dim).max()

	# backpropegate output gradient to input
	loss.backward(retain_graph=True)
	gradient = input_tensor.grad
	return gradient
```

Now we need to calculate $a'$, and here we assign $\epsilon=0.01$. Next we find the model's output for $a$ as well as $a'$, and throw in an output for $g$ for good measure

```python
def generate_adversaries(model, input_tensors, output_tensors, index):
	... # see source code for the full method with documentation
	single_input= input_tensors[index].reshape(1, 3, 256, 256)
	input_grad = torch.sign(loss_gradient(model, single_input, output_tensors[index], 5))
	added_input = single_input + 0.01*input_grad
	
	original_pred = model(single_input)
	grad_pred = model(0.01*input_grad)
	adversarial_pred = model(added_input)
```

Now we can plot images of $a$ and the output of each fed to the model by reshaping the tensor for use in `plt.imshow()` before finding the output class and output confidence (as we are using a softmax output) from $O(a; \theta)$ as follows

```python
	...
	input_img = single_input.reshape(3, 256, 256).permute(1, 2, 0).detach().numpy() # reshape for imshow 
	original_class = class_names[int(original_pred.argmax(1))].title() # find output class
	original_confidence = int(max(original_pred.detach().numpy()[0]) * 100) # find output confidence w/softmax output
```

and finally we can perform the same procedure to yield $g$ and $a'$.  

For an untrained model with randomized $\theta$, $O(a';\theta)$ is generally quite similar to $O(a;\theta)$. The figures below display typical results from computing $\mathrm{sign}(g)$ (center) from the original input $a$ (left) with the modified input $a'$ (right). Note that the image representation of the sign of the gradient clips negative values (black pixels in the image) and is not a true representation of what is actually added to $a$: the true image of $\mathrm{sign}(g)$ is 50 times dimmer than shown because by design $\epsilon * \mathrm{sign}(g)$ is practically invisible. The model's output for $a$ and $a'$ is noted above the image, with the softmax value converted to a percentage for clarity.

![adversarial example]({{https://blbadger.github.io}}/neural_networks/flower_start_adversarial0.png)

![adversarial example]({{https://blbadger.github.io}}/neural_networks/flower_start_adversarial1.png)

After training, however, we see some dramatic changes in the model's output (and ability to classify) the image $a$ compared to the shifted image $a'$. In the following three examples, the model's classification for $a$ is accurate, but the addition of an imperceptably small amount of the middle image to make $a'$ yields a confident but incorrect classification.

![adversarial example]({{https://blbadger.github.io}}/neural_networks/adversarial_example0.png)

![adversarial example]({{https://blbadger.github.io}}/neural_networks/adversarial_example1.png)

![adversarial example]({{https://blbadger.github.io}}/neural_networks/adversarial_example6.png)

In contrast, the addition of pixels that are randomly assigned only rarely changes the model's output significantly. The following is a typical example of the result of addition of a random tensor to a given input image.

![adversarial example]({{https://blbadger.github.io}}/neural_networks/flower_random_addition2.png)

Not all shifted images experience this change in predicted classification: the following images are viewed virtually identically by the model.  After 40 epochs of training a cNN, around a third of all inputs follow this pattern such that the model does not change its output significantly when given $a'$ as an input instead of $a$ for $\epsilon=0.01$.  Note that increasing the value of $\epsilon$ leads to nearly every input image having an adversarial example using the fast gradient sign method, even if the images are still not noticeably different.

![adversarial example]({{https://blbadger.github.io}}/neural_networks/adversarial_example13.png)

It is interesting to note that the gradient sign image itself may be confidently (and necessarily incorrectly) classified too.

![adversarial example]({{https://blbadger.github.io}}/neural_networks/gradpred_adversarial_example3.png)

Can we find adversarial examples for simpler inputs as well as complicated ones? Indeed we can: after applying the gradient step method to 28x28 pixel Fashion MNIST images using a model trained to classify these inputs, we can find adversarial examples just as we saw for flowers.

![adversarial example]({{https://blbadger.github.io}}/neural_networks/fmnist_adversarial_example.png)

It may see strange to take the sign of the gradient per pixel rather than the projection of the gradient itself, as would be the case if $a$ were a trainable parameter during gradient descent. The authors [this work](https://arxiv.org/pdf/1412.6572.pdf) made this decision in order to emphasize the ability of a linear transformation in the input to create adversarial examples, and went on to assert that the major cause of adversarial examples in general is excessive linearity in deep learning models.

It is probable that such linearity does indeed make finding adversarial examples somewhat easier, but if the argument on [this page](https://blbadger.github.io/nn-limitations.html) is accepted then attempting to prevent adversarial examples using nonlinear activation functions or specialized architectures is bound to fail, as all $f: \Bbb R^n \to \Bbb R$ are discontinuous if bijective.  Not only that, but such $f$ are everywhere discontinuous, which is why each input image will have an adversarial example if we assume that $f$ approximates a bijective function well enough.

What happens when we manipulate the image according to the gradient of the objective function, rather than its sign?  Geometrically this signifies taking the projection of the gradient $g$

$$
g = \nabla_a J(O(a; \theta), y)
$$

onto each input element $a_n$, which tells us not only which pixels to modify but also how much to modify them. When we scale this gradient by max norming

$$
g' = \frac{g}{\mathrm{max} (g)}
$$

and then applying this normed gradient to the input $a$ to make $a'$

$$
a' = a + \epsilon * g'
$$

we once again find adversarial examples, even with very small $\epsilon$.  Here the gradient tensor image is to scale: the center image is $\epsilon *g'$ and has values that are not scaled up as they were in the other images on this page (and the prediction for $a$ on the left is followed by the true classification for clarity)

![adversarial example]({{https://blbadger.github.io}}/neural_networks/continuous_adversarial_example.png)

Empirically this method performs as well if not better than the fast gradient sign procedure with respect to adversarial example generation: while keeping $\epsilon$ small enough to be unnoticeable, the majority of inputs may be found to have corresponding adversarial examples.

It is interesting to observe the gradient images in more detail: here we have the continuous gradient $\epsilon * g'$ scaled to be 60 times brighter (ie larger values) than $\epsilon * g'$ for clarity.

![adversarial example]({{https://blbadger.github.io}}/neural_networks/enhanced_continuous_adversarial_example.png)

Close inspection of the image of $\epsilon * g'$ reveals something interesting: the gradient tensor appears to mirror a number of the features of the original input image, except with dark blue petals instead of white and a mix of blue and yellow where the disk flourets (center yellow parts) are found in the original image. This may be thought of as a recapitulation of the features of an input image itself from the sole informational content of the gradient of the loss function with respect to the input,

$$
\nabla_a J(O(a; \theta), y)
$$

Recalling how forward propegation followed by backpropegation is used in order to compute this gradient, we find that these features remain after nearly two dozen vector arithmetic operations, none of which are necessarily feature-preserving.  From an informational perspective, one can think of this as the information from the input being fed into the neural network, stored as activations in the network's various layers, before that information is then used to find the gradient of the loss function with respect to the input.

The above image is not the only input that has features that are recapitulated in the input gradient: here some tulips 

![adversarial example]({{https://blbadger.github.io}}/neural_networks/adversarial_gen_tulip.png)

and a butterfly on a dandelion 

![adversarial example]({{https://blbadger.github.io}}/neural_networks/adversarial_gen_butterfly.png)

and the same is found for a daisy.

![adversarial example]({{https://blbadger.github.io}}/neural_networks/adversarial_gen_daisy.png)

It is important to note that untrained models are incapable of preserving practically any input features in the input gradient.  This is to be expected given that the component operations of forward and backpropegation have no guarantee to preserve any information.  

### Additive Attributions for Nonlocality

In the last section, we saw that the training process (here 40 epochs) leads to a preservation of certain features of the input image in the gradient of the input with respect to the loss function.  We can observe the process of feature preservation during model training as follows:

{% include youtube.html id='sflMrJLlb0g' %}

Gradientxinput has been criticized for relying entirely on locality: the gradient of a point in multidimensional space is only accurate in an infinitesmal region around that point by definition.  Practically this means that if an input were to change substantially, a pure gradient-based input attribution method may not be able to correctly attribute that change to the output (or loss function) if there is not a local-to-global equivalence in the model in question.

There are a number of ways to ameliorate this problem.  One is to directly interfere with (occlude) the input, usually in some fairly large way before observing the effect on the output.  For image data, this could mean zeroing out all pixels in a given region that scans over the entire input.  For sequential data as seen [here](https://blbadger.github.io/nn_interpretations.html), successive characters can be modified as the model output is observed.  Occlusion usually introduces substantial changes from the original input meaning that the observed output changes are not the result of local changes.  Occlusion can be combined with gradientxinput to make a fairly robust attribution method.

Another way to address locality is to add up gradients as the input is formed in a straight-line path from some null reference, an approach put forward in [this paper](https://arxiv.org/abs/1703.01365) by Yan and colleages.  More concretely, a blank image may serve as a null reference and the true image may be formed by increasing brightness (our straight-line path) until the true image is recovered.  At certain points along this process, the gradients of the model with respect to the input may be added to make one integrated gradient measurment. This method has some benefits but also has a significant downside: for many types of input, there is no clear straight-line path.  Image input data has a couple clear paths (brightness and contrast) but discrete inputs such as language encodings do not.

An alternative to this approach could be to integrate input gradients but instead of varying inputs for a trained network, we integrate the input gradients during training for one given input $a$.  If we were to use the loss gradient, for each configuration $\theta_n$ of the model during training we have

$$
g = \sum_{n} \nabla_a J(O(a; \theta_n), y)
$$

This method may be used for any input type, regardless of an ability to transform from a baseline.

## Image Generation with Classification Models I

### Modifying an input via gradient descent

The observation that a deep learning model may be able to capture much of the inportant information in the input image leads to a hypothesis: perhaps we could use a trained model in reverse to generate inputs that resemble the training inputs.

On [this page](https://blbadger.github.io/input-adversarials.html) we saw that the loss gradient with respect to the input $\nabla_a J(O(a; \theta), y)$ of a trained model is able to capture certain features of input images: in particular, the gradient mirrors edges and certain colors that exist in the input.  This observation leads to an idea: perhaps we could use this gradient to try to make our own input image by starting with some known distribution and repeatedly applying the loss gradient to the input.  This process mirrors how stochastic gradient descent applies the loss gradient with respect to the model parameters each minibatch step, but instead of modifying the model parameters instead we are going to modify the input itself.

If we want to apply the loss gradient (of the input) to the input, we need three things: a trained model with parameters $\theta$, and input $a$, and an output $y$.  One can assign various distributions to be the input $a$, and arbitrarily we can begin with a stochastic distribution rather than a uniform one.  Next we need an output $y$ that will determine our loss function value: the close the input $a$ becomes to a target input $a'$ that the model expects from learning a dataset given some output $y$, the smaller the loss value.  We can also arbitrarily choose an expected output $\widehat{y}$ with which we use to modify the input, but for a categorical image task it may be best to choose one image label as our target output $\widehat{y}$

Each step of this process, the current input $a_n$ is modified to become $a_{n+1}$ as follows

$$
a_{n+1} = a_n - \epsilon \nabla_a J(O(a; \theta), \widehat{y})
$$

Intuitively, a trained model should know what a given output category generally 'looks like', and performing gradient-based updates on an image while keeping the output constant (as a given category) is similar to the model instructing the input as to what it should become to match the model's expectation.

To implement this algorithm, first we need a function that can calculate the gradient of the loss function with respect to the input

```python
def loss_gradient(model, input_tensor, true_output, output_dim):
	...
	true_output = true_output.reshape(1)
	input_tensor.requires_grad = True
	output = model.forward(input_tensor)
	loss = loss_fn(output, true_output) # loss applied to y_hat and y

	# backpropegate output gradient to input
	loss.backward(retain_graph=True)
	gradient = input_tensor.grad
	return gradient
```

Then the gradient update can be made at each step

```python
def generate_input(model, input_tensors, output_tensors, index, count):
	... 
	target_input = input_tensors[index].reshape(1, 3, 256, 256)
	single_input = target_input.clone()
	single_input = torch.rand(1, 3, 256, 256) # uniform distribution initialization

	for i in range(1000):
		single_input = single_input.detach() # remove the gradient for the input (if present)
		predicted_output = output_tensors[index].argmax(0)
		input_grad = loss_gradient(model, single_input, predicted_output, 5) # compute the input gradient
		single_input = single_input - 10000*input_grad # gradient descent step
```

`single_input` can then be viewed with the target label denoted

![adversarial example]({{https://blbadger.github.io}}/neural_networks/generated_daisy_nosign.png)

but the output does not really look like a daisy, or a field of daisies.  

There are a few challenges using this approach.  Firstly, in general many iterations are required such that updating the input may take a good deal of computational power, and furthermore it may be difficult to predict how large to make $\epsilon$ without prior experimentation.

The second problem is that in general there are discontinuities present in the mathematical function describing the mapping of an input $a$ to the output loss $J(O(a; \theta))$ which necessarily makes the reverse function from output to input also discontinuous.  We are not strictly using the inverse of forward propegation but instead use the gradient of this loss function to generate the input, but if $J(O(a; \theta))$ is discontinuous then so will its gradients be too.

It has been hypothesized that adversarial examples exist in spite of the high accuracy achieved various test datasets because they are very low-probability inputs.  In some respects this is true, as the addition of a small vector in a random direction (rather than the direction of the gradient with respect to the output) very rarely changes the model's output.

![adversarial example]({{https://blbadger.github.io}}/neural_networks/flower_random_addition2.png)

But this is only the case when we look at images that are approximations of inputs that the model might see in a training or test set.  In the adversarial example approaches taken on [this page](https://blbadger.github.io/input-adversarials.html), small shifts are made to each element (pixel) of a real image to make an approximately real image.  If we no longer restrict ourselves in this way, we will see that adversarial examples are actually much more common.  Indeed that almost any input that a standard image classification model would classify as any given label with high confidence does not resemble a real image.  For more information on this topic, see the next section.

In practice the second problem is more difficult to deal with than the first, which can be overcome with clever scaling and normalization of the gradient.  The main challenge for input generation is therefore that the gradient of the loss (or the output, for that matter) with respect to the input $a$, $\nabla_a J(O(a, \theta))$ is approximately discontinuous in certain directions, which can cause a drop in the loss even though the input is far from a realistic image.  The result is that more or less unrecognizable images like the one above are confidently but erroneously classified as being an example of the correct label. For more on the topic of confident but erroneous classification using deep learning, see [this paper](https://arxiv.org/abs/1412.1897).

One way to ameliorate these problems is to go back to our gradient sign method rather than to use the actual gradient.  This introduces the prior assumption that   This allows us to restrict the changes at each iteration to a constant step, stabilizing the gradient update. 

$$
a_{n+1} = a_n - \epsilon \; \mathrm {sign} \; (\nabla_a J(O(a; \theta), \widehat{y}))
$$

which for $\epsilon=0.01$ can be implemented as

```python
	...
	for i in range(100):
		single_input = single_input.detach() # remove the gradient for the input (if present)
		# predicted_output = output_tensors[index].argmax(0)
		input_grad = loss_gradient(model, single_input, target_output, 5) # compute input gradient
		single_input = single_input - 0.01*torch.sign(input_grad) # gradient descent step
```

Secondly, instead of starting with a random input we can instead start with some given flower image from the dataset.  The motivation behind this is to note that the model has been trained to discriminate between images of flowers, not recognize images of flowers compared to all possible images in $\Bbb R^n$ with $n$ being the dimension of the input.  

This method is more successful: when the target label is a tulip, observe how a base and stalk is added to a light region of an image of a field of sunflowers,

![adversarial example]({{https://blbadger.github.io}}/neural_networks/generated_tulip.png)

and how an image of a daisy and a rock is modified to appear more like a field of tulips,

![adversarial example]({{https://blbadger.github.io}}/neural_networks/generated_daisy2.png)

but generally images of tulips are changed less, which is to be expected given that the gradient of the loss function with respect to the input will be smaller if our target output matches our actual output.

![adversarial example]({{https://blbadger.github.io}}/neural_networks/generated_tulip_orig.png)

### Generating Images using a Smoothness Prior

In the last section we saw that using the gradient of the output with respect to the input $\nabla_a J(O(a, \theta))$ may be used to modify the input in order to make it more like what the model expects a given parameter label to be.  But applying this gradient to an initial input of pure noise was not found to give a realistic representation of the desired type fo flower, because the loss function is discontinuous with respect to the input.  Instead we find a type of adversarial example which is confidently assigned to the correct label by the model, but does not actually resemble any kind of flower at all.

Is there some way we can prevent our trained deep learning models from making unrealistic images during gradient descent on a random input?  Research into this question has found that indeed there is a way: restrict the image modification process such that some quality of a real image is enforced.  We will proceed with this investigation using an Inceptionv3 (aka GoogleNetv3) trained on the full ImageNet dataset, which consists of labelled images of 1000 classes.

The inception architecture was designed to allow for deeper networks without an exorbitant increase in the number of parameters used.  InceptionV3 (as published [here](https://arxiv.org/pdf/1512.00567v3.pdf)) has the following architecture:

![adversarial example]({{https://blbadger.github.io}}/neural_networks/inceptionv3_architecture.png)

(image source: Google [docs](https://cloud.google.com/tpu/docs/inception-v3-advanced))

After loading the model with `model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True).to(device)`, we can use the following function

```python

def count_parameters(model):
	table = PrettyTable(['Modules', 'Parameters'])
	total_params = 0
	for name, parameter in model.named_parameters():
		if not parameter.requires_grad:
			continue
		param = parameter.numel()
		table.add_row([name, param])
		total_params += param 
	print (table)
	print (f'Total trainable parameters: {total_params}')
	return total_params
```
to find that this is a fairly large model with `27161264`, or just over 27 million, trainable parameters.

```
+--------------------------------------+------------+
|               Modules                | Parameters |
+--------------------------------------+------------+
|      Conv2d_1a_3x3.conv.weight       |    864     |
|       Conv2d_1a_3x3.bn.weight        |     32     |
|        Conv2d_1a_3x3.bn.bias         |     32     |
|      Conv2d_2a_3x3.conv.weight       |    9216    |
|       Conv2d_2a_3x3.bn.weight        |     32     |
|        Conv2d_2a_3x3.bn.bias         |     32     |
... (shortened for brevity)...
|              fc.weight               |  2048000   |
|               fc.bias                |    1000    |
+--------------------------------------+------------+
```

Now that we have our trained model, which qualities of a real image should be enforced during gradient descent? A good paper on this topic by [Olah and colleages](https://distill.pub/2017/feature-visualization/) details how different research groups have attempted to restrict a variety of qualities, but most fall into fouor categories: input or gradient regularization, frequency penalization, transformational invariance, and a learned prior. 

We will focus on the first three of these restrictions in this section.  

Suppose we want to transform an original input image of noise into an image of some category for which a model was trained to discern.  We could do this by performing gradient descent using the gradient of the loss with respect to the input

$$
a' = a - \epsilon \nabla_a J(O(a, \theta))
$$

which was attempted above.  One difficulty with this approach is that many models use a softmax activation function on the final layer (called logits pre-activation) to make a posterior probability distribution, allowing for an experimentor to find the probability assigned to each class.  Minimizing $J(\mathrm{softmax} \; O(a, \theta))$ may occur via maximizing the value of $O_n(a, \theta)$ where $O_n$ is the index of the target class, the category of output we are attempting to represent in the input.  But as pointed out by [Simonyan and colleagues](https://arxiv.org/pdf/1312.6034v2.pdf), minimization of this value may also occur via minimization of $O_{-n}$, ie minimimization of all outputs at indicies not equal to the target class index.

To avoid this difficulty, we will arrange our loss function such that we maximize the value of $O_n(a, \theta)$ where $O_n$ signifies the logit at index n.  A trivial way to maximize this value would be to simply maximize the values of all logits at once, and so to prevent this we use L1 regularization (althogh L2 or other regularizers should work well too).  We can either regularize with respect to the activations of the final layer, or else with respect to the input directly.  As the model's parameters $\theta$ do not change during this gradient descent, these approaches are equivalent and so we take the latter.

The objective function we will minimize is therefore

$$
J'(a) = J(O_n(a, \theta)) + \sum_i \vert a_i \vert
$$

and for our primary loss function we also have a number of possible choices. Here we will take the L1 metric to some large constant $C$

$$
J'(a) = (C - O_n(a, \theta)) + \sum_i \vert a_i \vert
$$

and this may be implemented using pytorch as follows:

```python
def layer_gradient(model, input_tensor, true_output):
	...
	input_tensor.requires_grad = True
	output = model(input_tensor)
	loss = torch.abs(200 - output[0][int(true_output)]) + 0.001 * torch.abs(input_tensor).sum() # maximize output val and minimize L1 norm of the input
	loss.backward()
	gradient = input_tensor.grad
	return gradient
```

Note that we are expecting the model to give logits as outputs rather than the softmax values.  The pytorch version of InceptionV3 does so automatically, which means that we can simply use the output directly without having to use a hook to find the appropriate values.

In a related vein, we will also change our starting tensor allow for more stable gradients, leading to a more stable loss as well. Instead of using a uniform random distribution ${x: x \in (0, 1)}$ alone, the random uniform distribution is scaled down by a factor of 25 and centerd at $1/2$ as follows:

```python
single_input = (torch.rand(1, 3, 256, 256))/25 + 0.5 
```

Now we can use our trained `model` combined with the `layer_gradient` to retrieve the gradient of our logit loss with respect to the input, and modify the input to reduce this loss.  

```python
def generate_input(model, input_tensors, output_tensors, index, count):
	...
	class_index = 292
	single_input = (torch.rand(1, 3, 256, 256))/25 + 0.5 # scaled normal distribution initialization
 
	single_input = single_input.to(device)
	single_input = single_input.reshape(1, 3, 256, 256)
	original_input = torch.clone(single_input).reshape(3, 256, 256).permute(1, 2, 0).cpu().detach().numpy()
	target_output = torch.tensor([class_index], dtype=int)

	for i in range(100):
		single_input = single_input.detach() # remove the gradient for the input (if present)
		predicted_output = model(single_input)
		input_grad = layer_gradient(model, single_input, target_output) # compute input gradient
		single_input = single_input - 0.15 * input_grad # gradient descent step
```

It should be noted that this input generation process is fairly tricky: challenges include unstable gradients resulting learning rates (here `0.15`) being too high or initial inputs not being scaled correctly, or else the learning rate not being matchd with the number of iterations being performed.  Features like learning rate decay and gradient normalization were not found to result in substantial improvements to the resulting images.

For most ImageNet categories, the preceeding approach does not yield very recognizable images.  Features of a given category are often muddled together or dispered throughout the generated input.  Below is a typical result, in this case when 'ant' is chosen (`class_index = 310`).  The initial random image is transformed as the class activation (logit) increases for the appropriate index.  Logits for the model (InceptionV3) given the input to the left are displayed as a scatterplot, with the desired output in red.

{% include youtube.html id='x5ydF_bORFQ' %}

A minority of classes do have vaguely recognizable images generated: when we use the category 'washing machine' as our target, the glass covers of side-loading washing machines are represented as small round objects in distorted rectangles.

![washer]({{https://blbadger.github.io}}/neural_networks/generated_washer.png)

The second Bayesian prior we will enforce is that images will not be too variable from one pixel to the next.  Reducing variability between adjacent pixels increases their correlation correlation with each other (see here for an account on pixel correlation [here](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)) such that that neighboring pixels are constrained to resemble each other, in effect smoothing out an image.  One way to reduce variability between nearby pixels is to perform a convolution, the same operation which our model uses judiciously in neural network form for image classification.  For an introduction to convolutions in the context of deep learning, see [this page](https://blbadger.github.io/neural-networks.html#convolutions-explained)

One choice for convolution is to use a Gaussian kernal with a 3x3 size.  The Gaussian distribution has a number of interesting properties, and arguably introduces the least amount of information in its assumption.  A Gaussian distribution in two dimensions $x, y$ is as follows:

$$
G(x, y) = \frac{1}{2 \pi \sigma^2} \mathrm{exp} \left( \frac{x^2 + y^2}{2 \sigma^2} \right)
$$

where $\sigma$ is the standard deviation, which we can specify.  Using the functional Gaussian blur module of `torchvision`, the default value for a 3x3 kernal is $\sigma=0.8$ such that the kernal we will use is to a reasonable approximation

$$
\omega = 
9/4
\begin{bmatrix}
1/4 & 1/2 & 1/4 \\
1/2 & 1 & 1/2 \\
1/4 & 1/2 & 1/4 \\
\end{bmatrix}
$$

where the $1/2$ and $1/4$ values are more precisely just over $201/400$ and $101/400$.  

Here we apply a Gaussian convolution to the input in a curriculum in which our convolution is applied at every iteration until three quarters the way through gradient descent, at which time it is no longer applied for the remaining iterations.  Heuristically this should a general shape to form before details are filled in.


```python
def generate_input(model, input_tensors, output_tensors, index, count):
	max_iterations = 100
	for i in range(max_iterations):
		...
		if i < (max_iterations - max_iterations/4):
			single_input = torchvision.transforms.functional.gaussian_blur(single_input, 3)
```

Choosing to optimize for a classification of 'ant' (`class_index = 310`) once again, we see that this smoothness prior leads to a more unstable class prediction but a more realistic generated image of an ant.

{% include youtube.html id='5oOgiRQfDyQ' %}

Smoothness along leads to a number of more recognizable images being formed,

![convolved array]({{https://blbadger.github.io}}/neural_networks/generated_blurred.png)

Observe that these images are somewhat dim.  This results from the presence of spikes in pixel intensity for very small regions, which the smoothing process does not completely prevent.  The rest of the images using the smoothness prior alone (but no others) displayed below have been adjusted for brightness and contrast for clarity. 

Depending on the initial input given to the model, different outputs will form.  For a target class of 'Tractor', different initial inputs give images that tend to show different aspect of a tractor: sometimes the distinctive tyre tread, sometimes the side of the wheel, sometimes the smokestack is visible.

![convolved tractor]({{https://blbadger.github.io}}/neural_networks/generated_tractor_array.png)

This is perhaps in part the result of the ImageNet training set containing images where not the entire tractor is visible at once.  This appears to be a typical result for this image generation technique: if we optimize for 'Badger', we usually see the distinctive face pattern but in a variety of orientations

![convolved badgers]({{https://blbadger.github.io}}/neural_networks/generated_badger_array.png)

Here for 'Ant' we have

![convolved ant]({{https://blbadger.github.io}}/neural_networks/generated_ant.png)

although for other classes, few perspectives are reached: observe that for 'Keyboard' the geometry of the keys but not the letters on the keys are consistently generated.

![convolved keyboard]({{https://blbadger.github.io}}/neural_networks/generated_keyboard.png)

### Generating Images using Transformation Resiliency

Next we will add transformational resiliency.  The idea here is that we want to generate images that the model does not classify very differently if a small transformation is applied.  This transformation could be a slight change in color, a change in image resolution, a translation or rotation, among other possibilities.  Along with a Gaussian convolution, we also apply to the first three quarters of all images one of five re-sizing transformations.

Re-sizing may be accomplished using the `torch.nn.functional.interpolate()` module, which defaults to a interpolation mode of nearest neighbors.  This is identical to the k-nearest neighbors algorithm with a value of $k=1$ fixed.  For images, the value of the center of a pixel is taken to be the value across the area of the entire pixel such that the new pixel's center always lies inside one pixel or another (or on a border).  To make this clearaer, suppose we were down-sampling an image by a factor of around 1.5. For the new pixel centered on a red dot for clarity, there is

![interpolation explanation]({{https://blbadger.github.io}}/neural_networks/interpolation_explanation2.png)

In addition, a small intensity change is applied to each pixel at random for each iteration using `torchvision.transforms.ColorJitter(c)` where `c` is a value of choice.  Specifically, $\epsilon \in [-c, c]$ is added to element $a_{x, y}$ of input $a$ to make element $a_{x, y}'= a{x, y} + \epsilon$ of a transformed input $a'$.  In the code sample below, we assign $\epsilon \in [0.0001,0.0001]$ but this choice is somewhat arbitrary.  Note that this transformation may also be undertaked with much larger values (empirically up to around $\epsilon = 0.05$) and for color, contrast, and saturation as well as brightness by modifying the arguments to `torchvision.transforms.ColorJitter()`.

One note of warning: `torchvision.transforms.ColorJitter()` is a difficult method for which to set a deterministic seed. `random.seen()`, `torch.set_seed()`, and `np.seed()` together are not sufficient to make the color jitter deterministic, so it is not clear how exact reproduction would occur using this method. If a deterministic reproduction is desired, it may be best to avoid using this module.

```python
def generate_input(model, input_tensors, output_tensors, index, count):
	max_iterations = 100
	for i in range(max_iterations):
		...
		single_input = torchvision.transforms.ColorJitter(0.0001)(single_input)
		if i < 76:
			...
			if i % 5 == 0:
				single_input = torch.nn.functional.interpolate(single_input, 256)
			elif i % 5 == 1:
				single_input = torch.nn.functional.interpolate(single_input, 198)
			elif i % 5 == 2:
				single_input = torch.nn.functional.interpolate(single_input, 160)
			elif i % 5 == 3:
				single_input = torch.nn.functional.interpolate(single_input, 180)
			elif i % 5 == 4:
				single_input = torch.nn.functional.interpolate(single_input, 200)
```

The class label (and input gradient) tends to be fairly unstable during training, but the resulting images can be fairly recognizable: observe the lion's chin and mane appear in the upper left hand corner during input modification.

{% include youtube.html id='yp9axdNcCG8' %}

Note that much of the fluctuation in the class activation scatterplot to the right is due to the change in input image resolution, which changes the activation value for each class even if it does not change the relative activation values of the target class compared with the others.

There is a noticeable translation in the image from the top left to the bottom right during the initial 75 iterations with the function above.  This is the result of using nearest neighbor interpolation while resampling using `torch.nn.functional.interpolate`.  It appears that in the case where multiple pixels are equidistant from the target, the upper left-most pixel is chosen as the nearest neighbor.  When downsampling an input, we are guaranteed to have four pixels that are nearest neighbors to the new pixels.

This can be remedied by choosing an interpolation method that averages over pixels rather than picking only one pixel value.  The following diagram illustrates the upper-left nearest neighbor interpolation method compared to a bilinear interpolation. Note how the upper left pixel becomes centered down and to the right of its original center for nearest neighbor interpolation, whereas in contrast the bilinear interpolation simply averages all pixels together and does not 'move' the top left value to the center.

![interpolation explanation]({{https://blbadger.github.io}}/neural_networks/interpolation_explanation.png)

We therefore have a method that effectively translates as well as resizes our original images, which is applied in addition to the Gaussian convolution and jitter mentioned above. For around half of our 16 random input we get a recognizable lion face and mane, and for others a paw is found

![generated lion]({{https://blbadger.github.io}}/neural_networks/generated_multiscalejitter_lion.png)

We can compare the effects of adding transformations to the modification process to the resulting image.  Here the convolved (such that smoothness is enforced) images are not adjusted for brightness and contrast as they were above.  The figures below demonstrates ho how re-sizing during input modification effectively prevents spikes in pixel intensity.  For a soccer ball and a tennis ball, we have

![generated soccerballs]({{https://blbadger.github.io}}/neural_networks/generated_soccerball_comparison.png)

![generated tennisballs]({{https://blbadger.github.io}}/neural_networks/generated_tennisball_comparison.png)

Some inputs are now unmistakeable, such as this badger's profile

![generated badger]({{https://blbadger.github.io}}/neural_networks/generated_singlebadger.png)

or 'Chainmail'

![generated badger]({{https://blbadger.github.io}}/neural_networks/generated_chainmail_multiscalejitter.png)

Note, however, that transformational invariance does not necessarily lead to a noticeable increase in recognizability for all class types.

### Image Transfiguration

It is worth appreciating exactly what we were able to do in the last section.  Using a deep learning model trained for image classification combined with a few general principles of how natural images should look, we were able to reconstruct a variety of recognizable images representing various desired classes.  

This is remarkable because the model in question (InceptionV3) was not designed or trained to generate anything at all, merely to make accurate classifications.  Moreover, the initial input being used as a baseline for image generation (a scaled uniform or normal random distribution) is quite different from anything that exists in the training dataset Imagenet, as these are all real images.  What happens if we start with a real image and then apply our input gradient descent method to that?  This process will be termed 'transfiguration' on this page to avoid confusion with 'transformation', which is reserved for the jitters, interpolations, rotations, and translations applied to an input.

To begin with, it may be illuminating to perform a control experiment in which the input is the same as the targeted class.  In this case we would expect to simply see an exaggeration of the features that distinguish the object of the class compared to other classes.  Applying our transformation-resistatant and Gaussian-convolved input gradient method to images of dalmations that are not found in the original Imagenet training dataset, we have

![transfigured dalmatian]({{https://blbadger.github.io}}/neural_networks/transformed_dalmatian_dalmatian.png)

The dalmatian's spots are slightly exaggerated, but aside from some general lack of resolution the dalmatians are still clearly visible. Now let's make a relatively small transfiguration from one breed of dog to another.  Beginning again with images of dalmatians but this time performing the input gradient procedure with a target class of 'Siberian Husky' we have

![transfigured dalmatian]({{https://blbadger.github.io}}/neural_networks/transformed_dalmatian_husky.png)

The spots have all but disappeared, replaced by thicker fur and the grey stripes typical of Huskies.  Note how even smaller detailes are changed: in the bottom right, note how the iris color changes from dark brown to light blue, another common Husky characteristic.

Transforming an input from one breed of dog to another may not seem difficult, but the input gradient procedure is capable of some very impressive changes.  Here we begin with images of flowers and target the 'Castle' class

![transfigured flowers]({{https://blbadger.github.io}}/neural_networks/transformed_flowers_castle.png)

We can get an idea of how this process works by observing the changes made to the original image as gradient descent occurs.  Here over the 100 iterations of transfiguration from a flower ('flowerpot' class in green on the scatterplot in the right) to a 'Castle' target class (the red dot on the right)

{% include youtube.html id='Q1BOOJnY9iM' %}

Even with as substantial a change as a flower to a castle some outputs are unmistakable, such as this castle tower.

![transfigured flowers]({{https://blbadger.github.io}}/neural_networks/single_castle.png)

Other transfigurations are possible, such as this badger from a rose bush

![transformed flowers]({{https://blbadger.github.io}}/neural_networks/flower_badger_single.png)

or this tulip bed into a 'Tractor'

![transformed flowers]({{https://blbadger.github.io}}/neural_networks/rose_into_tractor2.png)

or these flowers transfigured into 'Soccer ball'.

![transformed flowers]({{https://blbadger.github.io}}/neural_networks/transformed_flowers_soccerball.png)

Earlier it was noted that image resizing with `torch.nn.functional.interpolate()` leads to translation when downsampling the input. This can be avoided by switching the interpolation mode away from nearest neighbor, to instead average either using a bilinear or bicubic method.  This can be done in `torch.nn.functional.interpolate()` by specifying the correct keyword argument, or else we can make use of the module `torchvision.transforms.Resize()`, which defaults to bilinear mode.  The latter is the same method we used to import our images, and was employed below.  Notice how there is now no more translation in the input image.

![transfigured flowers]({{https://blbadger.github.io}}/neural_networks/transformed_flowers_strawberry.png)

### Input Generation with Auxiliary Outputs

We have seen how representatives of each image class of the training dataset may be generated using gradient descent on the input with the addition of a few reasonable priors, and how this procedure is also capable of transforming images of one class to another.  Generation of an image matching a specific class requires an output layer trained to perform this task, and for most models this means that we are limited to one possible layer.  But InceptionV3 is a somewhat unique architecture in that it has another output layer, called the auxiliary output, which is employed during training to stabilize gradients and then deactivated during evaluation with `model.eval()` if one is using Pytorch.

Let's investigate whether we can perform gradient descent to generate images using this auxiliary output rather than the usual output layer.  The architecture we want to use is

![Inception Architecture]({{https://blbadger.github.io}}/neural_networks/inception_aux.png)

Note that the Pytorch implementation of the above architecture does not include the softmax layer on either the auxiliary or main output.

It may be worth exploring some history behind how different deep learning libraries perform differentiation. The original Torch library employed symbol-to-number differentiation, which was also employed by Caffe.  In this approach, the derivatives (in the form of Jacobian matricies) of each layer are specified at the start of runtime, and computed to form numerical values for each node upon forward and backpropegation.  This approach is in contrast to symbolic differentiation, in which extra nodes are added to each model node (ie layer) that provides a symbolic description of the derivatives of those nodes.  The computational graph for performing backpropegation is identical between these two approaches, but the former hides the graph itself and the latter exposes it at the expense of extra memory.  The latter approach was taken by libraries like Theano and Tensorflow.

To this author's knowledge, however, these differences have blurred with the more modern incarnations of Tensorflow and Pytorch.  Pytorch now uses a method called automatic differentiation, which attempts to combine the speed of numerical differentiation with the stability and flexibility of symbolic differentiation. 

We wish to back-propegate from a leaf node of our model, but this node is not returned as an output during evaluation mode. Switching to training mode is not an option because the batch normalization layers of the model will attain new parameters, interfering with the model's ability to classify images.  

In symbolic differentiation -based libraries, computing the gradient of the input with respect to a layer that is not the output is relatively straightforward: the output and loss from the specific layer of interest are specified, and backpropegation can then proceed directly from that layer. But in Pytorch this is not possible, as all we would have from an internal node is a number representing the gradient with respect to the output, rather than instruction for obtaining a gradient from that node itself.

So instead we must modify the model itself.  If we were not using an open-source model this would not be feasible, but as we are indeed using a freely accessible model we can do this a number of different ways.  One way is to make a new class that inherits from `nn.Module` and is passed as an initialization argument the original Inceptionv3 model.  We then provide a new `forward()` method that modifies the original inceptionv3 method (located [here](https://github.com/pytorch/vision/blob/main/torchvision/models/inception.py)) such that the layer of choice is returned.

```python
class NewModel(nn.Module):

	def __init__(self, model):
		super().__init__()
		self.model = model

	def forward(self, x):
		# N x 3 x 299 x 299
		x = self.model.Conv2d_1a_3x3(x)
		# N x 32 x 149 x 149
		x = self.model.Conv2d_2a_3x3(x)
		# N x 32 x 147 x 147
		x = self.model.Conv2d_2b_3x3(x)
		# N x 64 x 147 x 147
		x = self.model.maxpool1(x)
		# N x 64 x 73 x 73
		x = self.model.Conv2d_3b_1x1(x)
		# N x 80 x 73 x 73
		x = self.model.Conv2d_4a_3x3(x)
		# N x 192 x 71 x 71
		x = self.model.maxpool2(x)
		# N x 192 x 35 x 35
		x = self.model.Mixed_5b(x)
		# N x 256 x 35 x 35
		x = self.model.Mixed_5c(x)
		# N x 288 x 35 x 35
		x = self.model.Mixed_5d(x)
		# N x 288 x 35 x 35
		x = self.model.Mixed_6a(x)
		# N x 768 x 17 x 17
		x = self.model.Mixed_6b(x)
		# N x 768 x 17 x 17
		x = self.model.Mixed_6c(x)
		# N x 768 x 17 x 17
		x = self.model.Mixed_6d(x)
		# N x 768 x 17 x 17
		x = self.model.Mixed_6e(x)
		# N x 768 x 17 x 17
		aux = self.model.AuxLogits(x)
		return aux
```

One important thing to note is that the Inceptionv3 module is quite flexible with regards to the input image size when used normally in evaluation mode, but now that we have modified the network it must be given images of size 299x299 or the dimensions of the hidden layers will not align with what is required.  This can be enforced by something as straightforward as

```python
image = torchvision.transforms.Resize(size=[299, 299])(image)
```
in the `__getitem__()` special method of our data loader (see the source code for more information). 

The gradients in this layer also tend to be slightly smaller than the original output gradients, so some re-scaling of our gradient descent constant $\epsilon$ is necessary. For clarity, at each iteration of our descent starting with input $a_n$, we make input $a_{n+1}$ by finding the L1-normalized loss of the auxiliary output layer $O'(a; \theta)$ with respect to the desired output $\widehat{y}$,

$$
a_{n+1} = a_n - \epsilon \nabla_a J(O'(a; \theta), \widehat{y})
$$

The results are interesting: perhaps slightly clearer (ie higher resolution) than when we used the normal output layer, but maybe less organized as well.

![Inception Architecture]({{https://blbadger.github.io}}/neural_networks/auxiliary_flowers_array.png)

### Jitter using Cropped Octaves

The generated images shown so far on this page exhibit to some extent or another the presence of high-frequency patterns, which can be deleterious to the ability to make an image that is accurate to a real-world example.  High frequency between image pixels often appears as areas of bright dots set near each other, or else sometimes as dark lines that seem to overlay light regions.  The wavy, almost ripple-like appearance of some of the images above appears to be the result of application of smoothing (via Gaussian kernal convolution or re-sizing) to the often chaotic and high-frequency gradient applied to the images during generation.

One way to address the problem of high frequency and apparent chaos in the input gradient during image generation is to apply the gradient at different scales. This idea was pioneered in the context of feature visualization by Mordvintsev and colleages and published in the [Deep dream](https://www.tensorflow.org/tutorials/generative/deepdream) tutorial, and is conceptually fairly straightforward: one can observe that generated images (of both features or target classes) have small-scale patterns and large-scale patterns, and often these scales do not properly interact.  When a lack of interaction occurs, the result is smaller versions of something that is found elsewhere in the images, but in a place that reduces image clarity.  The idea is to apply the gradient to different scales of the input image to address this issue.

One octave interval is very much like the gradient descent with Gaussian convolution detalied earlier. The main idea compared to the previous technique of image resizing is that the image is up-sampled rather than down-sampled for higher resolution, and that this up-sampling occurs in discrete intervals rather than at each iteration.  Another difference is that instead of maintaining a constant learning rate $\epsilon$ and Gaussian convolution standard deviation before removing the convolution, both are gradually decreased as iterations increase.

There are a number of different ways that octave-based gradient descent may be applied, but here we choose to have the option to perform gradient descent on a cropped copy of the input image, and apply a Gaussian convolution to the entire image at each step (rather than only to the portion that was cropped and modified via gradient descent).  This approach is similarto a positional jitter method that has been used elsewhere, but incorperates an increase in scale at specific stages of the positional jitter.

```python
def octave(single_input, target_output, iterations, learning_rates, sigmas, size, crop=True):
	...
	start_lr, end_lr = learning_rates
	start_sigma, end_sigma = sigmas

	for i in range(iterations):
		if crop:
			cropped_input, crop_height, crop_width = random_crop(single_input.detach(), size)
		else:
			cropped_input, crop_height, crop_width = random_crop(single_input.detach(), len(single_input[0][0]))
			size = len(single_input[0][0])
		single_input = single_input.detach() # remove the gradient for the input (if present)
		input_grad = layer_gradient(Inception, cropped_input, target_output) # compute input gradient
		single_input -= (start_lr*(iterations-i)/iterations + end_lr*i/iterations)*input_grad # gradient descent step
		single_input = torchvision.transforms.functional.gaussian_blur(single_input, 3, sigma=(start_sigma*(iterations-i)/iterations + end_sigma*i/iterations))

	return single_input
```

Usually multiple octaves are performed during gradient descent, and this can be achieved by chaining the previous function to resized inputs. Here we have three octaves in total, bringing an initial image of size 299x299 to an output of size 390x390.  
```python
# 1x3x299x299 input
single_input = octave(single_input, target_output, 100, [1.5, 1], [2.4, 0.8], 0, crop=False)

single_input = torchvision.transforms.Resize([340, 340])(single_input)
single_input = octave(single_input, target_output, 100, [1.3, 0.45], [1.5, 0.4], 320, crop=True)

single_input = torchvision.transforms.Resize([390, 390])(single_input)
single_input = octave(single_input, target_output, 100, [1.3, 0.45], [1.5, 0.4], 390, crop=False)
```

The results show some increased clarity, but also that images remain somewhat noisy with respect to the colors and textures that are formed. 

![Inception output]({{https://blbadger.github.io}}/neural_networks/octaves_inception3_test.png)

Note the light grey border on both images above.  This is the result of performing Gaussian blurring on the entire image whilst only performing a gradient-based update on a cropped portion, meaning that the areas not included in the crop are blurred but not updated and tend to wash out.  These borders may be cropped from the final image or else we may only blur the areas that were updated as follows:

```python
single_input[:, :, crop_height:crop_height+size, crop_width:crop_width+size] -= (start_lr*(iterations-i)/iterations + end_lr*i/iterations)*input_grad 
```

To see how using cropped portions of octaves implements a positional jitter, consider two consecutive updates to an image $a$.  The model (InceptionV3 in this case) receives inputs that are small translations of each other (along with the addition of a gradient descent step) such that the gradient $g$ of each input with respect to the output class that is being optimized does not depend on the exact position of each pixel value, only the relative (and approximate) position. 

Note that the size of the cropped portion of $a$ that is fed to the model has undetermined dimensions $h$ and $w$. Some model architectures require an input of a fixed size, while others are more flexible.  Implementations of models used for experimentation on this page are generally flexible, mostly due to their convolutional and pooling layers. Increasing the $h$ and $w$ resolution of the portion of the image that is used as an input to the model allows for the application of gradients at smaller scales, leading to a more detailed image at the expense of some coherence between portions of the imag

![Inception output]({{https://blbadger.github.io}}/neural_networks/cropped_octave_explanation.png)

Further optimizing for a coherent input with two cropped octave-based jitters over 720 iterations,

```python
single_input = octave(single_input, target_output, 220, [6, 5], [2.4, 0.8], 0, pad=False, crop=False)

single_input = torchvision.transforms.Resize([490, 490])(single_input)
single_input = octave(single_input, target_output, 500, [1.3, 0.45], [1.5, 0.4], 470, pad=False, crop=True) 
```

gives 

![Inception output]({{https://blbadger.github.io}}/neural_networks/octaves_inception3_test2.png)

### GoogleNet Image Generation

In images of inputs generated using gradient descent published by [Mordvintsev and colleagues](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html), generated images are generally better-formed and much less noisy than they appear on this page.  ygard [investigated](https://www.auduno.com/2015/07/29/visualizing-googlenet-classes/) possible methods by which these images were obtained (as the previous group did not publish their image generation code) and found a way to produce more or less equivalent images and authored a helpful [Github repo](https://github.com/auduno/deepdraw) with the programs responsible.

Both referenced studies have observed images that are in general clearer than most of the images on this page.  ygard adapted a method modified from an approach in the Tensorflow deep dream [tutorial](https://www.tensorflow.org/tutorials/generative/deepdream) octave method, which resulted in impressively clear images.

A wide array of possible modifications to the octave gradient descent method were attempted with little improvement on the clarity displayed in the last section.  This led to the idea that perhaps it is the model itself rather than the optimization method that was responsible for the increased noise relative to what other researchers have found.

The cited works in this section all used the original GoogleNet architecture to generate images. whereas this page has so far focused on the related InceptionV3 model.  GoogleNet may be depicted as follows:

![GoogleNet architecture]({{https://blbadger.github.io}}/neural_networks/googlenet_architecture.png)

It may be appreciated that this network is a good deal shallower than InceptionV3 (22 versus 48 layers, respectively) but otherwise contains a number of the same general features.  One notable change from the original is that the Pytorch version of GoogleNet uses Batch normalization, whereas the original did not. 

Optimizing for the 'Stop Light' ImageNet class, we now have a much more coherent image in terms of color and patterns over two octaves.

{% include youtube.html id='QEJeSm9xNa8' %}

Observing more of the images, we find that our octave-based input generation technique is effective for birds

![birds]({{https://blbadger.github.io}}/neural_networks/googlenet_birds.png)

as well as dogs and other animals.

![birds]({{https://blbadger.github.io}}/neural_networks/googlenet_animal.png)

Flames and fires are particularly accurate as well.

![flames]({{https://blbadger.github.io}}/neural_networks/googlenet_flames.png)

Note how the image generated for 'stove' has much larger flames than are realistic.  This kind of exaggeration is seen for a number of objects in which there is some differentiating features.  On the other hand, the Imagenet dataset has no classes that differentiate faces, and observe how the faces for 'neck brace' below are animal-like, not a surprise being that Googlenet will have seen far more dog and other animal faces than human ones.

![misc]({{https://blbadger.github.io}}/neural_networks/googlenet_highlights.png)

Also observe the fine point accuracy in some of the these images: we even find a sine wave forming in the image of 'oscilloscope', and the 'bolo tie' even has a silver fastener at the end of the tie. 

As reported previously, some many of an object contain peripheral features: observe how optimizing for a 'barbell' output includes an arm to pick up the barbell.

![misc]({{https://blbadger.github.io}}/neural_networks/googlenet_barbell.png)

For generated images of the entire suite of ImageNet classes, see [here](https://drive.google.com/drive/folders/1TrOa6sXWG8WVPhKQRYzG4lJVvwBPJ_iR?usp=sharing).

### Padding in the first octave

For certain ImageNet classes, generated images tend to have most of their relevant features focused on the periphery, as that is where the gradient is largest.  For example, optimizing for class 19 (chickadee) gives an image in which images of the bird of interest are well-formed but entirely near the image perimeter. It is interesting that certain classes tend to exhibit this phenomenon while others almost never do, and why gradient descent would lead to such a pattern is not clear.  Nevertheless, this can be effectively prevented using the following code:

```python
def octave(single_input, target_output, iterations, learning_rates, sigmas, size, crop=True):
	...
		if pad:
			single_input = torchvision.transforms.Pad([1, 1], fill=0.7)(single_input)
```

Adding a padding step to the first octave is most effective at preventing the peripheral gradient problem, and does not noticeably reduce the image quality.

![chickadee]({{https://blbadger.github.io}}/neural_networks/googlenet_chickadees.png)

{% include youtube.html id='Asl-hV8P1wA' %}

Comparing the images obtained using Googlenet compared to InceptionV3 (above), we find more coherent images for familiar objects.

![comparisons]({{https://blbadger.github.io}}/neural_networks/googlenet_comparisons.png)

### ResNet Image Generation

GoogleNet and InceptionV3 are fairly closely related model architectures, with the latter being designed as an improvement on the former in terms of ImageNet classification accuracy (although InceptionV3 is not, as we have seen, an improvement with regards input generation).  Both models used heterogenous convolutional layers combined with multiple outputs in order to increase the trainable depth, or in other words to allow the gradient to propegate such that early layers as well as late layers were able to be updated sucessfully during training.

A different approach to the problem of gradient propegation in very deep networks was taken by the team behind [Resnet](https://arxiv.org/abs/1512.03385), an architecture named after its judicious use of residual connections.  See [this page](https://blbadger.github.io/feature-visualization.html#mapping-resnet-features) for more detail on what residual connections are and why they may be beneficial to a deep convolutional network.

We can generate inputs representative of each ImageNet class using this model as well, and the results are interesting: images are perhaps slightly more jumbled together compared to the inputs generated by GoogleNet, but the colors are much clearer and even appear somewhat exaggerated.

![resnet generated examples]({{https://blbadger.github.io}}/neural_networks/resnet_generated_examples.png)

Earlier on this page it was noted that while the Octave method is able to make images that are far beyond the 299x299 resolution of the ImageNet training samples, there is a limit to how high a resolution one can hope to achieve because applying the gradient at different scales leads to incoherent images.  This can be illustrated using the ResNet model fairly easily due to the high quality of the images this model generates. Observe how changing the last Octave to 750x750 (with a 720x720 crop size) while optimizing for 'Collie' leads to the appearance of the dog's face at widely different scales in the bottom right, which leads to a somewhat more confusing if higher resolution image.

![comparisons]({{https://blbadger.github.io}}/neural_networks/resnet_resolution_comparison.png)

### InceptionV3 and GoogleNet Compared

So far this page has documented a variety of different methods across two models that allow for the generation of a coherent (ie human-recognizable) input using gradient descent on an initial input of noise, with varying degrees of success.  Measuring image coherence is somewhat difficult, as our metric is inherently subjective to the observer's preference.  Nevertheless, the octave method used with Googlenet has been observed to yield the most recognizable images thus far.

It may be wondered whether or not gradient descent using the same octaves would lead to similarly coherent images if applied to the InceptionV3 model, being that the choice of parameters, learning rates, and regularizers has optimized extensively for clarity whereas the octave approach for InceptionV3 documented above was not.  Perhaps most importantly, more iterations were used and no L1 regularization was employed during gradient descent on the optimized octaves, as somewhat paradoxically L1 regularization on either the gradient or input led to an increase in noise.  Using the optimized octave approach, images generated for all 1000 ImageNet classes using [InceptionV3](https://drive.google.com/drive/folders/1dR7Mgd6LYz3MnFbY8FNhSbKgGFPIDU5F?usp=sharing) can be compared to the images produced using [GoogleNet](https://drive.google.com/drive/folders/1TrOa6sXWG8WVPhKQRYzG4lJVvwBPJ_iR?usp=sharing).

It is worth investigating why inputs generated using GoogleNet are more coherent than those generated using InceptionV3, which is somewhat surprising being that InceptionV3 tends to be more accurate on ImageNet-based image identification challenges.

As a first step in this investigation, we can observe the process of input generation.  Below are videos with each gradient descent iteration plotted over time, with a scatterplot of the activation values for each output class at that iteration on the right. First we have GoogleNet generating an input for target class 'Stoplight':

{% include youtube.html id='mZi42au8rtQ' %}

and now we have InceptionV3 generating an image for the same class, using the same optimization technique with octaves:

{% include youtube.html id='Xybz0He5MLs' %}

One can observe that the classification of InceptionV3 for its generated input is less stable from one iteration to the next than what is found for GoogleNet, particularly in the second octave.  Resnet60 appears to be similar to GoogleNet with regards to activation stability.

{% include youtube.html id='1NXcvVckrgA' %}









## Input Generation III: Model Similarity and Autoencoding Representations

### Model Similarity Vector Space

In [part I](https://blbadger.github.io/input-generation.html#inceptionv3-and-googlenet-compared), it was observed that different models generate slightly different representations of each ImageNet input class.  It may be wondered whether or not we can use the gradients of two models to make an input, perhaps by combining the gradients of a model with parameters $\theta_1$ and another with parameters $\theta_2$ to make a gradient $g'$

$$
g' = \nabla_a \left( d(C - O_i(a, \theta_1)) + e(C - O_i(a, \theta_2)) \right)
$$

where $d, e$ are constants used to scale the gradients of models $\theta_1, \theta_2$ appropriately.  For GoogleNet and Resnet, we approximate $d = 2, e = 1$ by applying the constant only to $C$ as follows (note that there is little difference in applying the constant to the gradient or the constant alone)

```python
def layer_gradient(model, input_tensor, desired_output):
    ...
    loss = 0.5*((200 - output[0][int(desired_output)]) + (400 - output2[0][int(desired_output)]))
    ...
```

where `output` is the (logit) output of ResNet and `output2` is the output from GoogleNet.  

![googlenet and resnet input]({{https://blbadger.github.io}}/neural_networks/combined_mousetrap.png)

Images of all 1000 ImageNet classes generated using the combined gradient of GoogleNet and ResNet are available [here](https://drive.google.com/drive/folders/1mhj8vBm02Fd6QkQwEQ4jhI9yq34ZOtR0?usp=sharing). From these images it is clear that the combined gradient is as good as or superior to the gradients from only ResNet or GoogleNet with respect to producing a coherent input image, which suggests that the gradients from these models are not substantially dissimilar.

The above observation motivates the following question: can we attempt to understand the differences between models by generating an image representing the difference between the output activations for any image $a$? We can construct a gradient similar to what was employed above, 

$$
g'' = \nabla_a \left( d(C - O_i(a, \theta_1)) - e(C - O_i(a, \theta_2)) \right)
$$

again with `output` and `output2` signifying the logit output from ResNet and GoogleNet, respectively.

```python
def layer_gradient(model, input_tensor, desired_output):
    ...
    loss = 2*((200 - output[0][int(desired_output)]) - (400 - output2[0][int(desired_output)]))
    ...
```

Which yields

![googlenet and resnet input]({{https://blbadger.github.io}}/neural_networks/resnet_minus_googlenet.png)

Note that the roll-over bars present in the depiction of go-karts by ResNet50 are absent for GoogleNet's representation of the same class, and that consequently the representation using $g''$ exaggerates this feature.  The same exaggeration of features not found in GoogleNet's but that are found in ResNet's representation of a french horn (mostly the figures playing the instruments) is also observed.

For other ImageNet classes, however, there does not tend to be a substantial difference between inputs generated for each class via applying the gradient $g$ of ResNet50 compared to $g''$.  It is likely that this is because minimizing the activation for some particular class does not yield enough information to generate an input in certain cases.  For an in-depth investigation into generating the inputs corresponding to the subtracted term of $g''$, see [this page](https://blbadger.github.io/latent_output.html#introduction-with-opposites).

It is also apparent that the similarities and differences in model output may be compared by viewing the output as a vector space.  Say two models were to give very similar outputs for a representation of one ImageNet class but different outputs for another class. The identities of the classes may help inform an understanding of the the difference between models.

### Model Merging

In the last section it was observed that we can understand some of the similarities and differences between models by viewing the output as a vector space, with each model's output on each ImageNet representation being a point in this space.

What if we want one model to generate a representation of an ImageNet class that is similar to another model's representation?  We have already seen that some models (GoogleNet and Resnet) generally yield recognizable input representations whereas others (InceptionV3) yield somewhat less-recognizable inputs.  But if we were stuck with only using InceptionV3 as our image-generation source, can we try to use some information present from the other models in order to generate a more-recognizalbe image?

One may hypothesize that it could be possible to train one model to become 'more like' another model using the output of the second as the target for the output of the first.  Consider one standard method of training using maximum likelihood estimation via minimization of cross-entropy betwen the true output $Q$ and the model output $Q$ given input $x$,

$$
J(O(a, \theta), \widehat y) = H(P, Q) = -\Bbb E_{x \sim P} \log Q(x)
$$

The true output $Q$ is usually denoted as a one-hot vector, ie for an image that belongs to the first ImageNet category we have $[1, 0, 0, 0...]$.  A cross-entropy loss function measures the similarity between the output distribution model output $Q$ and this one-hot tensor $Q$.  

But there are indications that this is not an optimal training loss.  Earlier on this page we have seen that for trained models, some ImageNet categories are more similar with respect to the output activations $O(a, \theta)$ to some other ImageNet categories, and different from others.  These similarities moreover are by a nearest neighbors graphical representation intuitive for a human observer, meaning that it is indeed likely that some inputs are more similar than others.

Training requires the separation of the distributions $P_1, P_2, ... P_n$ for each imagenet category $n$ in order to make accurate predictions. But if we have a trained model that has achieved sufficient separation, the information that the model has difficulty separating certain images from others (meaning that these are more similar ImageNet categories by our output metric) is likely to be useful information in training a model.  This information is not likely to be found prior to training, and thus is useful for transfer learning.


These observations motivate the hypothesis that we may be able to use the information present in the output of one model $\theta_1$ to train another model $\theta_2$ to be able to represent an input in a similar way to $\theta_1$.  More precisely, one can use gradient descent on the parameters of model 1, $\theta_1$ to make some metric between the outputs $m(O(a, \theta_1), O(a, \theta_2))$ as small as possible. In the following example, we seek to minimize  the sum-of-squares residual as our metric

$$
J(O(a, \theta_1) = \sum_i \left(O_i(a, \theta_1) - O_i(a, \theta_2) \right)^2 
$$

which can be implemented as

```python
def train(model, input_tensor, target_output):
    ...
    output = model(input_tensor)
    loss = torch.sum(torch.abs(output - target_output)**2) # sum-of-squares loss
    optimizer.zero_grad() # prevents gradients from adding between minibatches
    loss.backward()
    optimizer.step()
    return 
```
Note that more commonly-used metrics like $L^1$ or $L^2$ empirically do not lead to significantly different results as our RSS metric here.  Whichever objective function is chosen, it can be minimized by gradient descent on the model parameters given an input image $a$ is

$$
\theta_1'= \theta_1 + \epsilon * \nabla_{\theta_1} J(O(a, \theta_1))
$$

which may be implemented as

```python
def gradient_descent():
    optimizer = torch.optim.SGD(resnet.parameters(), lr=0.00001)
    # access image generated by GoogleNet
    for i, image in enumerate(images):
        break 
    image = image[0].reshape(1, 3, 299, 299).to(device)
    target_output = googlenet(image).detach().to(device)
    target_tensor = resnet(image) - target_output
    for _ in range(1000):
        train(resnet, image, target_output)
```

Gradient descent is fairly effective at reducing a chosen metric between ResNet $\theta_1$ and GoogleNet $\theta_2$.  For example, given $a$ to be GoogleNet's representation of Class 0 (Tench), $J(O(a, \theta_1))$ decreases by a factor of around 15.  But the results are less than impressive: applying gradient descent to ResNet's parameters as $\theta_1$ to reduce the sum-of-squares distance between this model's output and GoogleNet's output does not yield a model that is capable of accurately representing this class.

It may be hypothesized that this could be because although ResNet's outputs match GoogleNet's for this class, each class has a different 'meaning', ie latent space location, which would undoubtedly hinder our efforts here.  But even if we repeat this procedure to train ResNet's outputs match GoogleNet's(for that model's representations) for all 1000 ImageNet classes, we still do not get an accurate representation of Class 0 (or any other class of interest).

![resnet trained to be googlenet tench]({{https://blbadger.github.io}}/neural_networks/resnet_trained_to_be_googlenet.png)

It is certainly possible that this method would be much more successful if applied to natural images rather than generated representations.  But this would go somewhat against the spirit of this hypothesis because natural images would bring with them new information that may not exist in the models $\theta_1, \theta_2$ even if the images come from the ImageNet training set.

These results suggest that modifying $\theta_1$ to yield and output that matches that of $\theta_2$ is a somewhat difficult task, at least if we limit ourselves to no information that is not already present in the model.  But this could simply be due to known difficulties in model training rather than an inability to explore the output latent space.  

Therefore a more direct approach to making one model yield another model's representations: rather than modifying the first model's parameters $\theta_1$ via gradient descent, we instead find the coordinates of the second model's representation of a given class in our output space and then use these coordinates as a target for gradient descent on an initially randomized input $a_0$.

For clarity, this procedure starts with an image representing the representation of a certain class with respect to some model $\theta_2$, denoted $a_{\theta_2}$.  We want to have a different model $\theta_1$ learn how to generate an approximation of this representation with no outside information. First we find a target vector $\widehat{y} \in \Bbb R^n$, with $n$ being the number of possible outputs, defined as

$$
\widehat{y} = O(a_{\theta_2}, \theta_1) 
$$

or in words the target vector is the output of the model of interest $\theta_1$ when the input supplied is the representation of some class for the model we want to emulate.

Now rather than performing gradient descent using a target $C \in \Bbb R^n$ being a vector with a large constant in the appropriate index for our class of interest and zeros elsewhere, we instead try to reduce the distance between $O(a_0, \theta_1)$ and the target vector $\widehat{y}$. Again this distance can be any familiar metric, perhaps $L^1$ which makes the computation as follows:

$$
g = \nabla_a \sum_n \lvert \widehat{y_n} - O_n(a, \theta_1) \rvert \\
$$

with the input modification being a modified version of gradient descent using smoothness (ie pixel cross-correlation) via Gaussian convolution $\mathcal{N}$ and translational invariance with Octave-based jitter, here denoted $\mathscr{J}$, were employed with the gradient for input $a_n$, denoted $g_n$.  The actual update procedure is

$$
a_{n+1} =\mathscr{J} \left( \mathcal{N}(a_n + \epsilon g_n) \right)
$$

where the initial input $a_0$ is a scaled random normal distribution. This gradient can be implemented as follows:

```python
def layer_gradient(model, input_tensor, desired_output):
    ...
    input_tensor.requires_grad = True
    output = resnet(input_tensor)
    loss = 0.05*torch.sum(torch.abs(target_tensor - output))
    loss.backward()
    gradient = input_tensor.grad
```

If this method is successful, it would suggest that our model of interest $\theta_1$ has the capability to portray the representation that another model $\theta_2$ has of some output class, and that this portrayal may be made merely by finding the right point in the output space.  And indeed for Class 0 this is the case: observe how the output on the right mimicks GoogleNet's representation of a Tench.

![resnet vectorized to be googlenet tench]({{https://blbadger.github.io}}/neural_networks/resnet_vectorized_to_be_googlenet.png)

The ability of the right point in output space to mimick the representation by another model (for some given class) is even more dramatic when the model's representations of that class are noticeably different.  For example, observe the representation of 'Class 11: Goldfinch' by ResNet and GoogleNet in the images on the left and center below.  ResNet (more accurately) portrays this class using a yellow-and-black color scheme with a dark face whereas GoogleNet's portrayal has reddish feathers and no dark face.  But if we perform the above procedure to ResNet, it too mimicks the GoogleNet output.

![resnet vectorized to be googlenet goldfinch]({{https://blbadger.github.io}}/neural_networks/resnet_vectorized_to_be_googlenet_goldfinch.png)

Likewise, ResNet's depiction of a transverse flute contains flute players in addition to the instrument itself, whereas GoogleNet's depiction does not.  When we vectorize ResNet's output to match that of GoogleNet, we see that ResNet's depiction of a transverse flute no longer contains the players.

![resnet vectorized to be googlenet flute]({{https://blbadger.github.io}}/neural_networks/resnet_vectorized_to_be_googlenet_flute.png)

### Natural Image Representation

It may be wondered whether we can apply our gradient descent method to form representations of natural images, rather than generated ones representing some ImageNet category.  All that is required is to change $a_{\theta_2}$ to be some given target image $a_{t}$ before performing the same octave-based gradient descent procedure.  


When we choose two images of Dalmatians as our targets, we see that the representations are indeed accurate and that the features they portray are significant: observe how the top image's representation focuses on the body and legs (which are present in the input) whereas the bottom focuses on the face (body and legs not being present in the input).

![resnet vectorized to be googlenet goldfinch]({{https://blbadger.github.io}}/neural_networks/resnet_output_embedding_comparison.png)








## Irrational numbers on the real line

Two related paradoxes regarding real numbers are described, which imply a number of interesting properties about dynamical systems.

### Lines are sequences of points, but the real numbers are non-enumerable

This almost goes without saying, and is implicit in the definitions of rational and irrational numbers and lines.  A geometric line consists of an infinite number of points arranged in a sequence: $p_1$ may come after $p_2$ or before, but not both after and before.  Take a line formed from the rational numbers: all rationals may be enumerated in a list based on their 'height' (defined as the numberator plus denominator) and thus arranged along a line. This is Cantor's famous diagonal argument for the countability of rational numbers $ \Bbb Q$

$$
\Bbb Q = \{0, \; 1/1, \; 1/2, \; 2/1, \; 1/3, \; 2/2 ... \}
$$

The problem here is that real numbers are non-enumerable: real (specifically irrational) numbers are unable to be listed in a sequence because they are non-enumerable.  This means that any single list of real numbers will inevitably miss some because the reals are uncountably infinite.  Because of this, we cannot arrange all the real numbers on a line because doing so would imply that there is some way of arranging the real numbers in a list.  This means that if we try to construct a true real number line, we cannot include all real numbers.  As rational numbers are countably infinite but irrationals are uncountable (non-enumerable), in particular it is the set of irrational numbers that are too many to fit on any line. 

### Rationals are everywhere dense in the real line but are countable

Consider the properties of a geometric line, a one-dimensional object.  Any point on a line is bordered by at most two other points. To be precise, any point $x$ on the real number line $\Bbb R$ is arbitrarily close to at most two other points $y$ and $z$

$$
d(x, y) = 0 \\
d(y, z) = 0
$$

The real numbers $\Bbb R$ are described as existing on a line, also called the real number line $R^1$.  Now topologically the rational numbers $\Bbb Q$ are dense in the real numbers, meaning that the limit of every sequence of real numbers is a rational number.  In other words, the distance between any real number and a certain 'closest' rational number is 0.  Bearing in mind that points may be on one side or the other if arranged in a line, this means that rational numbers and irrational numbers must alternate in sequence along this line as follows:

$$
\Bbb R = \{... x \in \Bbb Q, \; y \in \Bbb I, \; z \in \Bbb Q ...\}
$$

because otherwise there would be nonzero distance between an abritrary rational and an irrational number in one direction.  The paradox here is that rational and irrational numbers cannot alternate because there are far more irrationals than rationals on the real number line, and alternating points necessitates equal set cardinality (even if they are both infinite).  If the points did alternate in this way, either rationals and irrationals must both be countably infinite or else both uncountably infinite. 

### Irrational numbers and dynamics

One way to try to resolve these paradoxes could be to invoke another spatial dimension for irrational numbers, akin to the extra dimension occupied by imaginary numbers.  But this leads to a problem: rationals are no longer dense in the real numbers, because there are irrational numbers far from the rational number axis that do not contain a rational between them.

Instead, consider the idea that irrational numbers exist in a separate temporal dimension from rational numbers, meaning that irrational numbers act as though they are in motion along the real line.  This resolves the first paradox because irrationals are not able to be listed in sequence because any sequence of these numbers contains non-comparable elements, and the second paradox is addressed because a single rational number is at any one time bordered by two irrationals, but 'over time' it is bordered by an arbitrary number (an infinite number to be precise) of irrationals.

In Cantor's diagonal argument, no countable number of numbers between 0 and 1 can contain all real numbers because a number $b$ can always be generated that differs from all previously generated numbers ${a_1, a_2, a_3...}$ in at least one decimal place:

$$
a_1 = 0.a_{10}a_{11}a_{12}a_{13}a_{14}a_{15}... \\
a_2 = 0.a_{20}a_{21}a_{22}a_{23}a_{24}a_{25}... \\
a_3 = 0.a_{30}a_{31}a_{32}a_{33}a_{34}a_{35}... \\
... \\
b = 0.b_{1} b_{2}  b_{3} ... \; such \; that \; b_1\neq a_{10}, b_2\neq a_{21}, b_3\neq a_{32} ... 
$$

Now imagine we are constructing the number $b$ (which must be irrational because if it were rational, it would be in a countable set and therefore equal to one or ${a_1, a_2, a_3 ...}$) using this very method.  We start with a sequence of 0s (or any arbitrary sequence of digits) and a line representing the rational numbers $(0, 1)$, and place a pin at the origin because our number is currently:

$$
b = 0.00000...
$$

Now the first digit $b_1$ is added, which we can arbitrarily say is a 5:

$$
b = 0.50000...
$$

This is so far a rational number: one half.  But we know (see above) that the completed $b$ cannot be rational so we are not done.  We move the pin to the number $1/2$ and say that this is our starting point.  Now for a second digit $b_2$:

$$
b = 0.52000...
$$

The pin has moved to a new rational number $52/100$ but we are not done.  From here it should be clear what happens next: a third number $b_3$ is chosen

$$
b = 0.52800...
$$

and the pin moves again to a new rational $528/1000$ but we are not done

$$
b = 0.52810...
$$

And in fact we are never done, because otherwise we could convert the resultant number into a fraction.  Now at each stage the pin has moved to a new rational number but as we never finish, the pin does not cease its movement but instead hops ceaselessly from rational to rational, never settling on any of them.

### Implications for dynamical systems: aperiodicity and irrationality

Continuous functions may be defined on the rational numbers whereas discontinuous functions may only be defined on irrationals.  Consider that aperiodic maps may be thought of as discontinuous functions (see [here](/most-discontinuous.md) for more), and can only be defined on irrationals.  Recall that in aperiodic maps, points are everywhere unstable such that arbitrarily close initial values diverge after finite number of iterations (or after finite time if the system is continuous).  The idea that irrationals continually move provides a clear explanation as to why aperiodic maps are everywhere unstable:  a function that is defined on numbers that continually diverge will necessarily be sensitive to small changes in initial value. 

In contrast, functions that are defined on the rational numbers would not be expected to be sensitive to initial conditions, because these numbers are stationary and do not exist in this separate temporal dimension.  

### Aperiodic dynamical systems as irreducible dynamical systems

Periodic maps may be described without time because they can be reduced to a periodic behavior with a remainder.  To see why this is the case, consider that time proceeds in a loop in periodic systems: if a system at $t=0$ is identical to the system at $t=20$ and $t=40$ then we may say that the system returns to its initial state every twenty seconds.  Equivalently, time proceeds in a loop that is twenty seconds long.  Any problem of determining future position is reduced to finding the remainder of the future time divided by twenty seconds.  Now say that you have observed and recorded the behavior of this periodic system for more than twenty seconds: the only task to do in order to determine the future position is to find the past position with the same remainder.  This can be done without calculating future values.

In contrast, most aperiodic maps are unable to be separated from linear time in this way because they never revisit a previous state (some can be, as for example the logistic map for $r=4$ has a closed form solution).  Now being that aperiodic (and not asymptotically periodic) maps are defined on the real numbers and not only on the rationals, we may think of the (mostly nonlinear) dynamical equations that may form aperiodic maps as 'irreducible' dynamical equations, whereas the linear dynamical equations that result in periodic maps can be reduced to closed form non-dynamical expressions.






















## Julia sets

A Julia (named after G. Julia) set is the boundary of the sets of unbounded and bounded iterates of the family of functions

$$
f_a(x) = x^2 + a 
\tag{1} \label{eq1}
$$

where $a$ is fixed and $x_0$ varies about the complex plane $x + yi$.  Different values of $a$ lead to different Julia sets, and together this family of functions $f_a(x) \forall a \in \Bbb C$ are the Julia sets.

This means that any number in the complex plane is in a Julia set if it borders another number $u$ such that iterations of \eqref{eq1} are unbounded

$f^k_a(u) \to \infty \; \mathbf {as} \; k \to \infty$ 

as well as a number $b$ where iterations of \eqref{eq1} are bounded

$f^k_a(c) \not\to \infty \; \mathbf {as} \; k \to \infty$

If we restrict ourselves to the real line, such that $a$ and $x$ are elements of $\Bbb R$, iterations of \eqref{eq1} have a number of interesting features.  Some values of $a$ form Cantor sets (fractal dusts), which may be expected as \eqref{eq1} is a nonlinear equation similar in form to the logistic and Henon maps (see [here](https://blbadger.github.io/logistic-map.html)).   

### Plotting Julia sets with Python

What happens when we allow $a$ and iterates of \eqref{eq1} to range over the complex plane?  Let's find out! To start with, import the indespensable libraries numpy and matplotlib

```python
#! python3
import numpy as np 
import matplotlib.pyplot as plt 
```

and then define a function to find the values of a Julia set for a certain value of $a$, with a docstring specifying the inputs and outputs of the function.  To view a Julia set, we want an array corresponding to the complex plane with values that border diverging and non-diverging values specially denoted.  One way to do this is to count the number of iterations of \eqref{eq1} any given point in the complex plane takes to start heading towards infinity, and if after a sufficiently large number of iterations the point is still bounded then we can assume that the value will not diverge in the future. 

To make an array of complex numbers, the  class in numpy is especially helpful. We can specify the range of the x (real) and y (imaginary) planes using `ogrid` (special thanks to the numpy tutorial [here](https://numpy.org/devdocs/user/quickstart.html) for this idea), remembering that imaginary values are specified with `j` in python.  The array corresponding to the complex plane section is stored as `z_array`, and the value of $a$ is specified (the programs presented here are designed with a value of $ \lvert a \rvert < 2 $ in mind, but can be modified for larger $a$s).  This will allow us to keep track of the values of subsequent iterations of \eqref{eq1} for each point $z$ in the complex plane.  

Also essential is initialization of the array corresponding to the number of iterations until divergence, which will eventually form our picture of the Julia set.

```python
def julia_set(h_range, w_range, max_iterations):
	''' A function to determine the values of the Julia set. Takes
	an array size specified by h_range and w_range, in pixels, along
	with the number of maximum iterations to try.  Returns an array with 
	the number of the last bounded iteration at each array value.
	'''
	# top left to bottom right
	y, x = np.ogrid[1.4: -1.4: h_range*1j, -1.4: 1.4: w_range*1j]
	z_array = x + y*1j
  	a = -0.744 + 0.148j
	iterations_till_divergence = max_iterations + np.zeros(z_array.shape)
  
```

To find the number of iterations until divergence of each point in our array of complex numbers, we can simply loop through the array `z_array` such that each point in the array is 

```python
	for h in range(h_range):
		for w in range(w_range):

```

It can be shown that values where $\lvert a \rvert > 2$ and $\lvert z \rvert > \lvert a \rvert$, future iterations of \eqref{eq1} inevitably head towards positive or negative infinity. 

This makes it simple to find the number of iterations `i` until divergence: all we have to do is to keep iterating \eqref{eq1} until either the resulting value has a magnitude greater than 2 (as $z$ is complex, we can calculate its magnitude by multiplying $z$ by its conjugate $z^* $ and seeing if this number is greater than $2^2 = 4$.  If so, then we know the number of iterations taken until divergence and we assign this number to the 'iterations_till_divergence' array a the correct index. 

```python
			z = z_array[h][w]
			for i in range(max_iterations):
				z = z**2 + a
				if z * np.conj(z) > 4:
					iterations_till_divergence[h][w] = i
					break

	return iterations_till_divergence
```

This will give us an iteration number for each point in `z_array`.  What if \eqref{eq1} does not diverge for any given value? The final array `iterations_till_divergence` is initialized with the maximum number of iterations everywhere, such that if this value is not replaced then it remains after the loops, which signifies that the maximum number of iterations performed was not enough to cause the value to diverge.  Outside all these loops, we return the array of iterations taken.

Now we can plot the `iterations_till_divergence` array!  It is a list of lists of numbers between 0 and 'max_iterations', so we can assign it to a color map (see [here](https://matplotlib.org/3.2.1/tutorials/colors/colormaps.html) for an explanation of matplotlib color maps).  Remembering that the Julia set is the border between bounded and unbounded iterates of \eqref{eq1}, a cyclical colormap emphasizing intermediate values (ie points that do not diverge quickly but do eventually diverge) is useful. 

```python
plt.imshow(julia_set(500, 500, 70), cmap='twilight_shifted')
plt.axis('off')
plt.show()
plt.close()
```

Now we can plot the image! 

![julia set1]({{https://blbadger.github.io}}fractals/julia_set1.png)

The resolution settings (determined by h_range and w_range) are not very high in the image above because this program is very slow: it sequentially calculates each individual point in the complex array.  The image above took nearly 10 minutes to make!  

Luckily we can speed things up substantially by calculating many points simultaneously (well not really truly simultaneously, but using an optimized c array we can pretend that the calculations are happening a the same time) with numpy.  The idea is to apply \eqref{eq1} to every value of `z_array`, and make a boolean array corresponding to the elements of `z_array` that have diverged at each iteration.  The complex number array setup is the same as above, but we initialize another array `not_already_diverged` that is the same size as the `iterations_till_divergence` array but is boolean, with `True` everywhere as well as `diverged_in_past`, which is all `False` to begin with. 

```python
import numpy as np 
import matplotlib.pyplot as plt 

def julia_set(h_range, w_range, max_iterations):
	''' A function to determine the values of the Julia set. Takes
	an array size specified by h_range and w_range, in pixels, along
	with the number of maximum iterations to try.  Returns an array with 
	the number of the last bounded iteration at each array value.
	'''
	# top left to bottom right
	y, x = np.ogrid[1.4: -1.4: h_range*1j, -1.4: 1.4: w_range*1j]
	z_array = x + y*1j
	a = -0.744 + 0.148j
	iterations_until_divergence = max_iterations + np.zeros(z_array.shape)

	# create array of all True
	not_already_diverged = iterations_until_divergence < 10000

	# creat array of all False
	diverged_in_past = iterations_until_divergence > 10000
```

Instead of examining each element of `z_array` individually, we can use a single loop to iterate \eqref{eq1} as follows:
 
 ```python
	for i in range(max_iterations):
		z_array = z_array**2 + a
 ```

In this loop, we can check if any element of ```z_array``` has diverged,
$|z| > 2, z(z^* ) > 2^2 = 4$
and make a boolean array, `diverging` is made, which contains a `True` for each position in `z_array` that is diverging at that iteration. To prevent values from diverging more than once (as the `z_array` is reset to 0 for diverging elements), we make another boolean array `diverging_now` by taking all elements of positions that are diverging but have not already diverged (denoted by boolean values in the `not_already_diverged` array).  Values of `iterations_till_divergence` that are diverging are assigned to the iteration `i`.

```python
		z_size_array = z_array * np.conj(z_array)
		diverging = z_size_array > 4
		
		diverging_now = diverging & not_already_diverged
		iterations_till_divergence[diverging_now] = i
		
		not_already_diverged = np.invert(diverging_now) & not_already_diverged
```

In order to prevent overflow due to values heading towards infinity, values that are diverging or have diverged in the past are set to 0.  It is necessary to reset values upon each iteration because for some Julia sets the value $0 + 0i$ also diverges.

```python
		# prevent overflow (values headed towards infinity) for diverging locations
		diverged_in_past = diverged_in_past | diverging_now
		z_array[diverged_in_past] = 0

	return iterations_till_divergence
```

Finally the array of the number of iterations at each position is returned and an image is made 

```
plt.imshow(julia_set(2000, 2000, 70), cmap='twilight_shifted')
plt.axis('off')
plt.show()
plt.close()
```

which yields for $a = -0.744 + 0.148i$:

![julia set1]({{https://blbadger.github.io}}fractals/julia_set2.png)

This is much faster: it takes less than a second for my computer to make the low-resolution image that previously took nearly ten minutes! Using `cmap='twilight'`, and scaling the image by using the kwarg `extent`.

```python
plt.imshow(julia_set(2000, 2000, 200), cmap='twilight_shifted', extent=[-1.4, 1.4, -1.4, 1.4])
plt.axis('on')
plt.show()
plt.close()
```

![julia set1]({{https://blbadger.github.io}}fractals/julia_set__corrected.png)

There are a multitute of interesting Julia sets, each one defined by a different $a$ value.  We can make a video of the changes as we increment from $a=-0.29609091 + 0.62491i \to a = -0.20509091 + 0.71591i$ (how to do this is shown below) at the same scale as shown above.

![julia set1]({{https://blbadger.github.io}}fractals/julia_ranged_a.gif)

There are many beautiful Julia sets that one can make!  To make your own, follow [this link](https://jenerator.herokuapp.com/) for a web application that generates Julia sets given user input.

### Julia sets are fractals

As Gaston Julia found long ago, these sets bounded but are nearly all of infinite length.  Nowadays we call them fractals because they have characteristics of multiple dimensions: like 1-dimensional lines they don't seem to have width, but like 2-dimensional surfaces they have infinite length in a finite area.  Fractals are defined by having a counting dimension (Hausdorff, box, self-similarity etc) greater then their topological dimension, and nearly all fractals have fractional dimension (3/2, 0.616 etc). 

To put it in another way, fractals stay irregular over different size scales.  They can be spectacularly self-similar (where small pieces are geometrically similar to the whole) like many Julia sets and the Mandelbrot set, but most are not (see this excellent video by 3B1B on the definition of a fractal [here](https://www.youtube.com/watch?v=gB9n2gHsHN4).  The fractals formed by the [Clifford attractor](/clifford-attractor.md) and [pendulum maps](/pendulum-map.md) are not self-similar in the strictest sense.

How can a bounded line possibly have infinite length? If we zoom in on a point on the set, we can see why this is: the set stays irregular at arbitrary scale.  Let's make a video of the Julia set zoom!  Videos are composed of sequences of images that replace each other rapidly (usually around 30 times per second) in order to create an approximation of motion.  So to make a zooming-in video, we just need to iterate over the Julia set function defined above many times, decreasing the scale as we go!

The first thing to do is to pick a point in the complex plane to focus on, and to decide how fast the zoom should be.  For an zoom that does not appear to slow down or speed up, an exponential function must be used.  As magnifying an image by a power of two provides a clear image of what to think about, let's use exponents with base 2.  Say one wanted to increase scale by a factor of two over one second, with 30 individual images being shown in that second.  Then each frame should be scaled by a factor of $ \frac{1}{2 ^{1/30}}$, and the way I chose to do this is by multiplying the exponent $1/30$ by the frame number `t`.  I prefer a 4x zoom each second, so the exponent used is $t/15$.  

If the scale is mostly symmetrical (nearly equal x and y ranges), that is all for the magnification process!  Next we need to decide which point to increase in scale.  For the Julia set defined by $a = -0.29609091 + 0.62491i$, let's look at  $x = -0.6583867 - 0.041100001i$.

```python
def julia_set(h_range, w_range, max_iterations, t):
	# top left to bottom right grid
	y, x = np.ogrid[1.4/(2**(t/15)) + 0.041100001: -1.4 / (2**(t/15)) + 0.041100001:h_range*1j, \
			-1.4/(2**(t/15)) -0.6583867: 1.4/(2**(t/15)) -0.6583867:w_range*1j]

```

The `julia_set()` function needs to be called for every time step `t`, and one way to do this is to put it in a loop and save each image that results, with the intention of compiling the images later into a movie.  I prefer to do this rather than compile images into a movie without seeing them first.  The following code yields all 300 images being added to whatever directory the `.py` file running is held in, with the name of the image being the image's sequence number.  

An important step here is to increase the `max_iterations` argument for `julia_set()` with each increase in scale!  If this is not done, no increased resolution will occur beyond a certain point even if we increase the scale of the image of interest.  To see why this is, consider what the `max_iterations` value for the true Julia set is: it is infinite!  If a value diverges after any number of iterations, then we consider it to diverge.  But at large scale, there may not be a significatn change upon increase in `max_iterations` (although this turns out to not be the case for this particular Julia set, see below), so to save time we can simply start with a smaller `max_iterations` and increase as we go, taking more and more time per image.  The true Julia set is uncomputable: it would take an infinite number of iterations to determine which $x + yi$ values diverge for all $z \in \Bbb C$, and this would take infinite time.  

```python
for t in range(300):
	plt.imshow(julia_set(2000, 2000, 50 + t*3, t), cmap='twilight')
	plt.axis('off')
	plt.savefig('{}.png'.format(t), dpi = 300)
	plt.close()
```
Note that keyword arguments `vmin` and `vmax` may be supplied to `plt.imshow`, for example by changing the second line to `plt.imshow(julia_set(2000, 2000, 50 + t*3, t), vmin=1, vmax=50+t*3, cmap='twilight')`.  These signify the minimum and maximum range for the color map, and are not strictly necessary to make a video but prevent `pyplot` from initializing a new automatically defined color range for each image.  This auto-scaled range is unstable, leading to consecutive images having different colors for the same numerical values which appears as flashes in the background of the video.  This becomes more important for videos of polynomial root finding algorithms, among other things.

Now we have all the images ready for assembly into a movie!  For my file system, the images have to be renamed in order to be in order upon assembly.  To see if this is the case in your file system, have the folder containing the images listed.

```bash
(base) bbadger@bbadger:~/Desktop/julia_zoom1$ ls
```

My list was out of order (because some names were one digit and some 3 digits long), so I wrote a quick script to correct this.

```python
# renames files with a name that is easily indexed for future iteration
import os

path = '/home/bbadger/Desktop/file_name_here'

files = os.listdir(path)

for index, file in enumerate(files):
	name = ''
	for i in file:
		if i != '.':
			name += i
		else:
			break

	if len(name) == 2:
		os.rename(os.path.join(path, file), os.path.join(path, ''.join(['0', str(name), '.png'])))
	elif len(name) == 1:
		os.rename(os.path.join(path, file), os.path.join(path, ''.join(['00', str(name), '.png'])))

```

Now that the images are in order, we can assemble them into a movie!  This can be done many ways but I find `ffmpeg` to be very fast and stable.  Here we specify a mp4 video to be made with 30 frames per second, from in order matching `*.png` (read 'something'.png) and the resulting .mp4 file name is specified.

```bash
(base) bbadger@bbadger:~/Desktop/julia_zoom1$ ffmpeg -f image2 -r 30 -pattern_type glob -i '*.png' julia_zoom.mp4

```
which yields

{% include youtube.html id='mMOYJMgkFEs' %}

The bounded line stays irregular as we zoom in (with an increased `max_iterations` value), and if this irregularity continues ad infinitum then any two points on this set are infinitely far from each other.

### Slow divergence in a Julia set

The appearance of more diverged area (ie the purple 'river') in the zoom above suggests that this particular Julia set ($a = -0.29609091 + 0.62491i$) contains values that eventually diverge, but do so very slowly.  Which values are these?  To see this, let's see what happens when we go from 10 to 2500 maximum iterations:

![julia set1]({{https://blbadger.github.io}}fractals/julia_iterations.gif)

There is more and more area that diverges with an increasing number of maximum iterations.  What appears to be a solid area of no divergence at a small number of maximum iterations is revealed to be a mosaic of unconnected points, of area 0.

### Discontinuous transition from linear to nonlinear complex maps

Take the familiar example where $a = -0.744 + 0.148i$, but now let's see what happens when we move from 

$$
f(x) = x^1 + a \to \\
f(x) = x^4 + a \tag{2}
$$

![julia set1]({{https://blbadger.github.io}}fractals/julia_exponent_1_to_4.gif)


We can learn a few things from these maps.  The first is that non-differentiable regions (angular places) of the set experience a transition from a finite to an infinite number.  This means that the maps go from mostly smooth with a few sharp angles to being entirely composed of sharp bends and twists, which is a transition from differentiability most places to nowhere-differentiability.

Another thing we can learn is that there are abrupt transitions between sets with connected areas and disconnected dusts.  This is explored more fully in the [Mandelbrot set](/mandelbrot-set.md), but just from these videos we can see that such changes are extremely unpredictable.










## Language Modeling and Discrete Encodings

This page is a continuation of Parts [I](https://blbadger.github.io/language-representations.html) and [II](https://blbadger.github.io/language-representations-inputs.html). 

### Introduction

In [Part II](https://blbadger.github.io/language-representations-inputs.html) it was observed that hidden layer representations of the model input are typically hopelessly inaccurate, which is somewhat surprising being that vision transformers and convolutional models are capable of quite accurate input representation even in deeper layers.  This page begins by further testing representation accuracy before exploring the theory behind input representation accuracy for language models, and theory as to why this accuracy is important to language modeling tasks.

### Restricted Vocabulary Input Representation

Why are trained GPT-2 models incapable of accurate input representation?  Observing the spatial pattern of feature and input representations (see [here](https://blbadger.github.io/language-representations.html) for work on this topic) gives us a hypothesis: in that work it is apparent that trained GPT-2 transformer blocks introduce high-frequency noise into the input representation of two images such that the argument maximum value of all pixels is no longer meaningful.  On this page we deal with the argmax of logits which are transformed via a Moore-Penrose pseudoinverse, but it stands to reason that a similar phenomenon is occuring here such that the argmax of the logits becomes unpredictable due to the introduction of high-frequency noise in $a_g$.

One way to test this idea is to note that the vast majority of the $50257$ tokens present in the GPT-2 vocabulary signify words or word parts that are extremely unlikely to be observed in real text.  Indeed it appears that these very tokens are the ones that tend to result when the $\mathrm{arg \; max}$ function is called on the logits of those token indicies for trained GPT-2. 

To prevent high-valued but rare tokens from being selected during the decoding process, we can simply mask all the tokens that are not selected (not in the `allowed_tokens` tensor) with 0 and proceed with argmax as before.

```python
def masked_decode(logits: torch.tensor, allowed_tokens: torch.tensor) -> str:
	mask_tensor = torch.zeros(logits.shape).to(device)
	mask_tensor[:, :, allowed_tokens] = 1.
	masked_logits = logits * mask_tensor
	allowed_output = torch.argmax(masked_logits, dim=2)[0]
	output = tokenizer.decode(allowed_output)

	return output
```

If we are very selective with our `allowed_tokens` and only allow words in the target input to be selected, we find that the input may be exactly recovered by a trained GPT-2 transformer block, such that the input representation is for $N=2000$ is $a_g = \mathrm{The \; sky \; is \; blue.}$  With some learning rate tuning and a sufficiently large $N$, we can recover the target input even for 4 transformer blocks, but for all 12 blocks it is rare that $a_g = a$ even after tuning such that $\vert \vert a_g - a \vert \vert < \vert \vert a' - a \vert \vert$.

This is remarkable in that even a very small expansion of the allowed vocabulary results in a trained GPT-2 block being unable to accurately recognize the input.  For example, given the words of the sentence 'The sky is blue or red depending on the time of day.' as our allowed tokens, after $N=20k$ steps we have 

$$
a_g = \mathtt{\;  day \; or \; day \; blue \; or}
$$

In contrast, the same model before training is capable of recovering the target input after $N=200$ steps. Note that neither trained nor untrained GPT-2 model with all 12 blocks is capable of recovering the input for even that slightly expanded vocabulary.

### Direct Input Representation

Early on this page, two methods to deal with the discrete nature of language inputs were noted: the first was to simply ignore them during the gradient descent process and convert the outputs back to tokens via the Moore-Penrose pseudoinverse, and this is arguably the most natural choice as it involves the fewest changes to the model itself.  Another option is to transform the input tokens (an array of integers) into a vector space and perform gradient descent directly with that space being the terminal node of the backpropegation computational graph.

To be precise, what we want is to convert an array of input tokens $a_t$, which for example could be

$$
a_t = [1, 0, 2]
$$

to continuous values in a vector space, which for the above example could be

$$
a = \begin{bmatrix}
0 & 1. & 0 \\
1. & 0 & 0 \\
0 & 0 & 1. \\
\end{bmatrix}
$$

but note that there is no specific rule for converting a token to a vector value, for instance we could instead assign any small value as a baseline with a larger value as the corresponding indicies.  For GPT-2, we can find what the trained embedding assigns for each input element by finding the Moore-Pensore pseudoinverse of the weight matrix of the embedding, denoted $W^+$, as follows:

$$
a = W^+E(a_t)
$$

where $a_t$ corresponds to an array of integers and $E$ signifies the embedding transformation that maps these integers to an embedding vector space. 

which can be implemented as 

```python
embedding = model.transformer.wte(tokens) 
embedding_weight = model.transformer.wte.weight.float() # convert to float in case model is in 16-bit precision
inverse_embedding = torch.linalg.pinv(embedding_weight)
logits = torch.matmul(embedding, inverse_embedding) # invert embedding transformations
```

Verifying that this procedure is accurate is not too hard, and may be done by first converting the vector space back to integer tokens via the $\mathrm{arg \; max}$ of each logit, and then decoding this array of integers as shown below.  

```python
tokens = torch.argmax(target_logits, dim=2)[0]
output = tokenizer.decode(tokens)
```

With this procedure, we find that the trained GPT-2 word-token embedding maps the value at the index of the integer token to a value on the order of $1 \times 10^{-2}$ whereas the values of other indices are typically $\leq 1 \times 10^{-3}$. We can also simply mandate that the vector values at indicies corresponding to the appropriate tokens have some large positive value and that all others are zero as follows:

```python
target_logits = torch.zeros(logits.shape).to(device)
for i in range(len(tokens[0])):
	target_logits[:, i, tokens[0, i]] = 10
```

There does not appear to be any significant difference between these two approaches to mapping $a_t \to a$.

Now that we have a continuous $a$ input, we can perform gradient descent on the model's output as follows:

$$
a_{n+1} = a_n + \eta * \nabla_{a_n} ||O_l(a_n, \theta) - O_l(a, \theta)||_1 \\
\tag{5}\label{eq5}
$$

where $a_0 = \mathcal N(a, \mu=1/2, \sigma=1/20)$.  The GPT-2 model also needs to be modified as the word-to-embedding transformation is designed to take in integer 'words' rather than vectors, and this can be ameliorated by replacing this transformation with a multiplication of the input by the word-to-embedding transformation weights as follows.

```python
class InputGPT(nn.Module):

	def __init__(self, model):
		super().__init__()
		self.model = model

	def forward(self, x: torch.tensor) -> torch.tensor:
		# replaces wte transformation
		x = torch.matmul(x, self.model.transformer.wte.weight)
  
		for i in range(12):
			x = self.model.transformer.h[i](x)[0]

		return x
```

For clarity, the following figure provides a summary of the direct input representation method.

![direct input representation]({{https://blbadger.github.io}}deep-learning/llm_direct_representation.png)

With this direct gradient descent method, can a single trained transformer block in GPT-2 be inverted accurately? Given the input 

$$
a = \mathtt{This \; is \; a \; prompt \; sentence}
$$

the answer is no: iterating \eqref{eq5} such that $\vert \vert O_l(a_g, \theta) - O_l(a, \theta) \vert \vert < \vert \vert O_l(a', \theta) - O_l(a, \theta) \vert \vert$ (where $a'$ is a slightly shifted $a$ such that the tokenization of these vector values are identical) we have

```
 precarious NeighNASA Someonetre
lisherusersExp qualifying windshield
 ple propose18 joyful Genie
SHIP Geh lesbians inquiries Mat
1968 Carroll delibericycle consumers
```

Even when we limit the possible tokens to a very restricted set, specifically the tokens 

```
This is a prompt sentence with some extra words attached to increase the allowed vocabulary by a small margin
```

and optimizing \eqref{eq5} such that the inequality denoted above is observed, we have

$$
a_g =  \mathtt{This \; some \; by \; small \; is}
$$

indicating that a single trained GPT-2 transformer block is incapable of accurate input representation even for a restricted input vocabulary, even when the input is used to perform the gradient descent. This is also true when the gradient of \eqref{eq5} is calculated using the $L_2$ norm, indicating that it is not the choice of $L_1$ norm that prevents accurate input representation here.  

The same results are observed for one transformer embedding transformation and first block of the 1.5B parameter `gpt2-xl` model, such that neither the trained nor untrained model subsets are capable of accurate input representation even when the vocabulare is extremely limited.

For example, the top-5 decoding for the input 'This is a prompt sentence' for the word-to-embedding transformation followed by the first transformer block of an untrained `gpt2-xl` model is

```
 ple NeighNASA inquiries windshield
 duplusers 131 qualifyingtre
rypted Sharif deliber camping Genie
1968 Carroll maleicycle Humanity
 precariouscandExp semen consumers
 ```
 
 and for the trained model's embedding followed by the first transformer block, the top-5 input representations for the same prompt are
 
 ```
  silenced oppression Fav positioned
 Shows debugprint interference Deploy
 softened949313 absent transports
 Ambrose mono unanswered chantingAM
ogacessive dungeonJR785
 ```

There is a clear explanation for why the GPT-2 embedding transformation is non-invertible: the linear transformation corresponding to the token-to-embedding operation transforms a vector space of dimension $d(a) = 50257$ to a vector space of $d(O_e(a, \theta)) = 728$ for the base GPT-2 model, or $d(O_e(a, \theta)) = 1600$ for the 1.5B parameter `gpt2-xl`.  Non-invertibility is expected for both embeddings being that the output dimension is much smaller than the input, and as the input dimension exceeds the output there are many different inputs that will yield one identical output of the word to embedding transformation.  

But [elsewhere](https://blbadger.github.io/language-representations-inputs.html#sentence-representation-with-language-models) we have seen that non-invertible transformations can be effectively inverted using gradient descent, in the sense that an accurate input representation can be made accross non-invertible transformations.  Thus it remains to be seen whether direct input representations can be accurate for other models, even if they are not accurate for GPT-2.

### Multiple hidden layer outputs do not improve input representation

Why do trained language models exhibit such poor input representations?  In the previous section, we found that the largest version of GPT2 exhibits far less repetition in its input representations than the smallest version of the same model.  Unfortunately it is also clear that the larger model is no more capable of producing a coherent input representation (even for one transformer block) even after 1000 gradient descent iterations, corresponding to a generated distance <1/6 the magnitude of the shifted distance.

It may also be wondered whether or not input representation would be improved if we used the output of multiple layers rather than only one. For the first three layers of a model in which we perform gradient descent on the input directly, this could be implemented as follows:

```python
class InputGPT(nn.Module):

	def __init__(self, model):
		super().__init__()
		self.model = model

	def forward(self, x: torch.tensor) -> torch.tensor:
		# replaces wte transformation
		x = torch.matmul(x, self.model.transformer.wte.weight)

		o1 = self.model.transformer.h[0](x)[0]
		o2 = self.model.transformer.h[0](o1)[0]
		o3 = self.model.transformer.h[0](o2)[0]
		output = torch.cat((o1, o2, o3))
		return output
```

But the input representations resulting from this are no better than before, and appears to confer the same ability to accurately represent an input as simply taking the output of the last (in this case the third) block.

### Indirect input representation via cosine similarity

Another possibility is that the metric we are using to perform gradient descent is not idea for input representation.  Specifically, instead of minimizing the $L^1$ metric

$$
m = || O(a, \theta) - O(a_g, \theta) ||_1
$$

we can instead maximize the cosine similarity (ie the cosine of the angle $\phi$) between vectorized (ie flattened) versions of the output of the target input's embedding $O(e, \theta)^* $ and the vectorized output of the generated input embedding at iteration $n$, $O(e_n, \theta)^*$

$$
\cos (\phi) = \frac{O(e, \theta)^* \cdot O(e_n, \theta)^* }{||O(e, \theta)^* ||_2 * ||O(e_n, \theta)^* ||_2}
$$

such that gradient descent on the embedding $e_n$ is performed as follows:

$$
e_{n+1} = e_n - \eta \nabla_{e_n} \left( 1 - \cos (\phi) \right)
$$

where $\eta$ is a tunable learning rate parameter. Performing gradient descent on the input of the first transformer block of a trained GPT-2 to minimize $\cos(\phi)$ turns out to lead to no more accurate input embeddings than before: increasing $\cos (\phi)$ to $0.96$ via 500 iterations yields embeddings that may be inverted to give

```
[] become Holiday trollingMa calls
 UnierierylCAP PearlOTA
Hard obviousbys abiding73
arcer underCoolmanship autobiography outcomes
 slog bringsquaDirgap {:
```

Even for a very limited vocabulary ('The sky is blue or red depending on the time of day.') the one transformer decoder module from GPT-2 cannot accurately represent the input, although a significant improvement is made.

$$
\mathtt{This \; attached \; with \; prompt \; sentenceThis.}
$$

Therefore gradient descent is successful in minimizing the cosine distance between the output of the generated and target input (in this case embedding), but the generated input corresponds to nonsense.  The same is true even for single transformer decoders from untrained GPT-2 models, and as we earlier found that these modules can yield accurate input representations using gradient descent on the $L^1$ norm of the output difference, the cosine similarity may be viewed as a weaker measure for representation tasks.

Perhaps a linear combination (parametrized by constants $\alpha, \beta$ of the gradient of the cosine distance and a normed difference would be sufficient for accurate input representation from trained GPT-2 transformer modules.  Specifically we can see if the following update on the embedding is capable of accurate input representation. The update we make is

$$
e_{n+1} = e_n - \eta * \nabla_{e_n} \left( \alpha (1 - \cos (\phi)) + \beta ||O(a_g, \theta) - O(a, \theta)||_1 \right)
$$

where $\eta$ is the learning rate hyperparameter, which is typically scheduled such that $\eta = 1 \to \eta = 1/100$ as the input representation iterations $n = 0 \to n = N$. But even for this metric the input representation after only one transformer encoder is quite inaccurate: for the first transformer block of an untrained GPT-2, we have input representations for 'The sky is blue.'

```
 execute228 therein blue.
 politicians jihadist18having Phill
 Given patriarchal Lightning Hughes sem
 discriminatelich antitrust fraternity
ucking demos underrated310
```

which means that the linear combination is less accurate than using the $L^1$ norm loss alone.

### Model Interpretability

Given the extreme difficulty in accurate input representation typical of large language models when using gradient descent on an initially random input, it may be wondered whether the gradient on the input is capable of offering any useful information.  This may be tested in a number of different ways, but one of the simplest is to observe what is termed input attribution.  In the context of autoregressive language models trained for causal language modeling (ie predicting the next token), input attribution may be thought of as the importance of each prior token in predicting that next token.

How does one take a model and find which elements in the input are most responsible for the model output?  In this case the model gives the next token in a sentence, and the input is composed of all prior tokens.  We will briefly explore two options for finding the 'most important' input elements for a given output, although this is by no means a comprehensive survey of the field of input attribution.

One method is to find the gradient of the output with respect to each input element and multiply this gradient element-wise to the input itself. This method is known as 'gradientxinput' and may be found as follows:

$$
v = | \nabla_a  O(a, \theta) | * a
$$

where the vector of inputs is $a$ and the vector of saliency values is $v$ and $* $ denotes Hadamard (element-wise) multiplication, and $\vert * \vert$ signifies the element-wise absolute value operation.  This method is intuitively similar to measuring the effect of an infinitesmal change in the input (via the gradient of the output $O(a, \theta)$ with respect to the input $a$) on the output, which a larger output change at index $i$ resulting in a larger saliency value at that index.

Another approach is to simply remove each input element sequentially and observe the change in the ouput.  If $a_c$ correponds to the input where the token at position $c$ is replaced with an informationless substitute (perhaps a $<\vert PAD \vert >$ token) then we can find a metric distance between the model's output given this masked input compared to the original $a$ as follows:

$$
v = || O(a_c, \theta) - O(a, \theta) ||_1
$$

Here we use the $L^1$ metric as a measurement of the difference between outputs, but we could just as easily have chosen any other metric. This method is appropriately termed 'occlusion' after the English word for 'make obscure'.  This is in some ways the discrete counterpoint to saliency, as here we are observing what happens to the output after a big change (replacement of an entire token).  

For small input sequences, we can form a linear combination of occlusion and gradientxinput in order to include information from both attribution methods.  For longer sequences, occlusion becomes prohibitively expensive for LLMs and therefore it is usually best to stick with saliency.

To visualize input attribution on sequences of text, we can implement an HTML-based highlighter on the decoded input tokens as follows:

```python
	def readable_interpretation(self, decoded_input, metric='combined'):
		...

		# summed_ems is a torch.tensor object of normalized input attributions per token
		positions = [i for i in range(len(summed_ems))]

		# assemble HTML file with red (high) to blue (low) attributions per token
		highlighted_text = []
		for i in range(len(positions)):
			word = decoded_input[i]
			red, green, blue = int((summed_ems[i]*255)), 110, 110
			color = '#{:02x}{:02x}{:02x}'.format(red, green, blue)
			highlighted_text.append(f'<span style="background-color: {color}">{word}</span>')

		with torch.no_grad():
			embedded_inputs = torch.clone(self.model.transformer.wte(self.input_ids))
			output = self.model(inputs_embeds=embedded_inputs)[0][:, -1, :]
			predicted_word = self.model(self.input_ids)[0][:, -1, :] # should be equal to output
			assert torch.equal(output, predicted_word)

			predicted_word = int(torch.argmax(predicted_word, dim=1))
			predicted_word = tokenizer.decode(predicted_word)

		highlighted_text = ''.join(highlighted_text)
		highlighted_text += f'</br> </br> Predicted next word: {predicted_word}'
		with open('data.html', 'wt', encoding='utf-8') as file:
			file.write(highlighted_text)
		webbrowser.open('data.html')
```

This function displays the relative attribution (importance) of each previous token by assigning the attribution value to the red highlighter color, while keeping green and blue highlighter values constant (see the snippet below).  This makes higher-attribution tokens appear red, medium-attribution orange and grey, and low-attribution tokens green-blue.

```
red, green, blue = int((summed_ems[i]*255)), 110, 110
color = '#{:02x}{:02x}{:02x}'.format(red, green, blue)
```

After loading our model of choice, we can use this module (contained in the GPTEval class, see the [source code](https://github.com/blbadger/nnetworks/blob/transformer-explorer/language_attribution.py) for more information) to generate input attribution maps as follows:

```python
if __name__ == '__main__':
	input_sequence = 'The wipers on the bus go swish swish'
	input_sequence = input_sequence.strip()
	gevaluate = GPTEval(model, input_sequence)
	input_ids = gevaluate.input_ids[0]
	decoded_input = []
	for symbol in input_ids:
		decoded_input.append(tokenizer.decode(symbol))
	arr = gevaluate.readable_interpretation(decoded_input, metric='combined')
```

which when given the prompt 'The wipers on the bus go swish swish' for the base model of GPT-2 gives

<html>
<body>
<span style="color: white">
	<span style="background-color: #156e6e">The</span><span style="background-color: #746e6e"> wip</span><span style="background-color: #006e6e">ers</span><span style="background-color: #616e6e"> on</span><span style="background-color: #626e6e"> the</span><span style="background-color: #726e6e"> bus</span><span style="background-color: #ff6e6e"> go</span><span style="background-color: #9e6e6e"> sw</span><span style="background-color: #fa6e6e">ish</span><span style="background-color: #966e6e"> sw</span><span style="background-color: #e46e6e">ish</span>
	</span>
	<br>
	<br> 
	Predicted token:  sw
</body>
</html>

Observing input attribution for larger models applied to more complicated inputs, it is clear that the gradient of a model's output with respect to its input does indeed give useful information.  This being the case, we are left with the question we had at the start of this page: why are input representations so inaccurate for trained GPT-type transformers applied to natural language, and indeed why are they less accurate than for untrained models of the same type. 

### Sequential Input Representation

Continuing our efforts to understand why trained GPT-2 models give such poor input representations compared to trained vision transformers, one can observe that there are two notable differences between language and image inputs: language is fundamentally sequential and discreet whereas images are approximately continuous and of no particular orientation.  In this section we consider the importance of the sequential aspect of language as it relates to input representation, and in the next section discreeteness is examined.

To rephrase the topic for this section, one may wonder whether or not the sequential aspect of language could contribute to difficulties in input representation.  Specifically, observe that GPT-type language models always predict one word at a time, and thus may be better at representing one word at a time too.

We can implement this sequential input representation technique as follows:

```python
def generate_sequential_input(model: torch.nn, target: torch.tensor, lr=0.5) -> torch.tensor:
	n_tokens = len(target[0])
	full_input = []
	embedding_dim = target.shape[-1]
	starting_input = torch.randn(1, 1, embedding_dim).to(device)
	combined_input = torch.clone(starting_input)
	for i in range(n_tokens):
		random_input = torch.randn(starting_input.shape).to(device)
		focused_target = target[:, i, :]
		single_input = octave(random_input, focused_target, 2000, [lr, lr/10], i) # generate single token's embedding
		combined_input = torch.cat((combined_input, single_input), dim=1)

	return combined_input[:, 1:, :]
```

For an untrained model given the input 

$$
a = \mathtt{The \; sky \; is \; blue.} 
$$

we have a top-5 input representation of

```
 Pegasus quarantine Sunshine viral Dip
 secure Help Orion Saadesh
 partic Carbon Officereller Rory
 checked regained midst ERAtile
 criticize231 SOFTWARE trunkmatically
```

and therefore we conclude that it is not the sequential characteristic of language that is the cause of poor input representation.

### Is the trained word to token embedding to blame

It might also be wondered whether the loss of input representation ability during training is due to the model learning a poorly conditioned token embedding transformation. We can test this hypothesis by replacing the trained token embedding transformation with a non-trained one, and for the direct input generation method earlier on this page this can be implemented by simply swapping the weight matrix of the word-token embedding with an untrained model's version of the same.

```python
class InputGPT(nn.Module):

	def __init__(self, model, untrained_model):
		super().__init__()
		self.model = model
		self.untrained_model = untrained_model

	def forward(self, x: torch.tensor) -> torch.tensor:
		# replaces wte transformation
		x = torch.matmul(x, self.untrained_model.transformer.wte.weight)

		for i in range(1):
			x = self.model.transformer.h[i](x)[0]
		return x
```

However, we are not met with any success in making even one transformer block yield accurate input representations.  
```
 veiledusers lesbians inquiries windshield
 dupl NeighNASAicycletre
rypted IntExp qualifying Genie
 melts funded deliber camping Mat
SHIP rejoice malepc consumers
```

Similarly poor input representations are exhibited when we use the [Roy-Moore pseudo-inverse indirect input generation method](https://blbadger.github.io/language-representations-inputs.html#sentence-representation-with-language-models) following the 

```
PsyNetMessagePsyNetMessagePsyNetMessage MarketablePsyNetMessage
 srfAttach srfAttach MarketablePsyNetMessage srfAttach
irtual unfocusedRange unfocusedRange unfocusedRange partName
 partName Marketable srfAttachawaru Marketable
ascus partName partName srfAttach unfocusedRange
```
In conclusion, we find that the training of the embedding transformation alone cannot account for the poor representation that exists in trained language models.  This does not necessarily mean that the embedding transformation does not affect input representation, but only that the process of training the embedding alone does not account for the poor input representation in GPT-2 models after training.

### Big models exhibit accurate input representations

It has been claimed that language models as they exist today are incapable of any kind of fact- or reason- based task because they are merely trained to predict the next word (or more accurately token) in some text. We can show that this claim is incorrect, however, using a fairly straightforward argument.  We can define a language model as some generalizable (to examples unseen in the training data) representation function that maps input token sequences to an output token such that the negative log-likelihood of that output token is minimized relative to the training dataset.  

From this definition alone we find that language models must also be capable of all the implicit tasks present in the training dataset.  For example, suppose the training data contained the phrase 'two plus two equals four'.  Given the input 'two plus two equals' the function must return 'four' if it has minimized the negative log-likelihood of that token. Now suppose the training data also contains the phrase 'four plus one equals five' and that now we give the unseen phrase 'two plus two plus one equals' and our function returns a token.  If our function is sufficiently generalizable, we are almost guaranteed to return 'five' because the internal representation of 'two plus two' is nearly equivalent to 'four' and the model has already learned that 'four plus one equals five'.

It has been empirically observed that models with more parameters are generally better at these implicit natural language tasks, which typically lie under the umbrella definition of 'reasoning' problems.  There are a few explanations for why this could be: firstly, larger models entail higher-dimensional space, such that gradient descent is more [biased towards generalization](https://arxiv.org/abs/2211.09639), secondly because more parameters imply greater 'memory' such that the model can learn more complicated representations of an input (ie 'three' being both a word and an implicit number) and thirdly because it is apparent that biological neural networks require a very large number of neurons to deal with language relative to other tasks, such as object recognition.

But when we have observed that although larger models (~1 billion as opposed to 100 million parameters) tend to have less repetition, they are no noticeably better at input representation than smaller models (~100M parameters). Do even larger models fail to exhibit accurate input representation?  We can investigate by observing the ability of our gradient descent procedure to autoencode an input from transformer models of a trained GPT-J model, which contains ~6B parameters.  To save memory, we can load the parameters in 8-bit format using `bitsandbytes` 

```python
load_8bit = True
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6b")
model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-j-6b", load_in_8bit=load_8bit, device_map='auto')
```

GPT-J, like Llama and other transformer models used today, employs Rotary Positional Encoding that is applied to each attention module (introduced by [Su and colleagues](https://arxiv.org/abs/2104.09864)).  This means that there is no `wpe` transformation to invert, but we instead have to supply the appropriate positional information as `position_ids` to each transformer block.  Using the indirect input generation method in which gradient descent is performed on the first hidden layer before the resulting tensor is inverted, the modifed version of a trained GPT-J model may be specified as follows

```python
class AbbreviatedGPT(nn.Module):

    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, x: torch.Tensor):
        position_ids = torch.tensor([i for i in range(x.shape[1])])

        for i in range(1):
            x = self.model.transformer.h[i](x, position_ids=position_ids)[0]
        return x
```

Here we investigate the representations present various layers of a model using gradient descent on the embedding of a discrete language input, a technique explained [here](https://blbadger.github.io/language-representations-inputs.html#sentence-representation-with-language-models).  For the prompt 'The sky is blue.' after $N=200$ iterations with the first transformer block of GPT-J, we generate an $a_g$ such that $\vert \vert a_g - a \vert \vert < \vert \vert a' - a \vert \vert$ which yields the top-5 token matching of

```
The skyeton resp cease
hinPresidentVER compose Instruments
uddinuko be blue_
JJzbuse Oxy rite
earlyerent iswn seasoning
```

which is certainly somewhat better than we saw for the smaller models. Note, however, that even at $N=2000$ the input representation accuracy is not sufficient to unambiguously identify the input string.

On the other hand, if we restrict the input tokens to any of 'The sky is blue or red depending on the time of day.' we come very close to recovering the input.

$$
a_g = \mathtt{The \; sky \; is \; blue \; of}
$$

Which is a notable improvement upon the smaller trained models seen previously.

This begs the question: would an even larger model be capable of even more precise input representation? We can check this using various models but one in particular is the Llama model family introduced by [Touvron and colleagues](https://arxiv.org/abs/2302.13971), which may be loaded in 8-bit parameter quantization as follows:

```python
tokenizer = AutoTokenizer.from_pretrained("huggyllama/llama-13b")
model = AutoModelForCausalLM.from_pretrained("huggyllama/llama-13b", load_in_8bit=load_8bit, device_map='auto')
```

Llama models name their components somewhat differently than GPT-type models, and apply Rotary Positional Encoding via different dimensions, so after consulting the [source code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py) we subclass the model as follows:

```python
class AbbreviatedGPT(nn.Module):

    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, x: torch.Tensor):
        position_ids = torch.tensor([[i for i in range(x.shape[1])]])

        for i in range(4):
            x = self.model.model.layers[i](x, position_ids=position_ids)[0]
        return x
```

This model requires around 20GB when performing the gradient descent input representation algorithm, so the following experiments are performed using an Nvidia A100 GPU (available on Colab for a fee).

After $N=1000$ we have for the first transformer block:

$$
a_g = \mathtt{the \; sky \; the \; blue \;  }
$$

and at $N=2000$ we have the top-5 representations of

```
The sky is blue
K Sky isn Bluerows
skyBlue K
The K Kativecards
rowsXrows K?"
```

and even more remarkably, at a depth of two transformer blocks for $N=2000$ the model's representation is

$$
a_g = \mathtt{The \; sky \; is \; blue.}
$$

which means that we have found a way to get accurate input representations from a trained transfomer block! Apparently the we simply have to use an extremely large model.  Thus we would expect to observe accurate input representations from even larger models, and we test this using the 30 billion parameter version of Llama (which is about the largest model that will fit in the memory of a 40GB A100 using 8-bit quantization). For the first transformer block of this trained model after a mere $N=500$ we have top-5 representations of

```
The sky is blue<s>
 Sky: Blue 
HelpskyBlueww
smart Japan behblueATCH
cy Answer explating
```

where `<s>` signifies a sentence break token (which is nearly identical with a period in semantic meaning).  

Compare this with the top-5 representations of the first transformer block of a 7-billion parameter version of Llama, which at $N=500$ gives

```
The sky isYS2
The ofATIONhere-
..althOIN

,SE ${\reno
the
c blue,
```

although it should be noted that if the maximum number of gradient descent iterations $N$ increases to $N=1500$, the 7b parameter Llama yields a better

$$
a_g = \mathtt{The \; sky \; is \; blue2}
$$

With the 30 billion parameter Llama, we find that at least somewhat accurate input representations are made from deeper and deeper layers: representation after 4 transformer blocks is qualitatively similar to what is found for 1 block (above) even after 12 (!) transformer blocks, we get a recognizable input representation of

$$
a_g = \mathtt{The \; sky \; Stage \; blueInd}
$$

Note that with the 7 billion parameter version of Llama, a typical top-5 representation of the input at a depth of 12 blocks for the same $N$ is

```
havior skyichtetfiterson
burg speakeriffsbled
prisoners prisoners standardsunct speaker
learn studying relationshipsspanburg
rium Cup containingEEali
```

meaning that we find much poorer input representations in the 7b parameter Llama as opposed to the 30b model in the deeper layers, which is to be expected with the hypothesis that larger models are capable of transmitting input information more accurately than small ones.

Returning to the 30 billion parameter model, even after 21 transformer blocks the input representation is nearly synonymous with the prompt.

$$
a_g = \mathtt{Finally \; sky \; established \; blue \; instead}
$$

This is notable because smaller language models are capable of accurate input representation before training, but only for one or at most two transformer blocks.  But with increased model size, trained models are capable of fairly accurate input representation even in relatively deep layers.

It may also be wondered whether we can recover accurate input representations by optimizing a different metric on a hidden layer output.  Earlier on this page we saw that cosine similarity used as a metric for GPT-2 models was incapable of accurate input representation, but it is worth exploring whether this is the case now that we have a larger model to work with.  As before, we perform gradient descent on the embedding $e_n$ rather than a vectorized input.

A first experiment is not promising: given the same prompt as above ('The sky is blue') after $N=500$ iterations of gradient descent on the embedding of an input such that $\phi < 0.05$ we have

$$
a_g = \mathtt{WNWNWNWN}
$$

but the slight change to 'The sky is blue.' yields

$$
a_g = \mathtt{The \; sky \; is \; blue2}
$$

Longer prompts such as 'The sky is red or blue depending on the time of day'

$$
a_g = \mathtt{The \; sky \; is  \; blue  \; or \; red \; depending \; on \; the \; time \; OF \; day}
$$

or 'This is a prompt sentence.'
 
$$
a = \mathtt{This \; is \; a \; prompt \; sentence.}
$$

We find the same tendancy for poorer input representations at deeper layers (observed above for an L1 metric distance loss) to be the case for cosine similarity loss too. For example, at block 4 of Llama 7b we have

$$
a_g = \mathtt{This \; is \; a \; prompt \; sentence \; SU}
$$

but at block 12 we have

$$
a_g = \mathtt{Thusieraucin \; prompt \; Jersey \; Culture} 
$$

### Large models exhibit poor direct input representation when minimizing hidden layer L1 distance but not cosine similarity

In the last section we saw that very large models are capable of accurate indirect input representation when gradient descent performed on the input embedding, rather than directly on the input itself.  It may be wondered whether similarly accurate input representations are found from the same models if gradient descent is performed on the inputs diectly, after converting discrete token integers to a continuous vector space equivalent to those discrete tokens.

For Llama-type models that use RoPE, after converting a tokenized input into a vector-space equivalen `x` we can sub-class our chosen model as follows:

```python
class InputModel(nn.Module):

	def __init__(self, model):
		super().__init__()
		self.model = model

	def forward(self, x: torch.Tensor):
		# Matrix mult instead of embedding to prevent type incompatibility
		x = torch.matmul(x, self.model.model.embed_tokens.weight)
		position_ids = torch.tensor([[i for i in range(x.shape[1])]])

		for i in range(1):
			x = self.model.model.layers[i](x, position_ids=position_ids)[0]
		return x
```

Given the target input $a = \mathtt{This \; is \; a \; prompt \; sentence}$, the trained 7 billion parameter Llama model yields the following top-5 input representations at the output of the first transformer block:

```
sout spectconmicadm
Normdateiwi           
Son hasta cb
ccCollectennenzen Kirchen
alusEmemed expon.<
```

We find further that the trained 30 billion parameter Llama model is similarly incapable of accurate direct input representation.  For $N$ such that $\vert \vert O(a, \theta) - O(a_g, \theta) \vert \vert < \vert \vert O(a, \theta) - O(a', \theta) \vert \vert$ we have

```
  Schw  ded
nombreuses  FemAvailable modify
Cs projection polygonjekt
`"   dll 
dai vba p 
```

It may be wondered whether or not a different metric would give a more accurate input representation, at least for the first transformer block of Llama 7b. 

To be more precise, we want to minimize the angle $\phi$ between the vector corresponding to the hidden layer output of some target input $a$ and a generated input $a_g$ by maximizing the cosine similarity between the output of the target input $a$, denoted $O(a, \theta)$ and the generated input $a_g$, $O(a_g, \theta)$ via gradient descent on an initially random input $a_0$.  The cosine distance may be calculated on vectorized versions of these outputs, denoted $O()^*$, as follows:

$$
\cos (\phi) = \frac{O(a, \theta)^* \cdot O(a_g, \theta)^* }{||O(a, \theta)^* ||_2 * ||O(a_g, \theta)^* ||_2}
$$

such that gradient descent on a vectorized version of the input $a_n$ as follows:

$$
a_{n+1} = a_n - \eta \nabla_{a_n} \left( 1 - \cos (\phi) \right)
$$

Maximizing the cosine similarity (ie minimizing the angle $\phi$) between generated input $a_g$ and target input $a$ yields for the $a$ given above

$$
a_g = \mathtt{This \; is \; hasta \; prompt \; sentence}
$$

where $(\cos(\phi) > 0.9$.

For 

$$
a= \mathtt{This \; is \; a \; somewhat \; longer \; prompt \; sentence.}
$$ 

we have at $\cos(\phi) = 0.76$

$$
a_g = \mathtt{This \; are \; integrated \; somewhat \; longer \; prompt \; sentence }
$$

For another target sentence

$$
a = \mathtt{The \; sky \; is \; a \; light \; blue \; today.}
$$ 

at $\cos(\phi=0.75)$ we have

$$
a_g = \mathtt{primit \; sky \; is \; a \; light \; blue \; today \; }
$$

It is interesting to note that cosine similarity loss does not yield accurate input representations for small inputs.  For example, given $a = \mathtt{The \; sky \; is}$ the first block representation is

$$
a_g = \mathtt{sier \; fixeseden}
$$

Nearly every one-word target input gives very inaccurate input representations, with $a_g$ as any of 'George', 'The', 'when', or 'clearly' yielding $a_g = \mathtt{que}$, and inputs with two or only a few words are often quite inaccurately representation via cosine similarity.

```
prompt = 'Boba Fett'
Nacional**** tte

prompt = 'Boba Fett was a bounty hunder'
WNWN personinasacj

prompt = 'Boba Fett was a legendary bounty hunter in the outer rim'
Boba Fett wasondissement legendary bounty hunter in the outer rim
```

This is a marked contrast from performing gradient descent on the input embedding, where cosine similarity yields accurate representations even for one-word inputs (ie 'George' is represented 'George').

Why would the use of cosine similarity be unable to give accurate input representations when applied to input space of very small inputs but not larger ones? It is helpful to consider here what exactly is being optimized: the cosine of $\phi$ is equal to the dot product of two vectors divided by the norms of those vectors multiplied together.

### Cosine loss optimization and model training

Why is direct input representation so inaccurate for single token inputs, even though it is accurate for multi-token inputs?  Consider that the self-attention module in the Llama transformer blocks are based on the dot product operation: the attention between any query $q$ token and a key $k$ token is given as 

$$
A(q, k, v) = \mathrm{softmax} \left( \frac{q \cdot k}{\sqrt(d)} \right) v
$$

Recalling that the dot product is equivalent to the cosine of the angle between vectors $q, k$ divided by their norms, one can say that attention compares the direction between these vectors.

But something unexpected occurs when we attempt to use cosine similarity as a metric for an untrained Llama model: for one-token inputs gradient descent is able to minimize $\cos (\phi)$ on the output of any given transformer block, but when more that one token is present in the input this is no longer possible.  

We can deconstruct the transformer block by investigating the `LlamaDecoderLayer` module from the Llama [source code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py) and replicating that module in pieces as follows. 

```python
class InputGPT(nn.Module):

	def __init__(self, model):
		super().__init__()
		self.model = model

	def forward(self, x: torch.Tensor):
		# Matrix mult instead of embedding to prevent type incompatibility
		x = torch.matmul(x, self.model.embed_tokens.weight)
		position_ids = torch.tensor([[i for i in range(x.shape[1])]])

		for i in range(1):
			residual = x
			x = self.model.layers[i].input_layernorm(x)
			x = self.model.layers[i].self_attn(x, position_ids=position_ids)[0] + residual
			residual = x
			x = self.model.layers[i].post_attention_layernorm(x)
			x = self.model.layers[i].mlp(x) + residual
			
		return x
```

It turns out that removing the layer normalizations allows minimization of $\cos (\phi)$ on the output via gradient descent on the input, as long as the input is not too long (>5 tokens).  This is suggestive of a numerical underflow or overflow in the gradient backpropegation process, which appears to be exascerbated by the layer normalization transformations. 

Back to the question of whether untrained Llama models are capable of accurate input representation, we find that it is both more difficult to minimize $\cos(\phi)$ via gradient descent and that once minimized, the inputs are not accurate the the target $a$.  For $\cos (\phi) < 0.15$ we have

$$
a = \mathtt{The \; sky \; is \; blue} \\
a_g = \mathtt{ViewModeldfrac \; paths}
$$

It should be noted that untrained models appear to be worse at indirect input representation as well, as for the prompt $a = \mathtt{The \; sky \; is \; blue}$ the first transformer block from an untrained 7b Llama gives 

$$
a_g = \mathtt{leaf \; leaf  \; Connect \; leaf}
$$

at $N=1500$, whereas the trained model is much more accurate (see above).

But upon some reflection, it may not be surprising that minimizing a cosine distance between outputs of an untrained transformer block does not yield accurate input representations because the dot-product attention is followed by two fully connected layers.  If we instead observe the representation from the first ransformer block's self-attention whilst minimizing $\cos \phi$, 

```python
class InputGPT(nn.Module):

	def __init__(self, model):
		super().__init__()
		self.model = model

	def forward(self, x: torch.Tensor):
		position_ids = torch.tensor([[i for i in range(x.shape[1])]])

		for i in range(1):
			x = self.model.layers[i].self_attn(x, position_ids=position_ids)[0]
		return x
```
we have a permutation of the input 'The sky is blue.'

$$
a_g = \mathtt{blue \; is \; The \; sky}
$$

with a top-5 representation 

```
blue is The sky
The The sky is
sky blue is The
is sky blue blue
FLITEite 
```

and likewise for 'This is a prompt sentence' we have

$$
a_g = \mathtt{prompt. \; This. \; is \; prompt}
$$

Although with 4 self-attention blocks, more information is lost:

$$
a_g = \mathtt{sentence. \; This \; OK \; sentence}
$$

On the other hand, minimization of $L^1$ distance on the output of the first (untrained) full transformer block containing self-attention followed by an MLP gives

```
The ense The blue
sky is blue essere
pandas sky sky tr
blue land proud{$
.-- skill iseli
```

but for inputs composed of individual words (more precisely tokens) the representation is nearly perfect, for example any of the inputs 'This', 'is', 'a', 'prompt','sentence' give a representation identical to that input. As soon as the input contains multiple tokens, however, minimizing the $L^1$ distance on the output is not longer sufficient for perfectly accurate representation, for example 'the sky' is represented as 'sky sky'.

### Noise on a Discreet Channel

To recap, we have found that accurate input representations of language but not images are not formed in trained transformer models unless they contain a very large number of parameters, particularly in trained models.  

But first it remains to be seen why training would result in worse input representations, why larger models would be capable of much more accurate representation, and above all why accurate input representation appears to be so much more difficult for language than for images.

In developing the theory of communication over a noisy channel, Shannon and others found a mathematical explanation for a phenomenon that initially seemed most curious: the fact that the amount of information reaching the receiver often decreases imperceptably at first and then suddenly plummets as the amount of information increases (given constant noise).  

Given that deep learning models often behave as if they were noisy communication channels, it may be wonderd if the same observation would be made for these models.  Indeed it is found that language model input representation experiences a severe drop as the channel width decreases. For the 7 billion parameter trained Llama with 4096 feature neurons per layer, reducing this number to 2202 yields no incorrect input tokens for the input 'Mario the man versus Mario the idea' but decreasing this by even one neuron (ie taking `[:, :, :2201]` as the output used to perform gradient descent-based input optimization) leads to *no* tokens being correctly represented. The same is true when we consider only the attention layers in modules (no MLPs), where the number of necesary neurons now is 2083.  These observations are not due to the identity of the specific tokens being chosen, but can be thought of as a true bandwidth phenomenon: in the case of the attention-only module taking `[:, :, 1000:2083]` yields no token accurately found but taking `[:, :, 1000:3083]` gives every input token accurately.





## Language Model Features 

### Introduction

How do deep learning models make decisions?  This is a fundamental question for understanding the (often very useful) outputs that these models give.

One way to address this question is to observe that most deep learning models are compositions of linear operations (with nonlinear transformations typically applied element-wise) which means that one can attempt to examine each separable component of a model in relative isolation.  A particularly intuitive question one might ask is what input yields the maximum output for some hidden layer element (assuming other elements of that layer are relatively unchanged) because one may consider that input to be most 'recognizable' to that element.  

Feature visualization studies have shown that vision models learn to identify and generate shapes via a hierarchical sequence of shape speceficity model layer: early convolutional (or transformer) layers are most activated by simple patterns or repeated shapes whereas deeper layers are most activated by specific objects or scenes.  For much more information on this topic, see [this page](https://blbadger.github.io/feature-visualization.html).  

### Background

The fundamental challenge of applying input gradient descent or ascent methods is that language model inputs are discrete rather than continuous.  Therefore either some transformation must be made on the input in order to transform it into a continuous and differentiable space or else some other value must be substituted. A logical choice for this other value is to use the embedding of the inputs, as each input has a unique embedding that is continuous and differentiable.  

Therefore we will observe the features present in language models by performing gradient descent on the embedding of the input.

$$
e_{n + 1} = e_n - \eta \nabla_{e_n}|| c - O(e_n, \theta) ||_1
$$

This procedure was considered by previously by [Poerner and colleages](https://aclanthology.org/W18-5437/) but thought to yield generated embeddings $e_g$ that were not sufficiently similar to real word embeddings as judged by cosine distance, which is simply the cosine of the angle between angles of points in space.  For vectors $a$ and $b$

$$
a \cdot b = ||a ||_2 *||b||_2 \cos \phi \\
\cos \phi = \frac{a \cdot b}{|a ||_2 *||b||_2}
$$

It is not clear that this is actually the case for large language models, however. This is because the value of cosine distance is extraordinarily dependent on the number of parameters of $a$ and $b$, with higher-dimensional vectors yielding smaller cosine distances. When $\cos \theta$ of the embedding of a trained Llama 7b is measured between an $e_g$ that most closely matches 'calling' we find that this value is much larger than the cosine distance between embeddings of 'calling' and 'called'.

Secondly, [elsewhere](https://blbadger.github.io/language-discreteness.html) we have already seen that gradient descent on a language model embedding is capable of recovering a text input that exactly matches some given target. If $e_g$ in that case did not sufficiently resemble real inputs this procedure would have a vanishingly low probability of success.

With this in mind, we can go about observing what inputs activate each neuron in various layers of language models.  We will consider the outputs of transformer-based language models that are sometimes described as having a shape $[batch, \; sequence, \; feature]$.  We will generate only 1-element batches, so on this page any output of shape $[:, n, m]$ is equivalent to the ouptut $[0, n, m]$ where $[:]$ indicates all of the elements of a given tensor index.

The following figure provides more detail into what exactly we are considering a 'feature'.  Each 'feature' is essentially a single neuron's activation of the output of a transformer block (although it could also be assigned as the output of a transformer's self-attention module as this is the same shape).  

![llm_features](https://blbadger.github.io/deep-learning/llm_features_explained.png)

To find an input $a_g$ that represents each 'feature' of output layer $O^l$, we want to find an input $a_g$ that maximized the activation of some subset of that output layer for the model $\theta,

$$
a_g = \underset{a}{\mathrm{arg \; max}} \; O^l_f(a, \theta)
$$

with the reasoning that the input $a_g$ gives a large output in the feature $O^l_f$ and therefore is most emblematic of the sort of input that this feature recognizes in real inputs. 

Language inputs are fundamentally discrete, and so we cannot simply perform gradient descent on the input as was done for vision models. We can, however, perform gradient descent on the embedding of an input, which is defined as the matrix multiplication of the input $a$ and an embedding weight $e$. For language models the input is typically a series of integers which is converted into a vector space embedding via a specialized weight matrix $W$.  We will denote this mapping as follows:

$$
e = Wa
$$

and ignore the fact that $a$ cannot be multiplied directly to a weight matrix as it is not a vector for now. 

We can use gradient descent on the input's embedding where the objective function is a distance metric between a tensor of the same shape as $O^l_f$ comprised of some large constant $C$ and the output values as follows.  Here we choose an $L^1$ metric on the output.

$$
e_{n+1} = e_n + \eta \nabla_{e_n} (||C - O^l_f(e_n, \theta)||_1)
$$

After $N$ iterations we generate $e_g$, and recover the input which maps to this embedding using the Moore-Penrose pseudoinverse

$$
a_g = W^+e_g
$$

where $a_g$ is a vector that can be converted to a series of tokens by taking the maximum value of each sequence element in $a_g$.

### Llama features are aligned across different-sized models

Before examining which parts of a language model respond to what input, it is helpful to recall what we learned from the same question applied to vision models.  For both [convolutional](https://blbadger.github.io/feature-visualization.html) as well as [transformer](https://blbadger.github.io/transformer-features.html) -based vision models, the main finding was that shallow layers (near the input) learn features that detect simple patterns like repeated lines or checkers, whereas deeper layers learn to identify features that are much more complex (an animal's face, for instance, or a more complicated and irregular pattern).  

We focus here on the Llama family of models introduced by [Touvron and colleages](https://arxiv.org/abs/2302.13971), which are transformer decoder-based causal language models (meaning that these models are trained to predict the next token given a series of previous tokens).

The shallowest Llama modules yield similar results as what was observed for vision models: maximizing the output of any given output neuron for all tokens (here limited to five tokens total) results in a string of identical words. For example, the feature maps of first three neurons of the first Llama transformer (7 billion parameter version) block ($O^l = 1$ if one-indexed)

$$
O_f = [:, \; :, \; 0]  \\
a_g = \mathtt{<unk><unk><unk><unk><unk>}
$$

$$
O_f = [:, \; :, \; 1] \\
a_g = \mathtt{<s><s><s><s><s>}
$$

$$
O_f = [:, \; :, \; 2] \\
a_g = \mathtt{</s></s></s></s></s>}
$$

Here we will use a shorthanded notation for multiple features: $0-4$ for instance indicates features 0, 1, 2, 3, 4 (inclusive) in succession.

$$
O_f = [:, \; :, \; 2000-2004] \\
\mathtt{called \; called \; called \; called \; called} \\
\mathtt{ItemItemItemItemItem} \\
\mathtt{urauraurauraura} \\
\mathtt{vecvecvecvecvec} \\
\mathtt{emeemeemeemeeme}
$$

This is very similar to what is observed for vision transformers: there, the feature maps of single neurons (ie with the same shape as is observed here) in shallow layers yield simple repeated patterns in which the input corresponding to each token is more or less identical ([see here](https://blbadger.github.io/transformer-features.html).

If we observe feature maps from a single neuron at different tokens in the first module of Llama 7b, we find that the word.

$$
O_f = [:, 0 - 4, 2000] \\
\mathtt{\color{red}{called}\; Iger \; Alsolass} \\
\mathtt{are \; \color{red}{called}ger \; Alsolass} \\
\mathtt{are \; I \; \color{red}{called} \; Alsolass} \\
\mathtt{are \; Iger \; \color{red}{called}lass} \\
\mathtt{are \; Iger \; Also \; \color{red}{called}} \\
$$

When we combine features, somewhat unpredictable outputs are formed.  For example, optimizing an input for the first four features (denoted `0:4`, note that this is non-inclusive) yields

$$
O_f = [:, \; :, \; 0:4] \\
a_g = </s><unk><s><s><unk>
$$

and four different features combined give

$$
O_f[:, \; :, \; 2000:2004] \\
a_g = \mathtt{vec \; calledura \; calledvec}
$$

This is all to be expected given what was learned from features in vision models. It comes as some surprise therefore to find that different-sized Llama models have nearly identical features for the dimensions they share in common.  For example, if we observe the features of the 13 billion or 30 billion parameter versions of Llama, we find exactly the same features that were present for the 7 billion parameter Llama.  Note that the 13 billion Llama was trained on the same text as the 7 billion Llama, but Llama 30b was trained on more text and thus was exposed to different training data.

```
Transformer Block 1
=================================
Llama 13b [:, :, 2000-2004]
called called called called called
ItemItemItemItemItem
urauraurauraura
vecvecvecvecvec
emeemeemeemeeme

Llama 30b [:, :, 2000-2004]
called called called called called
ItemItemItemItemItem
urauraurauraura
vecvecvecvecvec
emeemeemeemeeme
```

It is worth considering how different this is compared to transformer-based models that are designed for vision tasks. For vision transformers, there is little similarity between identical features for different-sized models. The following figure shows models that were trained on an identical dataset that differ by size only, with each grid location showing the feature map of a certain feature in both models.

![vit feature comparison](https://blbadger.github.io/deep-learning/vit_b_vs_l_features.png)

###  Llama features are aligned across layers

For any given vision transformer neuron, these features are typically very different between different layers. Subsequent layers may have similar features but it is far from the case that every feature in each layer is identifiable from the last layer. In the figure below, it is clear that the features of the first four transformer blocks (where forward- and back-propegation occurs through all previous blocks to the input embedding) are generally distinct.

![vit feature comparison](https://blbadger.github.io/deep-learning/vit_b_32_layers_features.png)

This is in contrast to what is observed for language models, where the features of many outputs are identical. For example, given the 7 billion parameter version of Llama we have the following feature maps at the denoted layer (forward and back-propegation proceeding through all blocks prior)

```
Block 4
[:, :, 2000-2004]
called called called called called
ItemItemItemItemItem
urauraurauraura
vecvecvecvecvec
emeemeemeemeeme

Block 8
called called called called called
ItemItemItemItemItem
urauraurauraura
vecvecvecvecvec
emeemeemeemeeme
```

which is identical to what was seen at block 1.  Deeper layer features are not, however, exactly identical to what is seen for shallower layers: observing the inputs corresponding to may features together at different tokens, we find that an increase in depth leads to substantially different maps for the 13 billion parameter version of Llama.

```
block 1
[:, 0-2, :]
ther year return
therige year return
ther year return

block 4
[:, 0-2, :]
tamb would ]{da
column tamb mar marity
cal would tambIONils

block 8
[:, 0-2, :]
ports mar mar user more
endports mar user
tamb marports Elie

block 12
[:, 0-2, :]
ports mar tamb El mar
ports mar c
tamb marportsiche mar
```

It may also be wondered what the features of each layer (without all preceeding layers) look like.  We see the same identical input representations in most layers of Llama 13b (or Llama 7b or 30b for that matter).  For example, for the 32nd transformer block of this model we see the same features as those we saw for the 1st transformer block.

```
O_f = [:, :, 2000-2003]
called called called called called
ItemItemItemItemItem
urauraurauraura
vecvecvecvecvec
```

This is also not found in vision transformers: although certain features tend to be more or less identical across many transformer blocks the majority are not.

![vit feature comparison](https://blbadger.github.io/deep-learning/vit_b_32_singlelayers_features.png)


### Trained Llama features closely resemble untrained ones

Even more surprising is that an untrained Llama 7b would have identical features to the trained model.

```
Block 1
==================================
O_f = [:, :, 0-2]
<unk><unk><unk><unk><unk>
<s><s><s><s><s>
</s></s></s></s></s>

[:, :, 2000-2004]
called called called called called
ItemItemItemItemItem
urauraurauraura
vecvecvecvecvec
emeemeemeemeeme
```

When a single feature is optimized at different tokens, we find that as for the trained Llama model the token corresponding to the self token

```
[:, 0-2, 2000]
calledremger partlass
are calleddata Alsolass
are I calledamplelass
are Idata calledlass
are Idataample called

Block 4
[:, :, 2000]
are calleddata called met
Item larItemItem met
uraFieldurauraura
vecvecvec reallypha

[:, 0-2, :]
ypearchrodyause
ci helriesi
haromenIndoz s
```

### Heterogeneous features in a large Llama

So far we have seen that features are remarkably well-aligned in Llama-based language models.  But when we investigate the larger versions of Llama more closely, we find that 

```
Llama 30b
Block 1
[:, :, 2000-2002]
called called called called called
ItemItemItemItemItem
urauraurauraura

Block 4
[:, :, 2000-2002]
called called called called called
ItemItemItemItemItem
urauraurauraura

Block 32 
[:, :, 2000-2002]
called planlearason current
******** doesn())ason current
ura statlearason current
```

We also find that features of multiple neurons are quite different when comparing Llama 30b to Llama 13b,

```
block 4
Llama 30b
[:, 0-2, :]
osseringering How
oss yearsering yearsoss
estooss yearsoss

Llama 13b
[:, 0-2, :]
tamb would ]{da
column tamb mar marity
cal would tambIONils
```

### Complete Search Features

It may be wondered whether or not the feature generation method employed on this page is at all accurate, or whether the gradient descent-based generated inputs are unlike any real arrangement of tokens a model would actually observe. 

Because we are working with fixed-vocabulary language models, we can go about testing this question by performing a complete search on the input: simply passing each possible input to a model and observing which input yields the largest output for a particular output neuron will suffice.  We will stick with inputs of one token only, as the time necessary to compute model outputs from all possible inputs grows exponentially with input length (for GPT-2 for $n$ tokens there are $50257^n$ possible inputs).

Specifically, we want the input index $i$ that maximizes the output of our chosen output layer's feature $O^l_f$.

$$
a_i = \underset{a}{\mathrm{arg \; max}} \; O^l_f(a, \theta)
$$

We begin with the smallest version of GPT-2, which contains 117m parameters.  By batching inputs, we can iterate through every possible input in around two seconds on a T4 GPU.  On a trained model, we have for the first four features $[:,\; :,\; 0-4]$

|             | [:,:, 0]    | [:, :, 1] | [:, :, 2] | [:, :,  3] |
|-------------|-------------|-----------|-----------|------------|
| **Block 1** | Pwr         | Claud     | Peb       | View       |
| **Block 2** | ItemTracker | watched   | Peb       | (@         |
| **Block 4** | ItemTracker | watched   | Peb       | (@         |
| **Block 8** | ItemTracker | watched   | Peb       | (@         |

Complete search finds different tokens than gradient descent on the input or on the embedding. For example, performing gradient descent on $\nabla_e \vert \vert O(e_n, \theta) - O(e, \theta) \vert \vert_1$ on the same output yields

```
for, im, G,$
```

or cosine distance of the angle $\phi$ between vectorized versions of the output $O(e_n, \theta)$ and the target output $o(e, \theta)$  (ie $\nabla_e \cos(\phi)$) we have

```
!, ", qu, $
```

It is notable that we come to the same conclusion as with gradient-descent based feature visualization: the features of each neuron at a given index are aligned across a range of layers. GPT-2 medium (355m parameters) we find the same $[:,\; :,\; 0-4]$

|              | [:,:, 0] | [:, :, 1] | [:, :, 2] | [:, :,  3]  |
|--------------|----------|-----------|-----------|-------------|
| **Block 1**  |  Ender   | pmwiki    | Cass      | catentry    |
| **Block 4**  | refres   | Flavoring | carbohyd  | assetsadobe |
| **Block 8**  | refres   | Flavoring | carbohyd  | assetsadobe |
| **Block 12** | refres   | Flavoring | carbohyd  | assetsadobe |

This alignment of features across many layers is the result of training and is not present in randomly-initialized models: if we explore the features corresponding to $O_f = [:,\; :,\; 0-4]$ of an untrained 117m parameter GPT-2, we find

|              | [:,:, 0]      | [:, :, 1]   | [:, :, 2] | [:, :,  3]  |
|--------------|---------------|-------------|-----------|-------------|
| **Block 1**  | Yan           | criticizing | policy    |  Pont       |
| **Block 4**  | inski         | Giants      | consuming | Bert        |
| **Block 8**  |  distruptions | erie        |  N        |  Aut        |
| **Block 12** |  video        |  collection |  wipe     | EngineDebug |

which is what one would expect if the activation of any given neuron (at a specific layer) was random with respect to the input. 

### Gradient descent with priors

As the complete search method becomes computationally infeasible for larger inputs, we instead use gradient descent. Ideally want there to be a match between gradient descent-based feature visualization and complete search for short inputs, but we have seen that this is not the case.

Recalling what was learned for [vision model feature visualization](https://blbadger.github.io/feature-visualization.html), it is natural to wonder whether enforcing some Bayesian prior on the gradient descent process.  In the context of vision models applied to images of the natural world, feature visualization commonly adds priors of positional invariance (by re-scaling the input during generation) and local smoothness (by performing Gaussian convolution).

What priors should be enforced on a language model input? So far on this page we have explored performing gradient descent on vector-space embeddings of discrete inputs (which naturally cannot be used for gradient descent without conversion to a continuous vector space of some kind).  It is unclear if there are any universal priors to enforce on a language model embedding, but there is certainly a distribution-based prior one could enforce on an input that was more directly generated.  This is because if we (pseudo)invert the embedding transformation $E$ (which is $E^+ = (E^TE)^{-1}E^T$ and may be thought of as the inverse of $E$ that minimizes the $L^2$ metric on the solution vector $\vert \vert a^* \vert \vert_2$ for the equation $a^* = E^+y$. With our output $y = E(a)$, this is equivalent to

$$
a^* = E^+ \left( E(a) \right)
$$

Typically we find that $a^*$ is approximately a one-hot vector, which signifies a vector with one entry non-zero (typically one) and all others identically zero ($[0, 0, 1, 0]$ for example).  Any vector with non-identical elements may be transformed into a one-hot vector by simply assigning the value one to the maximum value of the vector and zero elsewhere. But we also want an input that may be optimized via gradient descent, and unless one were to assign a very large learning rate to the gradient this direct transformation to a one-hot vector is not useful for our purposes.

Instead we can take an idea from causal language modeling: in order to prevent a function from returning a differentiable vector that differs too much from the range of a one-hot vector. To prevent the generated input $a_g$ from diverging too much from $a''$ we can apply a softmax transformation during every iteration of gradient descent to make

$$
a_{n+1} = \mathrm{softmax\;} \left(a_n - \nabla_a ||O(a_n, \theta) - O(a, \theta)|| \right)
$$

When this method is applied to the 117m parameter GPT-2, we find that the input tokens do not correspond to those found via complete search, as instead we have:

|         | [:, :, 0] | [:, :, 1] | [:, :, 2] | [:, :, 3] |
|---------|-----------|-----------|-----------|-----------|
| **Block 1** | cris      | Cola      | Cass      | Holt      |
| **Block 4** | Geh       | 20439     | og        |           |
| **Block 8** | Geh       | 20439     | expend    | assetsadobe   |


### Greedy Search Features

In light of the difficulties are presented by gradient-based feature visualization, we return to take a second look at the complete search method introduced above.  We can make an observation that causal language models (which only infer the next word in a sentence, and are the type of model investigated on this page) exclusively read inputs from left to right (as long as they are trained on English and romance languages, for example, rather than languages like Hebrew). This suggests that when building an input that maximizes a model's feature, we can build it exclusively left to right as well. 

We focus on maximizing a given feature of the hidden layer's last sequence output only (rather than maximizing the feature over all sequence elements) and iteratively build the input by adding the token yielding maximal output of this feature to the end of the input, for a pre-determined number of iterations $N$ (equaling the token number).  The reason for this is that maximizing each last token only yields the maximal output if each maximal output is chosen per iteration, ie we undergo a greedy search. 

To be precise, for $n \in (0, 1, ...., N)$ we find $a_n$ by 

$$
a_n = \underset{a_n}{\mathrm{arg \; max}} \; O^l_{(f, n)}( a_n | a_{0:n-1}, \theta)
$$

We can further improve the computational efficiency of this greedy approach by batching together many inputs and feeding them to the model simultaneously.  This can be implemented as follows:

```python
@torch.no_grad()
def search_maximal(n_tokens, feature, batch_size=1000):
    vocab_size = model.transformer.wte.weight.shape[0]
    maximal_tokens = []
    for t in range(n_tokens):
        activations = []
        for i in range(0, vocab_size, batch_size):
            token_batch = torch.tensor([j for j in range(i, min(i + batch_size, vocab_size))])
            token_batch = token_batch.reshape((min(batch_size, vocab_size - i), 1)).to(device)
            maximal_tensor = torch.tensor(maximal_tokens).repeat(min(batch_size, vocab_size - i), 1).to(device)
            if t > 0:
                greedy_tokens = torch.cat((maximal_tensor, token_batch), dim=1).int()
            else:
                greedy_tokens = token_batch
            output = a_model(greedy_tokens)
            activations += output[:, -1, feature].flatten()

        activations = torch.tensor(activations)
        token_int = torch.argmax(activations)
        maximal_tokens.append(token_int)

    tokens = tokenizer.decode(maximal_tokens)
    return tokens
```

For the 117m parameter version of GPT-2, we have the following for $N=4$

```
Trained GPT-2 Base
[:, :, 0-3]
Block 1 
 Pwr PwrItemTrackerItemTracker
 Claud Lara Lara Lara
 Peb Peb Peb Peb
Viewtnctnctnc

Block 4 
ItemTracker interf interfItemTracker
watched watched watched watched
Peb Peb Peb Peb
 (@"></ guiName"></

Block 8 
ItemTracker interf interf interf
watched watched watched watched
 Peb Peb Peb Peb
(@ guiName"></ guiName

Block 12 
ItemTracker interf interf interf
 hostages herpes herpes herpes
 Kenn Peb Peb Peb
(@rawdownloadcloneembedreportprintrawdownloadcloneembedreportprintrawdownloadcloneembedreportprint
```

Once again we observe that the alignment of features between layers is a learned phenomenon, as the untrained 117m parameter GPT-2 yields inputs with no correlation between layers for identical features (see the following figure). 

```
Untrained GPT-2 Base
[:, :, 0-3]

Block 1
Officers ii Denis variability
onductASE Media tissue
 Comp accusationShiftShift
 separated activekef patented

Block 4
 Abortion contention variability variability
 one 185 (" Ic
 coin Foss multiplied multiplied
 Ae archetype faded faded

Block 8
 Preferencesstrip Installation Installation
 one logosAmy sheet
coin tiles unique backstory
 active MDMA incentiv thirst
```

It should be noted, however, that this alignment is not quite as absolute as was observed for gradient descent-based methods or for 1-token length complete search.  It is evident that GPT-2 model transformer blocks exhibit more closely aligned features than those in vision transformers, where it is usually not possible to determine features in block $n-1$ features given features from block $n$.

To see that the greedy approach using only the last sequence element's activation is equivalent to the greedy approach using all sequence activations, we can modify the algorithm as follows:

```python
def search_maximal(n_tokens, feature, batch_size=1000):
    ...
        output = a_model(greedy_tokens)
        focus = output[:, :, feature]
        aggregated_focus = torch.sum(focus, dim=-1)
```

and repeating the experiment above, we find that the same inputs are generated.  This is because language model transformer blocks only observe tokens to the left of a given sequence index (ie the third transformer block sequence element observes tokens 0, 1, 2, and 3 but not 4).  Therefore as only the last token is chosen, only the last transformer block sequence feature determines this token.

It is interesting to note that other models exhibit less alignment in their features after training: for example, for a trained Llama 7b we have for features `[:, :, 0-3]` in the first dozen transformer blocks (this model has 32 total)

```
Block 1
Hein Hein mang Hein
cyl szere Woj cyl
inf inf char inf
travers travers assim feb

Block 4
</s></s>
cyluvud Externe
character postgresql mysqliAppData
</s>Q</s>l

Block 8
</s></s>
cyllista Which Peru
</s></s></s></s>
</s>Q</s>Q

Block 12
</s></s>
cyl |
characterparameters\ \
</s>).
```

Observe that the first token in each block is identical for each feature, but that the following tokens are typically not the same among different blocks.  For deeper layers, this is no longer the case and an arbitrary feature in a certain block is typically non-identifiable from the same feature in previous blocks.

```
Block 16
1!--pusugo
 dataframe Broad Mediabestanden
</s> rp  
</s>ktiv

Block 20
</s> thor webpack winter
 Cs That roughly
 .= Repub
 traject traject traject

Block 24
</s>CURCURCUR
aussianclar circuit
</s> rp  
1) traject/
```

### Implications of feature alignment

The finding that features are at least partially aligned, particularly between layers of one model or of similar models, is remarkable for a number of reasons. Firstly, the alignment is much stronger than what is seen for vision models, even vision transformers that mirror the decoder-only transformer models used for language modeling.  Secondly, there is little theoretical basis for why different transformer modules would have such well-aligned features, particularly those that are defined on gradient-descent based feature visualization methods.

But perhaps most usefully this observation of feature alignment suggests that one can easily combine layers (ie transformer block modules or even individual linear transformation layers) from various different language models to make combination models. The reason for this is that different layers from different models are more likely to have new information than different layers from the same model, being that many models have layers with quite repetitive features, and may not contain as much new information as would be beneficial to a language tasks.

This is particularly interesting in light of the findings that large language models contain parameters that are approximately normal with a few exceptions that are large outliers among practically all layers.  Compressing each layer has proven difficult without resorting to parameter quantization methods, but removing and adding entire layers without further modification has not been a well-researched route.

On the other hand, there exists the beginnings of the sort of pure model merging detailed above already (meaning where one takes layers from various models and merges them together if of the same model family and size, without any additional transformations between layers) to make a single model (sometimes called a 'Frankenmodel'). For example the [Goliath 120b](https://huggingface.co/alpindale/goliath-120b) model is a direct merge two Llama 70b variants, and in the hands of the author is much superiour to instruction-tuned Llama-70b when it comes to factual recall, reasoning, and problem solving tasks. 




### Sentence Representation with Language Models

In the previous section we have seen that a trained language model is less capable of representing a visual input than a trained language model (both with similar transformer architectures).  Given the nature of the inputs each model type is trained on, this may not seem very unexpected.  It is more informative to consider the ability of language model layer outputs to represent language inputs, rather than images. More accurate representation implies more input information passing through the model to the chosen layer's output.

To orient ourselves, first consider the architecture of a typical transformer-based language model.

![gpt2 representation]({{https://blbadger.github.io}}/deep-learning/llm_representation_explained.png)

Language input generation presents a unique challenge to gradient-based optimization methods because language inputs are fundamentally discrete: a word either exists in a certain part of a sentence or it does not.  This is a notable contrast to images of the natural world, where each element (ie pixel) may take any value between 0 and the maximum possible intensity of that image (which is 255. for 8-bit images) and therefore are much better approximated by continuous values.

The standard approach to observing the input information present for some vision model layer is to start with a random normal input $a_0 = \mathcal{N}(a, \mu=1/2, \sigma=1/20)$ and then perform gradient descent on some metric (here $L^1$) distance between the target output $O_l(a, \theta)$ for $N$ total iterations, each step being

$$
a_{n+1} = a_n - \eta * \nabla_{a_n} ||O_l(a_n, \theta) - O_l(a, \theta)||_1 \\
\tag{1}\label{eq1}
$$

with $\eta$ decreasing linearly from $\eta$ to $\eta / 10$ as $n \to N$ which empirically results in the fastest optimization.  The more information of the input that is retained at that layer, the smaller the value of $\vert \vert a_N - a \vert \vert$.

This gradient descent method is not useful for language models without some modifications, given that $\nabla_{a_n}$ is undefined for discrete inputs which for language models are typically integer tokens.  Instead we must perform gradient descent on some continuous quantity and then convert to and from tokens.  

There are two ways that we can perform gradient descent on tokenized inputs: the first is to first convert the tokens to a lower-dimensional (and continuous) embedding and then perform gradient descent on that embedding, and the other is to convert to the tokens into a $d$-dimensional vector space and modify the model to perform gradient descent such that this vector space is the input (thus the leaf node of the gradient backpropegation computational graph).  We will take the first strategy for this section and only later revisit the second.

For large language models such as GPT-2, the process of conversion between a discrete token and a continuous embedding occurs using a word-token embedding, which is programmed as a fully connected layer without biases but is equivalent to a (usually full-rank) matrix multiplication of the input token vector $x$ and the embedding weight matrix $W$ to obtain the embedding vector $e$.

$$
e = Wa
$$

As $e$ is continuous, we can perform gradient descent on this vector such that $e_g$ may be generated from an initially random input $e_0 = \mathcal{N}(e, \mu=1/2, \sigma=1/20)$.

But we then need a way to convert $e_g$ to an input $a_g$. $W$ is usually a non-square matrix given that word encodings often convert inputs with the number of tokens as $n(a) = 50257$ to embeddings of size $n(e) = 768$.  We cannot therefore simply perform a matrix inversion on $W$ to recover $a_g = W^{-1}(e_g)$ because there are fewer output elements than input elements such that there are an infinite number of possible vectors $a_g$ that could yield $e_g$. 

Instead we can use a generalized inverse, also known as the Moore-Pensore pseudo-inverse.  The psuedo-inverse of $W$ is denoted $W^+$, and is defined as

$$
W^+ = \lim_{\alpha \to 0^+} (W^T W + \alpha I)^{-1} W^T
$$

which is the limit from above as $\alpha$ approaches zero of the inverse of $W^T W$ multiplied by $W^T$.  A more understandable definition of $W^+$ for the case where $W$ has many possible inverses (which is the case for our embedding weight matrix or any other transformation with fewer output elements than input elements) is that $W^+$ provides the solution to $y = Wa$, ie $a = W^+y$, such t hat the $L^2$ norm $\vert \vert a \vert \vert_2$ is minimized.  The pseudo-inverse may be conveniently calculated from the singular value decomposition $W = UDV^T$

$$
W^+ = VD^+U^T
$$

where $D^+$ is simply the transpose of the singular value decomposition diagonal matrix $D$ with all nonzero (diagonal) entries being the reciprocal of the corresponding element in $D$.

Therefore we can instead perform gradient descent on an initially random embedding $e_0 = \mathcal{N}(e, \mu=1/2, \sigma=1/20)$ using

$$
e_{n+1} = e_n - \eta * \nabla_{e_n} ||O_l(e_n, \theta) - O_l(e, \theta)||_1 \\
\tag{2}\label{eq2}
$$

and then recover the generated input $a_g$ from the final embedding $e_g = e_N$ by multiplying this embedding by the pseudo-inverse of the embedding weight matrix $W$,

$$
a_g = W^+e_N
$$

where each token of $a_g$ correspond to the index of maximum activation of this $50257$-dimensional vector.

Thus we can make use of the pseudo-inverse to convert an embedding back into an input token. To see how this can be done with an untrained implementation of GPT-2, the model and tokenizer may be obtained as follows:

```python
import torch
from transformers import GPT2Config, GPT2LMHeadModel

configuration = GPT2Config()
model = GPT2LMHeadModel(configuration)
tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'gpt2')
```

Suppose we were given a prompt and a tokenizer to transform this prompt into a tensor corresponding to the tokens of each word. 

```python
prompt = 'The sky is blue.'
tokens = tokenizer.encode(
	  prompt,
	  add_special_tokens=False,
	  return_tensors='pt',
	  max_length = 512,
	  truncation=False,
	  padding=False
	  ).to(device)
```

For GPT-2 and other transformer-based language models, the total input embedding fed to the model $e_t$ is the addition of the word input embedding $e$ added to a positional encoding $e_p$

```python
model = model.to(device)
embedding = model.transformer.wte(tokens) 

position_ids = torch.tensor([i for i in range(len(tokens))]).to(device)
positional_embedding = model.transformer.wpe(position_ids)
embedding += positional_embedding
```

The positional weight matrix is invariant for any given input length and thus may be added and subtracted from the input embedding so we do not have to solve for this quantity.  Therefore given the `embedding` variable, we can generate the input tokens by first subtracting the positional embedding $e_p$ from the generated embedding $e_N$ and multiplying the resulting vector by the pseudo-inverse of $W$ as follows:

$$
a_g = \mathrm{arg \; max \;} W^+(e_N - e_p) \\
\tag{3}\label{eq3}
$$

and this may be implemented as

```python
embedding_weight = model.transformer.wte.weight
inverse_embedding = torch.linalg.pinv(embedding_weight)
logits = torch.matmul(embedding - positional_embedding, inverse_embedding)
tokens = torch.argmax(logits, dim=2)[0]
```

It may be verified that Equation \eqref{eq3} is indeed capable of recovering the input token given an embedding by simply encoding any given sentence, converting this encoding to an embedding and then inverting the embedding to recover the input encoding.

Before investigating the representations present in a large and non-invertible model such as GPT-2, we can first observe whether a small and invertible model is capable of accurate input representation (from its output layer). If the gradient descent procedure is sufficiently powerful, we would expect for any input sentence to be able to be generated exactly from pure noise.

The following model takes as input an embedding tensor with `hidden_dim` dimension with the number of tokens being `input_length` and is invertible.  This MLP is similar to the transformer MLP, except without a residual connection and normalization and with equal to or more output elements as there are input elements for each layer (which means that the model is invertible assuming that the GeLU transformation does not zero out many inputs).

```python
class FCNet(nn.Module):

	def __init__(self, input_length=5, hidden_dim=768):
		super().__init__()
		self.input = nn.Linear(input_length * hidden_dim, 4 * input_length * hidden_dim)
		self.h2h = nn.Linear(4 * input_length * hidden_dim, 4 * input_length * hidden_dim)
		self.input_length = input_length
		self.gelu = nn.GELU()
	
	def forward(self, x):
		x = x.flatten()
		x = self.input(x)
		x = self.gelu(x)

		x = self.h2h(x)
		return x
```

Given the target input 

$$
\mathtt{This \; is \; a \; prompt \; sentence.}
$$

and starting with Gaussian noise as $a_0$, after a few dozen iterations of gradient descent on the output of the model on the target versus the noise we have we have the following:

$$
a_{20} = \mathtt{guiActiveUn \; millenniosyn \; CosponsorsDownloadha} \\
a_{40} = \mathtt{\; this \; millenna \;  Cosponsors.} \\
a_{45} = \mathtt{\; this \; millenn \; a \; prompt \; Cosponsors.} \\
a_{50} = \mathtt{ \; this \; is \; a \; prompt  \; sentence.} \\
a_{60} = \mathtt{This \; is \; a \; prompt\; sentence.}
$$

meaning that our small fully connected model has been successfully inverted.

It may be wondered if a non-invertible model (containing one or more layer transformations that are non-invertible) would be capable of exactly representing the input.  After all, the transformer MLP is non-invertible as it is four times smaller than the middle layer. If we change the last layer of our small MLP to have `input_length * hidden_dim` elements, we find that the generated inputs $a_g$ are no longer typically exact copies of the target.

$$
\mathtt{\; this \; millenn \; charism \; sentence.} \\
\mathtt{\; this \; adolesc \; a \; prompt \; sentence.}
$$

These representations yield the same next character output for a trained GPT-2, indicating that they are considered to be nearly the same as the target input with respect to that model as well.

To get an idea of how effective our gradient procedure is in terms of metric distances, we can construct a shifted input $e'$ as follows

$$
e' = e + \mathcal{N}(e, \mu=0, \sigma=1/20).
$$

Feeding $e'$ into a trained GPT-2 typically results in no change to the decoded GPT-2 output which is an indication that this is an effectively small change on the input. Therefore we can use 

$$
m = || O_l(e', \theta) - O_l(e, \theta)||_1
$$

as an estimate for 'close' to $O_l(e, \theta)$ we should try to make $O_l(e_g, \theta)$. For the first transformer block (followed by the language modeling head) of GPT, we can see that after 100 iterations of \eqref{eq2} we have a representation

$$
\mathtt{Engine \; casino \; ozlf \; Territ}
$$

with the distance of output of this representation for the one-block GPT-2 $O_l(e_g, \theta)$ to the target output

$$
m_g = || O_l(e_g, \theta) - O_l(e, \theta)||_1
$$

such that $m_g < m$.

### Language models translate nonsense into sense

To recap the last section, on this page we will observe the information present in each layer of a given language model by performing gradient descent in embedding space before converting the generated embedding back to tokens.  The following figure summarizes how this works for a typical transformer-based model.

![gpt2 representation]({{https://blbadger.github.io}}/deep-learning/llm_indirect_representation.png)

One of the primary challenges of large language models today is their ability to generate text that is gramatically and stylistically accurate to the prompt but is inaccurate in some other way, either introducing incorrect information about a topic or else veering off in an unintended direction.  

It can be shown, however, that these models are capable of a much more extreme translation from input nonsense into some real language output by making use the the input representations we have generated in the previous section. Suppose one were given the following prompt: 

$$
\mathtt{The \; sky \; is}
$$

Feeding this input into a trained GPT-2, we get the very reasonable $\mathtt{blue}$ as the predicted next word. This is clearly one of many possible English texts that may yield that same next word to an accurate language model. 

But it can also be shown that one can find many completely nonsensical inputs that also yield an identical output.  We will see this first with an untrained version of GPT-2 that has been tructated to include a certain number (below only one) of transformer blocks followed by the language modeling head.  The language modeling head allows us to obtain the next predicted word for an input into this model, which provides one measure of 'closeness' if our generated sentence has the same next predicted word as the target input.

```python
class AbbreviatedGPT(nn.Module):

	def __init__(self, model):
		super().__init__()
		self.model = model
	
	def forward(self, x: torch.Tensor):
		# choose the block depth
		for i in range(1):
			x = self.model.transformer.h[i](x)[0]

		x = self.model.lm_head(x)
		return x
```

When we generate an input after a few hundred iterations of Equation \eqref{eq2}, passing in the resulting embeddings to be inverted by Equation \eqref{eq3} for the target input

$$
\mathtt{The \; sky \; is \; blue.}
$$

we have

$$
 \mathtt{\; Lime \; Lime  \;is \; blueactly} \\
 \mathtt{\; enessidateidate \; postp.}
$$

If an (untrained) language modeling head is attached to this first transformer block, we find that these two inputs really are viewed essentially equivalently for the purposes of causal language modeling, in the sense that the next predicted token for both is $\mathtt{"}$ (for one particular random initialization for GPT-2). 

If we increase the number of maximum interations $N$ of our gradient descent procedure in \eqref{eq2} we have 

$$
 \mathtt{\; Terr \; sky \; is \; blue.} \\
 \mathtt{\; cyclists \; sky  \; is \; blue.}
$$

And increasing the total iterations $N$ further (to $N \geq 1000$) yields a smaller $L^2$ distance between $a$ and $a_g$ and a greater probability of recovering the original prompt,

$$
\mathtt{The \; sky \; is \; blue.}
$$

although most generated prompts are close but not quite equal to the original for $N = 1000$.  Further increasing $N$ leads to more $a_N$ being equivalent to the target $a$.  As gradient descent is capable of recovering a target input $a$ across one GPT-2 transformer block this block retains most information on the input, and this recovery is somewhat surprising given that the transformer model is not invertible, such that many inputs may yield an identical output.  

With an increase in the number of transformer blocks before the output modeling head, it becomes more difficult to recover the target inptut $a$.  For example, many iterations of Equation \eqref{eq2} a model with untrained GPT-2 blocks 1 and 2 we have a generated prompt of

$$
\mathtt{The \; sky \; is \; tragedies.}
$$

Using the full 12 transformer blocks of an untrained (base) GPT-2, followed by the language modeling head (parameters $N=2000, \eta=0.001$), we can recover inputs that yield the same output token as our original prompt but are completely different.  For example both $a_g$ of

$$
\mathtt{coastline \; DVDs \; isIGHTweak} \\
\mathtt{biologist \; Elephant \; Elephant \; Elephant \; Elephant}
$$

effectively minimize the $L^1$ distance for different initializations of GPT-2, and yield the same next word (bytecode) token as 'The sky is blue.' does.

### Language models become less trainable as they are trained

So far we have only considered input representations from untrained models. It may be wondered what the training process does to the model representational ability, and to do so we will use the same abbreviated model configuration above (with GPT-2 transformer blocks following the input and positional embedding and ending in the language modeling head output).

When performing the input representation procedure detailed above on a trained GPT-2 (importantly with a language modeling head attached), the first thing to note is that the model appears to be very poorly conditioned such that using gradient descent to modify an input to match some output requires careful tuning of $\eta$ and many iterations.  Indeed it takes a truly enormous number of iterations of \eqref{eq2} to generate $e_g$ such that the model's output given $e_g$ is closer to the model's output of $e$ than the slightly shifted input $e'$

 $$
 || O_l(e_g, \theta) - O_l(e, \theta) ||_1 < || O_l(e', \theta) - O_l(e, \theta) ||_1
 \tag{4} \label{eq4}
 $$

on the order to one hundred times as many as for the untrained model to be precise. This very slow minimization of the output loss also occurs when the gradient is calculated on is a different metric, perhaps $L^2$ instead of $L^1$.  A quick check shows that there is no change in the general lack of invertibility of even a single GPT-2 transformer module using this metric.  Thus it is practically infeasible to make the equality in \eqref{eq4} hold if $e'$ is near $e$ (for example, if $e' = e + \mathcal{N}(e, \mu=0, \sigma=1/25$). 

There is a trivial way to satisfy \eqref{eq4} for any $e'$, and this is to simply make the (formerly random) initial embedding $e_0$ to be equal to the target embedding $e$.  Although this is not particularly helpful in understanding the ability of various layers of GPT-2 to represent an input, we can instead make an initial embedding that is a linear combination of random Gaussian noise with the target input.

$$
e_0 = s * \mathcal{N}(e, \mu, \sigma) + t * e
$$

Given an input 

$$
a = \mathtt{This \; is \; a \; prompt \; sentence.}
$$

and with $(s, t) = (0.001, 0.999)$ we have at $N=1000$ an $a_g$ such that \eqref{eq4} holds even for $e'$ very close to $e$.

$$
\mathtt{interstitial \; Skydragon \; a \; behavi.}
$$

Note, however, that even values $(s, t) = (1, 1)$ give an $e_0$ that is capable of being optimized nearly as well as that above, only that more gradient descent iterations are required.  For example, the following generated input achieves similar output distance to the input above.

$$
\mathtt{opioosponsorsnesotauratedcffff \; conduc}
$$

Returning to the general case, it takes an extremely large number of iterations of \eqref{eq2} to approach the inequality \eqref{eq4}, and often it cannot be satisfied in a feasible number of iterations at all.  This observation suggests that the trained GPT-2 model cannot accurately pass gradients from the output to the input.  Why would the gradients arriving at the early layers of a model be inaccurate? It may be wondered if this is due to rounding errors in the backpropegation of gradients. One way to check this is to convert both the model and inputs in question to `torch.double()` type, ie 64-bit rather than the default 32-bit floating point values.  Unfortunately there is no significant change in the number of iterations required to make an input that satisfies \eqref{eq4}, and it remains infeasible to satisfy that inequality for $e'$ very close to $e$.

The relative inability of gradient updates to the input embedding to minimize a loss function on the model output suggests that model layers that are adjacent in the backpropegation computational graph (ie the first few transformer encoders) are also poorly optimized towards the end of training.  Indeed, the poor optimization to the input embedding given only one trained transformer block suggests that most of the model is poorly updated towards the end of training, and that only a few output layers are capable of effective updates at this point.

Is there any way to use gradient descent to more effectively minimize some metric distance between a trained model's output of $a$ versus $a_g$? It turns out that there is: removing the language modeling head reduces the number of iterations required to satisfy \eqref{eq4}, by a factor of $>100$ for a one-block GPT-2 model. This makes it feasible to generate $e_g$ that is accurate even when compared with stricter $e'$ but even these embeddings map to inputs that are more or less completely unrecognizable.

$$
\mathtt{srfAttachPsyNetMessage \; Marketable \; srfAttach \; srfAttachPsyNetMessage}
$$

This result indicates that even when capable of minimizing an $L^1$ metric between $O_l(a, \theta)$ and $O_l(a_g, \theta)$, trained language models still cannot differentiate between gibberish and language.  The ability to satisfy \eqref{eq4} once the language modeling head is removed suggests that this head is a kind of gradient bottleneck such that gradient-based optimization becomes much more difficult across the LM head.

### Approximate Token Mapping

So far we have seen something somwhat unexpected: given some small input token array $a$ we can recover these tokens from untrained but not trained language model transformer blocks.  This indicates that the trained model's decoder blocks (or the entire model) cannot distinguish between gibberish and a normal sentence.  It may be wondered if this is due to the discrete nature of the input: perhaps the $\mathrm{arg \; max}$ of the pseudoinverse of $e_g$ does not find accurate tokens but maybe the second or third highest-activated index could. 

We can select the indicies of the top 5 most activated input token positions as follows:

```python
tokens = torch.topk(logits, 5)[1][0] # indicies of topk of tensor
```

Given

$$
\mathtt{This \; is \; a \; prompt \; sentence.}
$$

For the invertible fully connected network following a token-to-embedding transformation from a trained GPT-2 (as used above) one can see that successive outputs are semantically similar, which is perhaps what one would expect given that a trained embedding would be expected to recover words that mean approximately the same thing (ie 'speedy' is an approximate synonym for 'prompt'). The top five input token strings are

```
This is a prompt sentence.
 this was an Prompt sentences.(
this are the promptssent,
 This has another speedy paragraph.[
 THIS isna prompted Sent."
```

but for a non-invertible fully connected model (with layer dims of $[768, 4*768, 768]$) we find that only the top one or two most activated input tokens are recognizable.  

```
 this is a prompt sentence.
thisModLoader an Prompt sentences.
This corridikumanngthlems.<
 THIS are another guiActiveUnDownloadha Mechdragon
 Thisuberty theetsk recomm.(
```

The non-invertible model results above are not wholly surprising given that the model used was not trained such that equivalent inputs would not be expected to be very semantically similar.  But a model composed of a single trained GPT-2 transformer block (no language modeling head) yields only gibberish as well.
 
 ```
PsyNetMessage MarketablePsyNetMessagePsyNetMessagePsyNetMessage
 srfAttachPsyNetMessagePsyNetMessagequerquequerqueartifacts
ocampartifacts unfocusedRange Marketable
irtualquerqueanwhileizontartifactsquerque
Accessorystaking-+-+igslistawaru
```

But when we compare the input representation of one transformer block of a trained model (above) to input representations for one transformer block for a randomly initialized and untrained model (below) we see something interesting: not only does training remove the ability of the first GPT-2 transformer decoder to accurately recover the input sentence from information in the output of that block, but the words corresponding to the input representation of a trained model are much less likely to exist in a real sentence  than the decoded input representation by an untrained model.  Specifically, observe that `Marketable` is the only word that would be likely to be ever found in real text above, whereas nearly every word below would likely be found given text of sufficient length.

```
This together a prompt sentence.
stock PER alotebra benevolentNBC
 estab hydraulic declaration Dyn highlighted Caucas
Formatorean DAM addressedball
ogeneity shows machine freed erasedcream
```

This observation suggests that training does indeed confer some ability to distinguish between real sentences: all nearly-sentence representations exhibited by the untrained model are no longer found near the target representation of the trained model, and only inputs that have almost zero probability of appearing in text are observed. 


### Orthogonal Walk Representations

For the investigation on this page, the main purpose of forming inputs based on the values of the output of some hidden layer is that the input generated gives us an understanding of what kind of information that hidden layer contains.  Given a tensor corresponding to output activations of a layer, the information provided by that tensor is typically invariant to some changes in the input such that one can transform one possible input representation into another without changing the output values significantly. The collection of all such invariants may be thought of as defining the information contained in a layer.

Therefore one can investigate the information contained in some given layer by observing what can be changed in the input without resulting in a significant change in the output, starting with an input given in the data distribution.  The process of moving along in low-dimensional manifold in higher-dimensional space is commonly referred to a 'manifold walk', and we will explore methods that allow one to perform such a walk in the input space, where the lower-dimensional manifold is defined by the (lower-dimensional) hidden layer.

There are a number of approaches to finding which changes in the input are invariant for some model layer output, and of those using the gradient of the output as the source of information there are what we can call direct and indirect methods.  Direct methods use the gradient of some transformation on the output applied to the input directly, whereas indirect methods transform the gradient appropriately and apply the transformed values to the input.

We will consider an indirect method first before proceeding to a direct one.  

Given the gradient of the output with respect to the input, how would one change the input so as to avoid changing the output?  Recall that the gradient of the layer output with respect to the input,

$$
\nabla_a O_l(a, \theta)
$$

expresses the information of the direction (in $a$ space) of greatest increase in (all values of) $O_l(a, \theta)$ for an infinitesmal change.  We can obtain the gradient of any layer's output with respect to the input by modifying a model to end with that layer before using the following method:

```python
def layer_gradient(model, input_tensor):
	...
	input_tensor.requires_grad = True
	output = a_model(input_tensor)
	loss = torch.sum(output)
	loss.backward()
	gradient = input_tensor.grad
	return gradient, loss.item()
```

If our purpose is to instead avoid changing the layer's output we want what is essentially the opposite of the gradient, which may be thought of as some direction in $a$-space that we can move such that $O_l(a, \theta)$ is *least* changed.  We can unfortunately not use the opposite of the gradient, as this simply tells us the direction of greatest decrease in $O_l(a, \theta)$.  Instead we want a vector that is orthogonal to the gradient, as by definition an infinitesmal change in a direction (there may be many) that is perpendicular to the gradient does not change the output value.

How can we find an orthogonal vector to the gradient?  In particular, how may we find an orthogonal vector to the gradient, which is typically a non-square tensor?  For a single vector $\mathbf{x}$, we can find an orthogonal vector $\mathbf{y}$ by solving for a solution to the equation of the dot product of these vectors, where the desired product is equal to the zero vector.

$$
\mathbf{x} \cdot \mathbf{y} = 0
$$

We can find that trivially setting $\mathbf{y}$ to be the zero vector itself satisfies the equation, and has minimum norm such that simply finding any solution to the above equation is insufficient for our goals. Moreover, language model input are typically matricies composed of many input tokens embedded such that we want to find vectors that are orthogonal to all input token embedding gradients rather than just one.  

To do so, we can make use of the singular value decomposition of the input gradient matrix.  Given a matrix $M$ the singular value decomposition is defined as 

$$
M = U \Sigma V^H
$$

and may be thought of as an extension of the process of eigendecomposition of a matrix into orthonormal bases to matricies that are non-square.  Here the columns of the matrix $U$ is known as the left-singular values of $M$, and the columns of matrix $V$ corresponds to the right-singular values, and $\Sigma$ denotes the singular value matrix that is rectangular diagonal and is analagous to the eigenvalues of an eigendecomposition of a square matrix.  $V^H$ denotes the conjugate transpose of $V$, which for real-valued $V$ is $V^H = V^T$.

The singular value decomposition has a number of applications in linear algebra, but for this page we only need to know that if $M$ is real-valued then $U$ and $V$ are real and orthogonal.  This is useful because for an $M$ that is non-square, we can find the right-singular values $V$ such that $V^H$ is square.  This in turn is useful because some columns (vectors) of $V^H$ are decidedly not orthogonal to $M$ by definition, but as there are more columns in $V^H$ than $M$ we have at least one column that is orthogonal to all columns of $M$. 

Now that a number of orthogonal vectors have been obtained, we can update the embedding $e$ to minimize the change in $O_l(e, \theta)$ by 

$$
e_{n+1} = e_n + \eta * b \left( V^H_{[j+1]} \right)
$$

where $\eta$ is a sufficiently small update parameter and $j$ indicates the number of tokens in $e$ and $b(V^H)$ indicates a broadcasting of $V^H$ such that the resulting tensor has the same shape as $e_n$, and we repeat for a maximum of $n=N$ iterations.  Alternatively, one can assemble a matrix of multiple column vectors of $V^H$ with the same shape as $e$. Empirically there does not appear to be a difference between these two methods.

For example, let's consider the input

$$
\mathtt{The \; sky \; is \; blue.}
$$

This is a 5-token input for GPT-2, where the embedding corresponding to this input is of dimension $[1, 5, 768]$.  Ignoring the first index (minibatch), we have a $5$ by $768$ matrix.  If we perform unrestricted singular value decomposition on this matrix and recover $V^H$, we have a $[768, 768]$ -dimension orthogonal matrix.  As none of the first $5$ columns of $V^H$ are orthogonal to the columns of $M$ we are therefore guaranteed that the next $763$ columns are orthogonal by definition.

The orthogonal vector approach may therefore be implemented as follows:

```python
def tangent_walk(embedding, steps):
	for i in range(steps):
		embedding = embedding.detach()
		gradient, _ = layer_gradient(a_model, embedding) # defined above
		gradient = torch.squeeze(gradient, dim=0)
		perp_vector = torch.linalg.svd(gradient).Vh[-1] # any index greater than input_length
		embedding = embedding + 0.01*perp_vector # learning rate update via automatically broadcasted vector
	
	return embedding
```

where we can check that the SVD gives us sufficiently orthogonal vectors by multiplying the `perp_vector` by `gradient` via 

```python
print (gradient @ perp_vector) # check for orthogonality via mat mult
```

If the value returned from this multiplication is sufficiently near zero, the `perp_vector` is sufficiently orthogonal to the `gradient` vector and should not change the value of $O_l(a, \theta)$ given an infinitesmal change in $a$ along this direction.

Note that gradients are functions from scalars to vectors (or tensors etc.) rather than functions from vectors to other vectors, meaning that we must actually taking the gradient

$$
\sum_i \nabla_a O_l(a, \theta)_i
$$

which may be implemented as follows:

```python
def individual_gradient(model, input_tensor):
	input_tensor.requires_grad = True
	gradient = torch.zeros(input_tensor.shape).double().to(device)
	output = a_model(input_tensor)
	for i in range(len(output)):
		output = a_model(input_tensor) 
		loss = output[i]
		loss.backward()
		gradient += input_tensor.grad
		input_tensor.grad = None

	return gradient
```

But it is much more efficient to calculate the gradient

$$
\nabla_a \sum_i O_l(a, \theta)_i
$$

which may be implemented as

```python
def summed_gradient(model, input_tensor):
	input_tensor.requires_grad = True
	output = a_model(input_tensor)
	sout = torch.sum(output)
	sout.backward()
	gradient = input_tensor.grad.detach().clone()
	return gradient
```

It may be empirically verified that Pytorch evaluates these gradients nearly equally, as the difference between the input gradient as calculated by these methods is on the order of $1 \times 10^{-7}$ or less per element.

Equivalently, the above method is computationally identical to

```python
def layer_gradient(model, input_tensor):
	input_tensor.requires_grad = True
	output = a_model(input_tensor)
	output.backward(gradient=torch.ones_like(output).to(device))
	gradient = input_tensor.grad.detach().clone()
	return gradient
```

This completes the relevant details of the orthogonal walk.  Unfortunately this method is not capable of finding new locations in $a$-space that do not change $O(a, \theta)$ significantly.  This is due to a number of reasons, the most prominent being that for transformer-based models the `perp_vector` is usually not very accurately perpendicular to the `gradient` vector such that multiplication of the orthogonal vector and the gradient returns values on the order of $1 \times 10^{-2}$.  This is an issue of poor conditioning inherent in the transformer's self-attention module leading to numerical errors in the SVD computation, which can be seen by observing that a model with a single transformer encoder yields values on the order of $1 \times 10^{-3}$ whereas a three-layer feedforward model simulating the feedforward layers present in the transformer module yields values on the order of $1 \times 10^{-8}$.

Therefore instead of applying the orthogonal walk approach firectly to the GPT-2 model, we can first apply it to a model architecture that allows the SVD to make accurate orthogonal vectors, the instability of the input gradient landscape makes finite learning rates give significant changes in the output, which we do not want. To gain an understanding for what the problem is, suppose one uses the model architecture mimicking the transformer MLP (with three layers, input and output being the embedding dimension of 768 and the hidden layer 4*768).  Obtaining an orthogonal vector from $V^H$ to each token in $e$, we can multiply this vector by $\nabla_a O_l(a, \theta)$ to verify that it is indeed perpendicular.  A typical value of this multiplication for the attentionless transformer model block is `[ 3.7253e-09, -2.3283e-09,  1.9558e-08, -7.4506e-09,  3.3528e-08]`, indicating that we have indeed found an approximately orthogonal vector to the gradient for all inputs. 

But when we compare the $L^1$ distance metric on the input $d_i =\vert \vert e - e_N \vert \vert_1$ to the same metric on the outputs $d_o =\vert \vert O_l(e, \theta) - O_l(e_N, \theta) \vert \vert_1$ we find that the ratio of output to input distance $d_o / d_i = 2$ when the model used is the three-layer fully connected version.  Even using `torch.float64` precision, there is typically a larger change in the output than the input although the orthogonal vectors multiplied by the columns of $e$ are then typically on the order of $1 \times 10^{-17}$.  After some experimentation, it can be shown that the inefficacy of the orthogonal vector approach is due to the number of elements in the model: for a random initialization of the three-layer MLP for 10 or 50 neurons in the embedding the ratio $d_o / d_i$ is typically $1/2$ or less, but increasing the embedding dimension to $200$ or more leads to the probability of $d_o / d_i < 1$ decreasing substantially.

These results indicate that the orthogonal walk method is not capable of changing the input whilst leaving the output vector unchanged for all except the smallest of models.  Nevertheless, for the untrained 3-layer MLP after a few dozen iterations we have

$$
\mathtt{\; The \; sky \; is \; blue.} \\
\mathtt{\; and \; sky \; is \; blue.} \\
\mathtt{\; and \; sky \; isadvertisement.} \\
\mathtt{advertisement \; skyadvertisementadvertisement.}
$$

Although the orthogonal walk method is not particularly successful when applied to GPT-2, we can nevertheless observe a sequence of generated inputs $a_N$ as $n$ increases.  One again, however, we do not find inputs of semantic similarity to the desired input.

$$
\mathtt{The \; sky \; is \; blue.} \\
\mathtt{confir \; sky \; is \; blue.} \\
\mathtt{\; confir \; behavi \; confir \; confir.}
$$

When `torch.float64` precision is stipulated we find that a much more accurate orthogonal vector may be found, where for any column of the input gradient vector times the right-singular broadcasted vector,  $\mathbf g \cdot \mathbf {b(V^H_{[j+1]})}$, is typically on the order of $1 \times 10^{-17}$.  Therefore with increased numerical precision we find that computation of the SVD allows one to procure an accurate orthogonal vector to even the poorly conditioned input gradient for GPT-2.  But it is remarkable that even for this more accurate orthogonal vector, typically $d_o / d_i > 20$ which means that the orthogonal walk procedure fails spectacularly in finding inputs $a_g$ such that the output is unchanged.

In conclusion, one can walk along a manifold in $a$-space that maps to a unique vector value $O_l(a, \theta)$ by repeated application of a small shift in a direction orthogonal to $\nabla O_l(a, \theta)$, but poor conditioning in this gradient vector means that this method is not capable (at up to 64 bit precision) of an accurate walk (where $O_l(a_n, \theta)$ is nearly identical to the target $O_l(a, \theta)$ for transformer models. 

### Clamped Gradient Walk Representations

Another approach to changing the input while fixing the output is to clamp portions of the input, add some random amount to those clamped portions, and perform gradient descent on the rest of the input in order to minimize a metric distance between the modified and original output.  The gradient we want may be found via

```python
def target_gradient(model, input_tensor, target_output):
	...
	input_tensor.requires_grad = True
	output = a_model(input_tensor)
	loss = torch.sum(torch.abs(output - target_output))
	print (loss.item())
	loss.backward()
	gradient = input_tensor.grad
	return gradient
```

and the clamping procedure may be implemented by choosing a random index on the embedding dimension and clamping the embedding values of all tokens up to that index, shifting those values by adding these to random normally distributed ones $\mathcal{N}(e; 0, \eta)$.  For a random index $i$ chosen between 0 and the number of elements in the embedding $e$ the update rule $f(e) = e_{n+1}$ may be described by the following equation,

$$
f(e_{[:, \; :i]}) = e_{[:, \; :i]} + \eta * \mathcal{N}(e_{[:, \; :i]}; 0, \eta) \\
f(e_{[:, \; i:]}) = e_{[:, \; i:]} + \epsilon * \nabla_{e_{[:, \; i:]}} || O_l(e, \theta) - O_l(e^*, \theta)  ||_1
$$

where $e^*$ denotes the original embedding for a given input $a$ and $\eta$ and $\epsilon$ are tunable parameters.

```python
def clamped_walk(embedding, steps, rand_eta, lr):
	with torch.no_grad():
		target_output = a_model(embedding)
	embedding = embedding.detach()

	for i in range(steps):
		clamped = torch.randint(768, (1,))
		shape = embedding[:, :, clamped:].shape
		embedding[:, :, clamped:] += rand_eta*torch.randn(shape).to(device)
		gradient = target_gradient(a_model, embedding, target_output)
		embedding = embedding.detach()
		embedding[:, :, :clamped] = embedding[:, :, :clamped] - lr*gradient[:, :, :clamped]
		
	return embedding
```

This technique is far more capable of accomplishing our goal of changing $a$ while leaving $O_l(a, \theta)$ unchanged.  For a 12-block transformer model without a language modeling head such that the output shape is identical to the input shape, tuning the values of $\eta, \epsilon, N$ yields an $L^1$ metric on the distance between $m(e, e_N)$ that is $10$ times larger than $m(O_l(e, \theta), O_l(e_n, \theta))$.  The ratio $r$ defined as

$$
r = \frac{|| e - e_n ||_1} {|| O_l(e, \theta) - O_l(e_N, \theta) ||_1}
$$

may be further increased to nearly $100$ or more by increasing the number of gradient descent iterations per clamp shift step from one to fifty.  

It is interesting to note that the transformer architecture is much more amenable to this gradient clamping optimization than the fully connected model, which generally does not yield an $r>3$ without substantial tuning.

Now that a method of changing the input such that the corresponding change to the output is minimized has been found, we can choose appropriate values of $N, \eta, \epsilon$ such that the $e_N$ corresponds to different input tokens.  For successively larger $N$, we have for a trained GPT-2 model (without a language modeling head)

$$
\mathtt{elsius \; sky \; is \; blue.} \\
\mathtt{elsius \; skyelsius \; blue.} \\
\mathtt{elsius \; skyelsiuselsius.}
$$

Once again, we find that the trained GPT-2 approximates an English sentence with gibberish. This is true even if we take the top-k nearest tokens to $e_N$ rather than the top-1 as follows:

```python
elsius skyelsiuselsius.
advertisementelsiusadvertisementascriptadvertisement
eaturesascripteaturesadvertisementelsius
 destroadvertisementhalla blue.]
thelessbiltascript":[{"bilt
```

### Representation Repetitions

It is interesting to note that practically every example of a poorly-formed input representation we have seen on this page suffers from some degree or other of repetition.  Take the top-1 token input found above with spaces added for clarity:

$$
\mathtt{elsius \; sky \; elsius \; elsius}.
$$

where $\mathtt{sky}$ is the only target word found and all else are repetitions.  

This is interesting in light of the observation that language models (particularly smaller ones) often generate repeated phrases when instructed to give an output of substantial length.  This problem is such that efforts have been made to change the output decoding method: for example [Su and colleages](https://arxiv.org/abs/2202.06417) introduced contrastive search for decoding as opposed to simply decoding the model output as the token with the largest activation (which has been termed a 'greedy' decoding approach) during autoregression.

The tendancy language models tend to generate repetitive text during autoregression has been attributed by [Welleck and colleages](https://arxiv.org/pdf/1908.04319.pdf) to the method by which language models are usually trained, ie maximum likelihood on the next token in a string.  The authors found that two measures ameliorate this repetition: modifying the objective function ('maximum unlikelihood estimation') and modifying the decoding method to instead perform what is called a beam search.  For all inputs $a$ of some dataset where each input sequence is composed of tokens $a = (t_0, t_1, t_2, ..., t_n)$ where the set of all possible tokens $T$ such that $t_n \in T$, minimization of the log-likelihood of the next token $t_i$ may be expressed as

$$
t_i = \underset{t_i}{\mathrm{arg \; min}} \; - \sum_a \sum_{T} \log p(t_i | O(t_{i-1}, t_{i-2}, ..., t_1; \; \theta))
$$

Beam search instead attempts to maximize the total likelihood over a sequence of tokens rather than just one.  For two tokens, this is

$$
t_i, \; t_{i-1} = \underset{t_i, \; t_{i-1}}{\mathrm{arg \; min}} \; - \sum_a \sum_{T} \log p(t_i, t_{i-1} | O(t_{i-2}, t_{i-3}, ..., t_1; \; \theta))
$$

where $t_{i-1}$ may be any of the number of beams specified.  A good explanation of beam search relative to topk or other methods may be found [here](https://huggingface.co/blog/how-to-generate). 

Returning to our model representation findings, it is clear that GPT-2 indeed tends to be unable to distinguish between repetitive sequences and true sentences.  But viewed through the lense of representation theory, there is a clear reason why this would be: training has presumably never exposed the model to a sequence like $\mathtt{elsius \; sky \; elsius \; elsius}.$ as the training inputs are generally gramatically correct sentences.  Therefore there is no 'hard' penalty on viewing these nonsensical phrases as being identical to real ones, in the sense that the loss function would not necessarily have placed a large penalty on a model that generates this phrase.

On the other hand, it is also clear that there is indeed *some* kind of penalty placed on this phrase because it should never appear during training.  This is analagous to the idea of opportunity cost in economics, in which simply not choosing a profitable activity may be viewed as incurring some cost (the lost profit).  Here a model that generates a nonsensical phrase is penalized in the sense that this output could not possibly give the model any benefit with regards to maximum likelihood training on the next token, whereas even a very unlikely but gramatically correct phrase could appear in some text and therefore could be rewarded.

Equivalently, observe that a model that has effectively minimized its log-likelihood objective function should not repeat words or phrases unless such repetitions were found in the training data, as errors such as these (on a per-token basis) would necessarily increase the negative log-likelihood.  Therefore all language models of sufficiently high capacity, when trained on enough data are expected to avoid repetition.

These observations suggest that if one were to train a language model of sufficiently large capacity on a sufficiently large dataset, repetition would cease to be found as this model would more effectively minimize negative log-likelihood.  From the performance of more recent and larger models than GPT-2 we can see that this may indeed be correct.  Does this mean that the representation in larger models trained on more data will also be less prone to repetition?  

The first part of this question answered by performing the same input representation procedure on larger versions of GPT-2, where the models have been trained on the same dataset (40 GB of text from Reddit links mostly). On this page we have thus far considered the base GPT-2 model with 117M parameters, and now we can observe the input representation repetition for the `gpt2-xl` with 1.6B parameters. Once we have downloaded the appropriate trained models from the HuggingFace transformers store, which for `gpt2-xl` may be done as follows:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2-xl")
```

we find that optimizing an input to match the output of the first transformer block is perhaps easier than it was for the `gpt2-base` model with a tenth of the parameters, where a mere 20 iterations of gradient descent is more than enough to satisfy the desired inequality in which the measure of distance between the generated input's output is smaller than the shifted input's output as described in \eqref{eq4}. 

```
".[.>>...
ADA.annabin.>>.>>
kus".[ADAwanaiannopoulos
iott Gleaming.>>".[olor
 Gleamingoloriottiannopouloswana
```

Interestingly, however, we can see that there is now much less repetition for each topk decoding.  The same is true if we use the first 12 transformer decoders, which yield a top5 input representation as follows:

```
iets largeDownloadnatureconservancy".[twitter
udeauhexDonnellramids".[
 largeDownload2200ledgePsyNetMessage>[
retteimanclipsTPPStreamerBot
 Citizenudeau2200brycekson
```

and for a 24-block hidden architecture the top5 decodings of the input representation are

```
 Marketable (" Dragonboundju.>>
ossessionoptimisoliband().
minecraftsocketramid,...!.
iczisphere Canyonpiresaer
eloowntgur Pastebinnatureconservancy
```

and even for 40 blocks,

```
iczisphere Dragonboundheit\
umerableaddonszoneNT>.
auerohnramidohlSTEM
ozisonssoleolin.<
uskyankaFSibandrawl
```

The author's local GPU (an RTX 3060 with 12GB memory) runs out of memory trying to perform backpropegation on the full 48-block `gpt2-xl`.  In this case we are faced with a few options: either we can use larger GPUs or else we can compress the model or model's parameters in some way.  Model parameters are usually set to a 32-bit floating point datatype by default, but for this input representation visualization process we do not actually need full precision. Instead, we can load and convert the model to (mostly) 8-bit precision using Tim Dettmer's very useful [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) library (or the ported [bitsandbytes-windows](https://github.com/fa0311/bitsandbytes-windows) version if you are using Windows), which is integrated into the `AutoModelForCausalLM` module in the Huggingface Transformers library such that a model may be loaded to a GPU in 8 bit precision using the following:

```python
model = AutoModelForCausalLM.from_pretrained("gpt2-xl", load_in_8bit=True, device_map='auto')
```

`bitsandbytes` does not actually convert all model parameters to 8-bit precision due to the ubiquitous presence of outlier features in large language models, as shown by [Dettmers and colleages](https://arxiv.org/abs/2208.07339).  Note that most CPUs do not support linear algebraic operations with any datatype less than 32 bits, so a GPU should be used here. Once the model has been loaded, for the full 48-block stack of GPT-2 we have a top-5 representation of the input 'The sky is blue.' represented as

```
 COURisphere Dragonboundheit\<
=>addonsGWolin>.
umerableankaOrgNTrawl
CVE '[soleiband.<
fter ('ramidohl$.
```

It is interesting to note that gradient-based optimization of the input is much more difficult for the full 48-block GPT-2 than even first 24-block subset of this model, indicating that the later transformer blocks are poorly conditioned relative to the earlier blocks.  This is true even when 8-bit quantization is performed on the smaller subsets, indicating that quantization is not in this case responsible for difficulties optimizing via gradient descent.

To summarize this last section, simply increasing the model's size does seem to reduce the amount of repetition, but even the larger models we have investigated thus far do not exhibit accurate representations of language inputs.  This investigation is continued in [Part III](https://blbadger.github.io/language-discreteness.html).

### Implications

In summary, transformer-based language models in the GPT-2 family are unable to distinguish between real English sentences and pure gibberish.  Given a point in a transformer block hidden layer space corresponding to an input of a real sentence, we have found that most nearby points correspond to inputs that are not even approximately sentences but are instead completely unintelligible.  

There exists a notable difference between trained language and vision transformer models: the latter contain modules that are at least partially capable of discerning what the input was composed of, whereas the latter does not.  But when we consider the training process for language models, it is perhaps unsurprising that input representations are relatively poor.  Note that each of the gibberish input generations were almost certainly not found in the training dataset precisely because they are very far from any real language.  This means that the language model has no *a priori* reason to differentiate between these inputs and real text, and thus it is not altogether unsurprising that the model's internal representations would be unable to distinguish between the two.

In a sense, it is somewhat more unexpected that language models are so poorly capable of approximate rather than exact input representation.  Consider the case of one untrained transformer encoder detailed in a previous section: this module was unable to exactly represent the input (for the majority of random initializations) but the representations generated are semantically similar to the original prompt.  This is what we saw for [vision models](https://blbadger.github.io/vision-transformers.html), as input representations strongly resembled the target input even if they were not exact copies.  The training process therefore results in the loss of representational accuracy from the transformer encoders.

In this page's introduction, we considered the implications of the 'no free lunch' theorems that state (roughly) that no on particular model is better than any others at all possible machine learning tasks.  Which tasks a given model performs well or poorly on depends on the model architecture and training protocol, and on this page we saw that trained language models perform quite poorly at the task of input generation because even early layers are unable to differentiate between English sentences and gibberish. Non-invertibility alone does not explain why these trained models are poor discriminators, but the training protocol (simply predicting the next word in a sentence) may do so.

When one compares the difference in vision versus language transformer model input representational ability, it is clear that language models retain much less information on their inputs.  But when we consider the nature of language, this may not be altogether surprising: language places high probability on an extraordinarily small subset of possible inputs relative to natural images.  For example, an image input's identity is invariant to changes in position, orientation (rotation), order, smoothness (ie a grainy image of a toaster is still a toaster), and brightness.  But a language input is not invariant to any of these transformations.  A much smaller subset of all possible inputs (all possible word or bytecode tokens in a sequence) are therefore equivalent to language models, and the invariants to be learned may be more difficult to identify than for vision models.

Finally, it is also apparent that it is much more difficult to optimize an input via gradient descent on the loss of the model output after training versus before.  This appears to be a somewhat attention-intrinsic phenomenon and was also observed to be present in Vision Transformers, but we find that language modeling heads (transformations from hidden space to output tokens) are particularly difficult to optimize across. This observations suggests that efficient training of transformer-based models is quite difficult, and could contribute to the notoriously long training time required for large language models.  Freezing the language modeling head transformation would therefore be expected to assist the training process.
## Language Model Representations and Features

### Introduction to Language

One of the most important theorems of machine learning was introduced by [Wolpert](https://direct.mit.edu/neco/article-abstract/8/7/1341/6016/The-Lack-of-A-Priori-Distinctions-Between-Learning) and is colloquially known as the 'No Free Lunch' theorem, which may be stated as the following: no particular machine learning method is better or worse than any other when applied to the problem of modeling all possible statistical distributions.  A similar theorem was shown for the problem of optimization (which covers the processes by which most machine learning algorithms learn) by [Wolpert and Macready](https://ieeexplore.ieee.org/abstract/document/585893), and states that no one algorithm is better or worse than any other across all classes of optimization problems.

These theorems are often used to explain overfitting, in which a model may explain training data arbitrarily well but explains other similar data (usually called validation data) poorly.  But it should be noted that most accurately these theorems do not apply to the typical case in which a finite distribution is being modeled, an d therefore are not applicable to cases of empirical overfitting.  But they do convey the useful idea that each machine learning model and optimization procedure is usueful for only some proper subset of all possible statistical distributions that could exist.

The assumption that not all possible distributions may be modeled by a given learning procedure does not affect the performance of machine learning, if one assumes what is termed the manifold hypothesis.  This hypothesis posits that the set of all elements for a given learning task (ie natural images for a vision model or English sentences for a language model) is a small subset of the set of all elements that could exist in the given space (ie all possible images of a certain resolution or all possible permutations of words and spaces).  The task of the learning program is simply to find the manifold, which is the smooth and approximately connected lower-dimensional space that exists in the higher-dimensional input, such that the objective function associated with the learning task is minimized.

It has been observed on [this page](https://blbadger.github.io/depth-generality.html) that subsequent layers of vision models tend to map arbitrary inputs to manifolds learned during training, rather than simply selecting for certain pieces of input information that may be important for the task at hand. The important difference is that we can see that vision models tend to 'infer' rather than simply select information about their input during the manifold mapping process.  Typically the deeper the layer of the vision model, the more input information is lost in the untrained model and the more information that is inferred in the trained one.

We may wonder whether this is also the case for language models: to these also tend to infer information about their input? Are deeper layers of language models less capable of accurately reconstructing the input, as is the case for vision models?  Do deeper layers infer more input information after training?

### Spatial Learning in Language Model Features

The most prominent deep learning model architecture used to model language is the Transformer, which combines dot product self-attention with feedforward layers applied identically to all elements in an input to yield an output.  Self-attention layers originally were applied to recurrent neural networks in order to prevent the loss of information in earlier words in a sentence, a problem quite different to the ones typically faced by vision models. 

Thus convolutional vision and language models diverged until it was observed that the transformer architecture (with some small modifications) was also effective in the task of image recognition.  Somewhat surprisingly, [elsewhere](https://blbadger.github.io/transformer-features.html) we have seen that transformers designed for vision tasks tend to learn in a somewhat analagous fashion to convolutional models: each neuron in the attention module's MLP output acts similarly to a convolutional kernal, in that the activation of this neuron yields similar feature maps to the activation of all elements in one convolutional filter.

It may be wondered whether transformers designed for language tasks behave similarly.  A first step to answering this question is to observe which elements of an input most activate a given set of neurons in some layer of our language model. One way to test this is by swapping the transformer stack in the [vision transformer](https://arxiv.org/abs/2010.11929) with the stack of a trained language model of identical dimension, and then generating an input (starting from noise) using gradient descent on that input to maximize the output of some element.  For more detail on this procedure, see [this page](https://blbadger.github.io/transformer-features.html).  

To orient ourselves to this model, the following figure is supplied to show how the input is split into patches (which are identical to the tokens use to embed words or bytepairs in language models) via the convolutional stem of Vision Transformer Base 16 to the GPT-2 transformer stack.  GPT-2 is often thought of as a transformer decoder-only architecture, but aside from a few additions such as attention masks the architecture is actually identical to the original transformer encoder and therefore is compatible with the ViT input stem.

![architecture explained]({{https://blbadger.github.io}}/deep-learning/transformer_activation_explained.png)

Here we are starting with pure noise and seek to maximize the activation of a set of neurons in some transformer module using gradient descent between the neurons' value and some large constant tensor.  To be consistent with the procedure used for vision transformers, we also apply Gaussian convolution (blurring) and positional jitter to the input at each gradient descent step (see [this link](https://blbadger.github.io/transformer-features.html) for more details).  First we observe the input resulting from maximizing the activation of an individual neuron (one for each panel, indexed 1 through 16) across all patches. In the context of vision transformers, the activation of each single neuron in all patches (tokens) forms a consistent pattern across the input (especially in the early layers).  

When the activation of a single neuron in all tokens of GPT-2 is maximized using the ViT input convolution with the added transformations of blurring and positional jitter, we have the following:

![gpt2 feature visualization]({{https://blbadger.github.io}}/deep-learning/gpt2_features_viz.png)

The first thing to note is that we see patterns of color forming in a left-to-right and top-to-bottom fashion for all layers.  This is not particularly surprising, given that the GPT-2 language model has been trained to model tokens that are sequential rather than 2-dimensional as for ViTs, and the sequence of tokens for these images is left to right, top to bottom.

It is somewhat surprising, however, that much of the input (particularly for early layers) is unchanged after optimization, implying that as the patch number increases there is a smaller chance that activation of a neuron from that patch actually requires changing the input that corresponds to that patch, rather than modifying earlier patches in the input.  It is not altogether surprising that maximizing the activation of all GPT-2 tokens should lead to larger changes to the first compared to the last tokens.  This is because each token only attends to itself and earlier tokens.  

For an illustration of why we would expect for early input tokens to have more importance than later ones assuming equal importance placed on all inputs, consider the simple case in which there are three tokens output $t_1, t_2, t_3$ and $f'(t_n)$ maximizes the output of all input tokens $i_0, i_1, i_2$ with index $m \leq n$ equally with unit magnitude.  Arranging input tokens in an array as $[i_0, i_1, i_2]$ for clarity,

$$
f'(t_1) = [1, 0, 0] \\
f'(t_1, t_2) = [2, 1, 0] \\
f'(t_1, t_2, t_3) = [3, 2, 1] \\
$$

In the images above we are observing the inputs corresponding to $f'(t_1, t_2, ..., t_n)$.  From the figure above it appears that there is a nonlinear decrease in the input token weight as the token index increases, indicating that there is not in general an equal importance placed on different tokens.  

There also exists a change in relative importance per starting versus ending token in deeper layers, such that early layer neurons tend to focus on early elements in the input (which could be the first few words in a sentence) whereas the deeper layer neurons focus more broadly.

Positional jitter and Gaussian blurring convolutions are performed on vision models to enforce statistical properties of natural images on the input being generated, namely translation invariance and smoothness.  There is no reason to think that language would have the same properties, and indeed we know that in general language is not translation invariant.

We therefore have motivation to see if the same tendancy to modify the start of the input more than successive layers (as well as more broad pattern of generation with deeper layers) also holds when jitter and blurring are not performed.  As can be seen in the figure below, we see that indeed both observations hold, and that the higher relative importance of starting compared to ending input patches is even more pronounced.

![gpt2 feature visualization]({{https://blbadger.github.io}}/deep-learning/gpt2_features_viz_2.png)

When we instead activate all elements (neurons) from a single patch, we see that in contrast to what is found for vision transformers, early and late layers both focus not only on the self-patch but also preceding ones too.  Only preceding patches are modified because GPT-2 is trained using attention masks to prevent a token peering ahead in the sequence of words.  Note too that once again the deeper layer elements focus more broadly than shallower layer elements as observed above.

![gpt2 feature visualization]({{https://blbadger.github.io}}/deep-learning/gpt2_features_viz_3.png)

The incorperation of more global information in deeper layers is also observed for vision models, although it is interesting to note that transformer-based vision model patches typically do not incorperate as much global information in their deeper layers as MLP-based mixers or convolutional models. 

### Image Reconstruction with Language Models

How much information is contained in each layer of a language model?  One way to get an answer to this question is to attempt to re-create an input, using only the vector corresponding to the layer in question.  Given some input $a$, the output of a model $\theta$ at layer $l$ is denoted $y = O_l(a, \theta)$.  Does $O_l(a, \theta)$ contain the information necessary to re-create $a$? If we were able to invert the forward transformation $O_l$, then it would be simple to recover $a$ by

$$
a = O_l^{-1}(y, \theta)
$$

But typically this is not possible, as for example if multiple inputs $a_1, a_2, ..., a_n$ exist such that

$$
O_l(a_1, \theta) = O_l(a_2, \theta) = \cdots = O_l(a_n, \theta)
$$

which is the case for transformations present in many models used for language and vision modeling. Besides true non-invertibility, linear transformations with eigenvectors of very different magnitudes are often difficult to invert practically even if they are actually invertible.  This is termed approximate non-invertibility, and has been seen to exist for vision models [here](https://blbadger.github.io/depth-generality.html).

The ability of the information present in $O_l$ to generate $a$ from noise can be thought of as a measure of representational accuracy.  How does representational accuracy for transformers trained for language modeling compare to those trained for image classification?  In the following figure, we can see that the trained GPT-2 has less accurate input representations in the early layers than ViT Large does, although these layers are still capable of retaining enough information to generate recognizable images.

![gpt2 vs vit representation]({{https://blbadger.github.io}}/deep-learning/vit_vs_gpt2_representation.png)

This is also the case for out-of-distribution images such as this Tesla coil.  In particular, the first few dozen tokens (top few rows in the grid of the input image, that is) of the input for both images are comparatively poorly represented, and display high-frequency inputs that is common for representations of poorly conditioned models. 

![gpt2 vs vit representation]({{https://blbadger.github.io}}/deep-learning/vit_vs_gpt2_representation_2.png)


### Conclusion

On this page we have seen that deeper modules focus more broadly on input tokens whereas shallower layers tend to focus on specific portions (here early parts) of the input.  Compared with vision transformers, it is apparent that language transformers are less self-attentive such that each patch (token) tends to focus on more than itself.

In some respects, it is surprising that models designed for language processing would be capable of retaining most input information even across a single transformer block.  



## Input Generation II: Vectorization and Latent Space Exploration

This page is part II on generating inputs using deep learning models trained for image classification. For part I, follow [this link](https://blbadger.github.io/input-generation.html). For part III, see [here](https://blbadger.github.io/input-representation.html)

### Introduction with Opposites

It is remarkable that deep learning models tasked with image classification are capable of producing coherent images representing a given output class. The task of image generation is far different than classification, but nevertheless recognizable images may be generated by optimizing the output for a given class.  In a previous section on this page, we also saw that images may be generated with a linear combination of two classes, which allowed us to transform a generated image of a husky into an image of a dalmatian.  

These observations lead to a natural idea: perhaps we can treat the output tensor of a deep learning model as a latent space. A latent space, also known as an embedding, is informally a space that captures meaningful information about the possible objects in that space.  More precisely, it is a manifold in which object similarity correlates with a distance metric on that manifold. 

We will consider the final 1000-dimensional fully connected layer activation of possible ImageNet categories as the output in question. At first glance it is not clear that this could be any kind of latent space: during training, each category is denoted by a one-hot vector in this space such that all possible categories are the same distance apart from each other.  This means that there is no prior information encoded in one output class versus another, which is exactly what one wants when training for classification without prior knowledge.

On the other hand, within this 1000-dimensional space we can view each class as a basis vector for this space and instead consider the possible vectors that exist in this space. A meaningful vector space of the outputs allows us to explore interesting questions by simply converting each question into a vector arithmetic operation.  

On a memorable episode of the popular comedy 'Seinfeld', the character George decides to do the opposite of what he would normally do with appropriately comedic results.  But one might wonder: what is the opposite?  For a number of ideas, there seems to be a natural opposite (light and dark, open and closed) but for others ideas or objects it is more difficult to identify an opposite: for example, what is the opposite of a mountian?  One might say a valley, but this is far from the only option.  Likewise, objects like a tree and actions like walking do not have clear opposites.

In [part I](https://blbadger.github.io/input-generation.html) we saw that deep learning models are capable of forming an image that represents some target output $\widehat y$.  This target output was usually a vector in which the entry at the index (signifying the ImageNet class) of choice was a large constant and zeros everywhere else, and the input image was modified using gradient descent in order to miminize the loss metric between the output and the target output.  With certain restrictions on how the gradient can be applied (smoothness in the input, for example) coherent and recognizable images can be generated.  Observation of the outputs shows that indeed the class of interest is maximized, as for example GoogleNet applied to maximize the activation of element 920 (signifying 'stoplight')

![opposite example]({{https://blbadger.github.io}}/neural_networks/negatives_example.png)

Observe that even though only index 920 was optimized, other output class activations have been affected as well.  It may be hypothesized that these activations correspond to a similarity between each of the 999 other ImageNet categories and class 920, with a higher activation signifying a more similar class where 'similar' is a measure on the function $f$ representing the GoogleNet model.  As a similarity metric this measure is actually quite effective (see the [below section](https://blbadger.github.io/latent_output.html#graphs-on-imagenet-classes) for more information) and it may be wondered whether the element with the smallest activation could be an accurate representation of an 'opposite' to the class being maximized.

More formally, we want to find the index $k$ 

$$
k = \mathrm{arg} \; \underset{i} {\mathrm{min}} \; O_i(a; \theta)
$$

where $a$ is an input generated to maximize some output class and $\theta$ denotes the configuration and parameters of the model used. 

The above method does not empirically yield very interesting results: the opposites of many ImageNet categories tend to be only a few classes, usually with no apparent relation to the category of interest. There is a clear theoretical basis for why this measure is not very effective: observe that there are many values that are near the minimum for the above image of a 'stoplight'.  It is not clear therefore that the index $k$ is well chosen, being that there is such a small difference between the outputs for many indicies.  Instead we want to find an index associated with a large distance between the value of the output at that index and the next smallest output. 

Finding a meaningful opposite using our image-generating procedure applied to deep learning models will not be difficult if the output is indeed a latent space.  We want to perform gradient descent on the input $a$ in order to minimize the activation of the output category of interest $O_i$, meaning that our loss function $J$ is

$$
J(O(a, \theta)) = O_i(a, \theta)
$$

and the gradient we want is the gradient of this loss with respect to the input, which is

$$
g = \nabla_a (O_i(a, \theta))
$$

The above formula can be implemented by simply assigning the loss to be the output of the output category as minimization is equivalent to maximization of a negative value.

```python
def layer_gradient(model, input_tensor, desired_output):
    ...
    input_tensor.requires_grad = True
    output = model(input_tensor)
    loss = output[0][int(desired_output)] # minimize the output class activation
    loss.backward()
    gradient = input_tensor.grad
    return gradient
```

and as before this gradient $g$ is used to perform gradient descent on the input, but now we will minimize rather than maximize the category of interest.

$$
a_{n+1} = a_n - \epsilon * g
$$

In geometric terms, this procedure is equivalent to the process of moving in the input space in a direction that corresponds with moving in the output space towards the negative axis of the dimension of the output category as far as possible.  

At first consideration, this procedure might not seem to be likely to yield any meaningful input $a_n$, as there is no guarantee that moving away from some class would not yield an input that is a mix of many different objects.  And indeed many generated opposite images are apparently a mix of a number of objects, for example this 'Piano' opposite that appears to be the image of a few different insects, or the opposite of 'Bonnet' that appears to be a mousetrap spring on fire.

![opposites]({{https://blbadger.github.io}}/neural_networks/googlenet_opposites_mix.png)

Despite it being unlikely that any of the 1000 ImageNet categories would have only one opposite, we can find the category of the image as classified by our model of choice (GoogleNet) by finding which element of the tensor of the model's output $O(a_n, \theta)$, denoted `output`, has the maximum activation.

```python
predicted = int(torch.argmax(output))
```

Now we can label each generated image according to which ImageNet category it most activates using a model of choice, here GoogleNet to be consistent with the image generation.  The following video shows the generation of an input $a$ that minimizes the GoogleNet activation for ImageNet class 55: Green Snake (red dot in the scatterplot to the right). Once again, two octaves with Gaussian convolutions are applied during gradient descent.

{% include youtube.html id='czayyaAi1cw' %}

Notice how a number of different categories have been maximized, and how the image appears to be a combination of different parts (an axolotl's gills with the feet and scales of a crocodile are perhaps the two most obvious).  Some objects have more coherent, even reasonable opposites: toilet paper is soft, flat, and waivy whereas syringes are thing and pointy.  

![opposites]({{https://blbadger.github.io}}/neural_networks/googlenet_opposites.png)

Dogs are perhaps the most interesting image category for this procedure nearly every ImageNet dog class has a coherent opposite that is also a dog, and the opposites generated seem to be logically motivated: observe how the opposites for large, long-haired dogs with no visible ears are small, thin, and perky-eared breeds.

![dog opposites]({{https://blbadger.github.io}}/neural_networks/googlenet_shaggy_opposites.png)

and likewise the opposites of a dog with longer fur and a pointed face (border collie) is one with short fur and squashed face (bloodhound), and the opposite of an image of a small dog with pointed ears (Ibizan hound) is a large dog with droopy ears (Tibetan Mastiff). Observe that opposites are rarely commutative: here we see a close but not quite commutative relation, where the opposite of an Ibizan is a Mastiff but the opposite of a Mastiff is a Terrier.  In general opposites are further from being commutative than this example.

![dog opposites]({{https://blbadger.github.io}}/neural_networks/googlenet_nonshaggy_opposites.png)

It is fascinating to see the generated images for the opposites of other animal classes.  

![animal opposites]({{https://blbadger.github.io}}/neural_networks/googlenet_animal_opposites.png)

The opposites of snakes are curiously usually lizards (including crocodiles) or amphibians (including axolotls) and the opposites of a number of birds are species of fish.  Opposites to all ImageNet class images according to GoogleNet may be found by following [this link](https://drive.google.com/drive/folders/1nE9X0PkG51RIL5euIHwOHfuV5OWWQm0i?usp=sharing). 

### Dog Transfiguration

In the last section, inputs representing the opposites of ImageNet classes were generated using gradient descent such that the gradient used to minimize the activation of the ImageNet class in question $g'$ was $g' = -g$, i.e. the gradient used to maximize the activation value multiplied by negative one.  As the negative value cancels with the gradient descent term $a_{i+1} = a_i - \epsilon * (-g)$ this procedure is sometimes called gradient ascent.  Multiplying by negative one is far from the only transformation we can perform, however: here we explore linear combinations of two target classes using InceptionV3, with a real image as a starting point.

We can view the difference between a Husky and a Dalmatian according to some deep learning model by observing what changes as our target class shifts from 'Husky' to 'Dalmatian', all using a picture of a dalmatian as an input.  To do this we need to be able to gradually shift the target from the 'Husky' class (which is $\widehat y_{250}$ in ImageNet) to the 'Dalmatian' class, corresponding to $\widehat y_{251}$.  This can be accomplished by assigning the loss $J_n(0(a, \theta))$ $n=q$ maximum interations, at iteration number $n$ as follows:

$$
J_n(O(a, \theta)) \\
= \left(c - \widehat y_{250} * \frac{q-n}{q} \right) + \left(c - \widehat y_{251} * \frac{n}{q} \right) 
$$

and to the sum on the right we can add an $L^1$ regularizer if desired, applied to either the input directly or the output.  Applied to the input, the regularizer is as follows:

$$
L_1 (a) = \sum_i \lvert a_i \rvert
$$

Using this method, we go from $(\widehat y_{250}, \widehat y_{251}) = (c, 0)$ to $(\widehat y_{250}, \widehat y_{251}) = (0, c)$ as $n \to q$.  The intuition behind this approach is that $(\widehat y_{250}, \widehat y_{251}) = (c/2, c/2)$ or any other linear combination of $c$ should provide a mix of characteristics between target classes. 

{% include youtube.html id='1bdpG1caKMk' %}

Using InceptionV3 as our model for this experiment, we have we see that this is indeed the case: observe how the fluffy husky tail becomes thin, dark spots form on the fur, and the eye color darkens as $n$ increases.

{% include youtube.html id='PBssSJoLOhU' %}

### Vector Addition and Subtraction

We have so far seen that it is possible to generate recognizable images $a'$ that represent the opposites of some original input $a$, where the gradient descent procedure makes the input $a' = -a$ according to how the model views each input.  Likewise it has been observed that linear combinations of the output corresponding to two breeds of dog yield recognizable images where $a' = ba_0 + ca_1$ for some constant $d$ such that $b + c = d$.

We can explore other vector operations.  Vector addition is the process of adding the component vectors in a space, and may be thought of as resulting in a vector that contains some of the qualities of both operands. One way to perform vector addition during gradient descent on the input is to perform each update $a' = a + \epsilon g$ such that the gradient is

$$
g = \nabla_a (C - O_1(a, \theta)) + \nabla_a(C - O_2(a, \theta)) \\
= \nabla_a (2C - O_1(a, \theta) - O_2(a, \theta)) 
$$

which leads to the appearance of merged objects, for example this turtle and snowbird

![resnet_addition]({{https://blbadger.github.io}}/neural_networks/vectorized_resnet_1.png)

This sort of addition we can call a 'merging', as characteristics of both target classes $a_1, a_0$ are found in the same contiguous object.  Merging is fairly common using the above gradient.

![resnet_addition]({{https://blbadger.github.io}}/neural_networks/vectorized_resnet_2.png)

Some target classes tend to make recognizable shapes of one but not both $a_0, a_1$.  Instead we can try to optimize the activation of these categories separately, choosing to optimize the least-activated neuron at each iteration. The gradient is therefore

$$
g' =
\begin{cases}
\nabla_a(C - O_2(a, \theta)),  & \text{if} \; O_1 \geq O_2 \\
\nabla_a(C - O_1(a, \theta)),  & \text{if} \; O_1 < O_2
\end{cases}
$$

This $g'$ more often than $g$ when applied to gradient descent does give an image which places both target objects are recognizable and separated in the final image, which we can call juxtaposition.

![resnet_addition]({{https://blbadger.github.io}}/neural_networks/vectorized_resnet_3.png)

However the addition is performed, there are instances in which the output is neither the merging nor juxtaposition of target class objects.  For example, (1) applied to addition of a snowbird to a tarantula yields an indeterminate image somewhat resembling a black widow.

![resnet_addition]({{https://blbadger.github.io}}/neural_networks/resnet_junco_tarantula.png)


### Feature Latent Space

Suppose one were to want to understand which of the ImageNet categories were more or less similar to another.  For example, is an image of a cat more similar to a fox or a wolf?  Specifically we want this question answered with abstract ideas like facial and tail structure, rather than some simple metric like color alone.

This question is not at all easy to address.  We seek a metric that will determine how far ImageNet category is from every other category, but the usual metrics one can place on an image will not be sufficient.  Perhaps the simplest way to get this metric is to take the average image for each category (by averaging the values of all images of one category pixel per pixel) and measure the $L^2$ or $L^1$ distance between each image.  This is almost certain to fail in our case because there is no guarantee that such a distance would correspond to higher-level characteristics rather than lower-level characteristics like color or hue.

Instead we want a measurement that corresponds to more abstract quantities, like the presence of eyes, number of legs, or roundness of an object in an image. We could use those three traits alone, and make a three-dimensional representation called an embedding consisting of points in space where the basis of the vector space is precisely the values attached to each of these characteristics.  For example, if we have some object where `[eyes, legs, roundness] = [4, 10, 0.2]` we would likely have some kind of insect, whereas the point `[-10, -2, 10]` would most likely be an inanimate object like a beach ball.

Happily for us, deep learning models are capable of observing high-level characteristics of an image.  We have seen that [feature maps](https://blbadger.github.io/feature-visualization.html) of certain hidden layers of these models tend to be activated by distinctive patterns, meaning that we can use the total or average activation of a feature map as one of our basis vectors.

Somewhat arbitrarily, let's choose two features from GoogleNet's layer 5a as our basis vectors.  For reference, here are the maps for the features of interest (meaning that the following images were found to maximally activate the features via gradient descent):

![resnet_addition]({{https://blbadger.github.io}}/neural_networks/googlenet_features_latent.png)

Feature 0 seems to respond to a brightly colored bird-like pattern whereas feature 4 is maximally activated by something resembling a snake's head and scales.  We can observe the activation of these layers for GoogleNet-generated images representing each ImageNet class in order to get an idea of which categories these layers score as more or less similar from each other.  The following code allows us to plot the embedding performed by these features by plotting the average activation of the two features for each generated output.

```python
def plot_embedding():
    x, y, labels_arr = [], [], []
    for i, image in enumerate(images):
        label = image[1]
        image = image[0].reshape(1, 3, 299, 299).to(device)
        output = network(image)
        x.append(float(torch.mean(output[0, 0, :, :])))
        y.append(float(torch.mean(output[0, 4, :, :])))
        i = 11
        while label[i] not in ',.':
            i += 1
        labels_arr.append(label[11:i])

    plt.figure(figsize=(18, 18))
    plt.scatter(x, y)
    for i, label in enumerate(labels_arr):
        plt.annotate(label, (x[i], y[i]))
    plt.show()
    plt.close()
    return
```

this yields

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/googlenet_5a_04_embedding.png)

which has noticeably skewed distribution,

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/googlenet_5a_distribution.png)

It appears that Feature 0 corresponds to a measure of something similar to 'brightly-colored bird' whereas Feature 4 is less clear but is most activated by ImageNet categories that are man-made objects.

### Graphs on ImageNet classes

Investigating which ImageNet categories are more or less similar than each other was explored in the previous section using two features from one layer of a chosen model (GoogleNet).  But in one sense, these embeddings are of limited use, because they represent only a very small portion of the information that the model possesses in respect to the input images, as there are many more features in that layer and many more layers in the model. To be specific, the embedding diagram in the last section denotes that 'Jay' is the ImageNet class most similar to 'Indigo Bunting' for GoogleNet, but only for two out of over 700 features of one specific layer.

Each of the features and layers are important to the final classification prediction, and moreover these layers and features are formed by non-linear functions such that the features and layers are non-additive.  Therefore although the embeddings of the output categories using feature activation as the basis space is somewhat useful, it is by no means comprehensive.  Another approach may be in order in which the entire model is used, rather than a few features.

There does exist a straightforward way to determine which ImageNet categories are more or less similar than each other: we can simply take the model output (with ImageNet classification occuring in the last layer) vector given the generated input $a_g$ which is $y = O(a_g, \theta)$ and observe the magnitudes of the components of this vector.  Assuming that the model correctly identifies the generated input, the component of $y$ that is largest will be the target class that $a_g$ was generated to represent.  The second-largest component corresponds to a different class that the model considers to be 'most similar' to the target class.

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/nearest_neighbor_explanation.png)

There exists a problem with using the this approach as a true similarity metric, however: $y = O(a', \theta)$ is not guaranteed to be symmetric, or in other words generally $m(a, b) \neq m(b, a)$.  This means that we cannot use the findings from the generation metric to make a vector space or any other metric space as the allowable definition of a distance metric is not followed.  

But because pairs of points exhibit an asymmetric measurement, we cannot portray this as a metric space.  But it is possible to portray these points as an abstract graph, with nodes corresponding to ImageNet categories (ie outputs) and verticies corresponding to relationships between them.  We will start by only plotting the 'nearest neighbor' relationship, which is defined as the output that is most activated by the generated image distinct from the target output $\widehat y$.  


```python
import networkx as nx

def graph_nearest():
    # convert array of pairs of strings to adjacency dict
    closest_dict = {}
    for pair in closest_array:
        if pair[0] in closest_dict:
            closest_dict[pair[0]].append(pair[1])
        else:
            closest_dict[pair[0]] = [pair[1]]

    G = nx.Graph(closest_dict)
    nx.draw_networkx(G, with_labels=True, font_size=12, node_size=200, node_color='skyblue')
    plt.show()
    plt.close()
    return
```

The first half of the 1000 ImageNet categories are mostly animals, and plotting a graphs for them yields

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/nearest_neighbors_animal_embedding.png)

Nodes that are connected together form a 'component' of the graph, and nodes that are all connected to each other form a complete component called a 'clique'.  Cliques of more that two nodes are extremely rare for ImageNet nearest neighbors, but non-trivial (ie those with more than two nodes) components abound, often with very interesting and logical structures.  Observe how cats form one component, and terrier dogs preside in another, and mustelids and small mammals in another

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/animal_components.png)

For the non-animal half of ImageNet, graphing nearest neighbors yields a component with more members than was observed for any animal component.

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/nearest_neighbors_nonanimal_embedding.png)

This contains many diverse objects, yet often still exhibit relationships that seem reasonable to a human.

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/nonanimal_components1.png)

Smaller components are often the most illuminating: observe the sports balls clustering together in one component, and the utensils in another component

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/nonanimal_components2.png)

### Output Latent Space

The above measurement is illuminating but is not a true metric space.  Instead of finding the second largest output for our generated inputs, we can find the ImageNet class corresponding to the second closest point (including the point of interest) to our point in the output space. This means that we wish to perform an embedding of the output with respect to the model.  The reasoning here is that our trained GoogleNet model with parameters $\theta$ may be viewed as a (very complicated) function $O$ that maps input images $a$ to an output $y$, which is a 1000-dimensional vector where the element of index $n$ denoted by $y_n$.

$$
y = O(a, \theta)
$$

Now if we use real images from some category, there is no guarantee that $v$ will be unchanged and it is unclear which $v$ best respresents the image category.  Instead we can use an image $a'$ generated to maximize the output category of interest (for more information on this, see [here](https://blbadger.github.io/input-generation.html)) as an approximation of the input that will most resemble the output category of interest $\widehat y_n = O_n(a, \theta)$.

$$
a' = \mathrm{arg} \; \underset{a}{\mathrm{max}} \; O_n(a, \theta)
$$

Using these representative inputs $a'$ applied to $O$, we can find the coordinates of all model outputs $y_n \in \Bbb R^{1000}$.  This means that we can find the coordinates of the representative input $a'$ in the 1000-dimensional output space. For a three-dimensional example of this method see the following figure.  Note that the metric $m(y_1, y_2)$ may be chosen from any number of possible methods.

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/vector_output_explanation.png)

As spaces with more than two or three dimensions are hard to visualize, we can perform a dimensionality reduction method for visualization, and here we will find a function $f$ to take $f: y_n \in \Bbb R^{1000} \to z_n \in \Bbb R^2$.  We shall employ principle component analysis, which is defined as the function $f(y)$ that produces the embedding $z$ such that a decoding function $g$ such that $x \approx g(f(y))$, where $g(y) = Dy$ and $D \in \Bbb R^{1000x2}$.  Therefore PCA is defined as the encoding function $f$ that minimized the distance of the encoded value $z$ from the original value $y$ subjected to the constraint that the decoding process be a matrix multiplication.  To further simplify things, $D$ is constrained to have linearly independent columns of unit norm.  The minimization procedure may be accomplished using eigendecomposition and does not requre gradient descent.

When we find the coordinates of $y_n$ for all $n$ ImageNet categories using GoogleNet and then map these points using the first two principle components

![googlenet embedding]({{https://blbadger.github.io}}/neural_networks/googlenet_output_embedding.png)

but the result is somewhat underwhelming.  Principle conponents 1 and 2 account for only $15$ and $4$ percent (respectively) of the variance, meaning that they capture very little of the original distribution.

Why is this the case?  The failure lies in PCA's expectation of a linear space, in which transformations $f$ are additive and scaling

$$
af(x + y) = f(ax) + f(ay)
$$

and where in particular the intuitive metric of distance stands.  As points in this space were generated using a nonlinear function (gradient descent of GoogleNet on a scaled normal input), there is no reason to think that a linear decomposition would be capable of capturing much of the variance in that function.


















### Information between tokens

So far we have been observing the ability of the information in a transformer block's output to represent some input.  It may also be wondered whether only part of the output would suffice, as would be expected if the information from the input were to be sufficiently mixed among the transformer's tokens before reaching the output.  [Elsewhere](https://blbadger.github.io/vision-transformers.html) it was observed that vision transformers are generally much worse than attentionless MLP mixers at representing input regions not aligned to the output's token.

We first repeat the procedure used in the page linked above: we give some target input to the model and then attempt to have the model generate that input using only the information in some output layer, specifically only the first x tokens in the sequence. For Llama 7b, it is clear that this language model also does not tend to mix information in the forward direction between tokens readily: given the prompt

$$
a = \mathtt{The \; sky \; is \; blue.}
$$

For the first transformer block we have

$$
a_g = \mathtt{The \; sky \; is \; blue \; fail}
$$

but if we restrict the output to $[:, :2, :]$ we have

$$
a_g = \mathtt{The \; sky \; Sho \; advancedblock}
$$

Upon some reflection, it may be appreciated that this observation does not by itself mean that transformer-based language models do not share information between their tokens (more accurately the model's activations at each token).  This is because `llama` and other similar causal language models are trained to predict some passage's next token, such that these models should never receive information from tokens to the right of the current token.  This was not the case for vision transformers, where key, query, and value projections are performed amoung all tokens. 

Specifically causal language models perform the query-key matrix multiplications as shown in the following figure: a token's query projection is multiplied to all previous (ie left-oriented) token's value projections (and possibly its own as well). This means that information in a clm transformer's block can be influence by any token to the left of the current token, but not to the right. 

![clm explanation]({{https://blbadger.github.io}}deep-learning/clm_explanation.png)

For causal language models, therefore, we should instead check that a model's activations for tokens *after* a given token are able to specify that input token if masked. 
Effectively we can ask whether enough information passes between tokens for the language model to determine the identity of the first two tokens, given the activations present in all following tokens.  This may be done by restricting the output to $[:, 2:, :]$, which gives us

$$
a_g = \mathtt{masterlex \; is \; blue2}
$$

indicating that for this input, the information present in the last three tokens (in the output of the first transformer module) is insufficient to determine the identity of the first two tokens. The same is observed for the input 'This is a prompt sentence', which for $[:, 2:, :]$ at the first output layer gives the representation

$$
a_g = \mathtt{lex \; Sho \; prompt \; sentence2}
$$

indicating that there is insufficient information transfer between later and earlier tokens to uniquely specify the masked tokens.  Similarly, taking the output of the first transformer block of all layers except the first token for the input ``Four score and seven years ago`` gives ``lex score2 seven years ago``.

It may be wondered whether a different metric might allow for more information to pass between tokens. In particular, consider the information that may pass between tokens via the multi-head attention transformation.  Recall from the last section that given vectors $q, k, v$ the attention operation is equivalent to a scaled version of the following

$$
A(q, k, v) = \mathrm {softmax} \left( q \cdot k \right) v
$$

although attention is usually calculated using matricies $Q, K, V$ such that the inner product $QK^T$ rather than the dot product is computed. Regardless, the information that passes between the query token (which for a causal language model is usually the last token) and the key token (typically one of the previous tokens).  

The dot product may be thought of as a measure of vector alignment which is a function of both distance and angle, and in that respect it is perhaps unsurprising that optimizing a pure distance norm metric would be insufficient for information transfer. Instead one could optimize angle metric, for example the cosine distance (see https://blbadger.github.io/language-discreteness.html#indirect-input-representation-via-cosine-similarity for a more thorough explanation).

Given the somewhat lengthy input prompt

$$
a = \mathtt{The \; sky \; is \; red \; or \; blue \; depending \; on \; the \; time \; of \; day.}
$$

we can test the accuracy of the input representation when the output of most but not all tokens is used to reconstruct the input. The direct input representation found cosine similarity for the trained 7 billion parameter Llama (taking the first transformer block only) for $[:, 3:, :]$ is

$$
a_g = \mathtt{XI \; Macdet \; red \; or \; blue \; depending \; on \; the \; time \; of \; day.}
$$

indicating that the information in all following tokens for this prompt is insufficient to specify any of the first three tokens. This is also the case when both cosine distance and $L^1$ distance are minimized simultaneously (via optimization of a linear combination of these measures), meaning that even if we minimize both aspects of the dot product information still does not flow sufficiently between tokens to uniquely identify them.

A little experimentation convinces one that these findings are not peculiar to the choice of input sentence but are found regardless of what input if given to the 7 billion parameter version of Llama. This observation is also not limited to the Llama model family, as Mistral 7b experiences the same incaccuracy: for [:, 1:, :] from the first transformer block we have for the input **Mario the man versus the idea**

$$
a_g = \mathtt{agenteli \; man \; versusPEG}
$$

and for **The sky is blue** the generated representaiton for the same output, the corresponding generated input $a_g$ is `sky<s> blue<s>`.

Being that we have seen larger models to be more and more capable at accurate input representation, it may next be wondered whether a larger model might be capable of more information transfer between tokens as well. Conceptually an arbitrarily large model would be capable of arbitrarily large information transfer between token model elements, but in particular it may be wondered if a somewhat larger version of Llama may be capable of accurate non-self token representation.

We test this by observing the ability of a model approximately twice the size of what has been used thus far: a 13 billion parameter version of Llama, again quantized to 8 bits per parameter.  We try the representation gradient descent procedure first using $L^1$ distance, for the input **This is a prompt sentence** we have `` is a prompt sentence`` if we use the information from all except the first token, ie $[:, 1:, :]$ from the first transformer block.

Likewise, for the 13 billion parameter version of llama when attempting input representation via cosine distance for the input 

$$
\mathtt{Mario \; the \; man \; versus \; mario \; the \; idea} 
$$

we have ``Mario the man versus mario</s> idea`` if none of the model output is masked, and

$$
\mathtt{depois \; the \; man \; versus \; mario</s> \; idea}
$$

if the first token's output is masked after the first transformer layer. Increasing the number of transformer blocks results in a less-coherent input representation and does not improve masked token identification.

For the even larger 30 billion parameter version of Llama, given the same target input as above and taking all output except that of the first token ($[:, 1:, :]$) of the first transformer block, after optimizing the $L^1$ distance on this output (using indirect input representation) the input representation $a_g$ is

$$
\mathtt{'rell \; The \; man \; versus \; mario \; The \; idea}
$$

And similarly for the input **This is a prompt sentence** we have the generated input representation $a_g$ of ``Dragon is    prompt sentence <s>`` if the output of the first token is masked for the same model. 

When we test this larger model's input representation using a cosine distance metric, we again see that there is insufficient information to uniquely identify the first token: for the target input **The sky is blue.** the model yields an $a_g$ of ``quietly sky is blue<s>`` and for **Blue is the sky.** the representation is ``quietly is The sky<s>``.

Upon testing a deeper layer (here the 8th transformer block) with the target input **This is a prompt sentence.** using an $L^1$ metric on $[:, 1:, :]$ we have

$$
a_g = \mathtt{delta \; Gray \; wer \; prompt \; sentenceInd}
$$

And for the target input **Mario the man versus Mario the idea** minimizing cosine distance on all tokens ($[:, :, :]$) we have

$$
a_g = \mathtt{Mario \; Inside \; man \; versus \; Mario \; Fund \; Mor}
$$

but when the output of the first token is masked,

$$
a_g = \mathtt{dbaum \; man \; versus \; Mario \; processes \; idea}
$$

In the [last section](https://blbadger.github.io/language-discreteness.html#cosine-loss-optimization-and-model-output) it was observed that the cosine similarity metric could be used to find somewhat accurate representations of an input for an untrained Llama 7b, but only if the output of the attention layer rather than the output of the first transformer block (attention layer followed by two fully connected layers) was used to perform optimization upon.  It may then be wondered if we might observe a similar phenomenon here: perhaps if we optimize the cosine similarity of an attention layer, the identity of a token whose output is masked may be found. 

For untrained langauge models the representation for 7b and 30b llama is mostly the same as trained, but the untrained model exhibits one advantage: given the first transformer block's self-attention transformation alone we can find accurate non-self token input representations using $\cos \phi$ as our metric: given the prompt **Mario the idea versus Mario the man** and the outputs $[:, 1:, :]$ we have a top-5 encoding of

```
Mario man the the Mario man idea
the the man Mario versus the man
la versus versus man the Mario Mario
isi Mario Mario idea idea versusperiment
Price  idea versus manouvelle
```

and for **Geralt of Rivia** we have a representation ``Ger Rivalt Rivia`` given the output of all except the first token ($[:, 1:, :]$) and ``Ger Riv of Rivia`` for all except the first two tokens, $[:, 2:, :]$. As observed before, the tokens are often permuted such that the masked token is not necessarily in the correct place. It should be noted that accurate input representations for any input tokens are not obtained once more than one attention layer is used, or once attention is followed by the MLP layers that normally comprise a transformer block.

An aside: the keen observer will not that the information from the last token is able to apparently travel to earlier tokens, which should be impossible if each attention dot-product operation only takes place in the reverse direction. This is due to the a lack of a causal mask in the attention module, which would normally be added during the causal language modeling training process.

On the other hand, some experimentation is sufficient to convince one that this is not the case for trained models: for a 7 billion parameter trained Llama, the attention layer output does not yield accurate masked token identity.  

It may be wondered why there is insufficient information passed between tokens for a trained model: which operation is required to pass information sufficiently between one layer and the next for a single token?  This is easy to answer, and removal of each transformation in the self-attention module of a trained Llama ([here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py) is the source code) shows that the removal of the residual connection across the self-attention layer is alone capable of preventing even a single input token from being correctly identified (given complete outputs) regardless of what modules follow. As there are not residual connections between token elements in transformer models, it is little surprise in that sense that there is insufficient information to correctly identify the correct input token if that token's output is masked.

Likewise, for an untrained Llama 7b we find the opposite phenomenon: removal of the residual connection across the first transformer block's self-attention layer results in little change in input representation ability: given the input 'Mario the idea versus Mario the man' we have 'Mario man the the versus man idea' without residuals and 'Mario man the the Mario man idea' with residuals intact.  Once again we find that input representation is quite inaccurate when more than one transformer layer (with or without residuals) is stacked. 

Before concluding that insufficient information passes between tokens (via attention transformations) for accurate non-self token identification, however, consider that one might have made a similar conclusion for self-token identification when using models smaller than ~1 billion parameters, but larger models (7 billion parameters and larger) do turn out to be capable of accurate self token representation (due to their increase in layer width, which parallels a communication line's bandwidth).  We may wonder if the same phenomenon would not occur for non-self tokens.

A little experimentation convinces us that an increase in model size is indeed sufficient for non-self token representation: if we initialize one transformer module of the same dimensions as the 70 billion parameter Llama-2 as follows

```python
llama_config_kwargs = {
    'hidden_size': 8192,
    'intermediate_size': 4*8192,
    'num_hidden_layers': 1,
    'num_heads': 64
}

# Initializing a LLaMA llama-70b style configuration
configuration = LlamaConfig(**llama_config_kwargs)
model = LlamaForCausalLM(configuration).to(0)
```

and using indirect input representation (minimizing an $L^2$ metric on the output) we find that the generated input for an output where the first three tokens are masked such that the output is `[:, 3:, :]` for the input **Mario the man versus Mario the idea** we have a top-2 token representation of

```
Mario the thecenwij the
versus Mario idea man versus
```

and if all tokens but the last are masked (`[:, -1, :]`) we find

```
versus census thegg canad Mariort
```

where the non-self tokens for `Mario, versus, the` are correctly found, even if they are not in the correct token index (note that as for other randomly initialized, untrained models the exact input representation is somewhat dependent on the weight initializations).

For the trained 65 billion parameter Llama, however, the same prompt 'Mario the man versus Mario the idea' for the output of the first transformer block without the first token (`[:, 1:, :]`) representation of

$$
a_g = \mathtt{absorF \; man \; versus \; Mario \; The \; idea}
$$

which indicates that the self-token representation improves upon training, but the non-self token representation ability does not (the full `[:, :, :]` representation is `MarioThe man versus Mario The idea`). Similarly, for `[:, -1, :]` we have a_g = `absor verb ForumFOimerwho idea`, meaning only the last token is found. This is not peculiar to the chosen prompt: for example, not one of the top-5 representations for **The sky is blue.** finds the masked first token for `[:, -1, :]`,

```
ignation sky is blue?
CC Skyson Blue when
okaysky COblueriften
broke publishebrace 
bo System AskFur
```

or for different models: the trained 70 billion parameter Llama-2 gives for **Mario the idea versus the man** a top-5 representation for `[:, 1:, :]` of the first transformer block of

```
Priceww idea versusww man
existedaire vsww
 the ideasww the Man
histor fils winningining cacheono
limited mostly Pitts await
```

none of which correctly identifies the masked first token.  

Even if we restrict the [vocabulary](https://blbadger.github.io/language-discreteness.html#restricted-vocabulary-input-representation) such that the represented tokens may only be those that are found in the following sentence: `A sky is blue or red depending on the time of day, and further depending on the size of the vocabulary present.` for the input **The sky is blue**, we find an $a_g$ of ` size sky is blue<s>`.

It may therefore be wondered just how large a model must be in order for non-self tokens to be accurately represented. For untrained models, the chance of accurate masked token representation depends on parameter initialization but increases with model size: perhaps 1/5ths of models with a hidden dimension of 4096 (Llama 7b size) but more than 4/5ths of models with a hidden dim of 8192 (llama 70b).

To conclude, we find that there is insufficient information passing from one token to another via self-attention for trained large language models to uniquely identify inputs in which the corresponding token's hidden layer activations are masked.  The lack of information transfer between tokens is observed regardless of whether a distance, angle, or combination of distance and angle metric is used to optimize the input's representation similarity to a partially masked target. If this model is representative of others, the finding implies that transformer-based language models typically do not transfer sufficient information between tokens (via QKV matrix multiplications) for unique token identification.

This conclusion is similar to what was found for vision transformers: unlike MLP mixers (which accurately represented masked tokens), vision transformers were found to be unable to transmit sufficient information between tokens for accurate visual representation of tokens using the information in the hidden layers of other tokens.

### Mixer Information Transfer

In the last section it was observed that large untrained language models are capable of accurate non-self token representation (although the tokens may not be in the correct sequence index), but that this ability vanishes during the training process. Similar conclusions were reached [elsewhere](https://blbadger.github.io/transformer-features.html) for trained dot product attention-based transformers but curiously not for MLP mixer models that replace the self-attention operations with matrix multiplication operations. It may therefore be wondered whether an MLP mixer model might also be capable of accurate non-self token representation for language, as is the case for vision.

An implementation of a language mixer may be found on [this page](https://blbadger.github.io/smaller-lms.html), but the main idea is that one replaces the self-attention transformations with a 1-dimensional convolution (such that all feature neurons from a certain input element are multiplied by the same weight, and all these multiplied values are added together) to share weights among a token's feature neurons, but not between sequence elements. There are actually two sequential convolutions (separated by a nonlinear activation) connecting each pair of token's feature neurons, and we use a triangular mask on the convolution weights in order to allow for causal language modeling.

Note that in this section, we use a 4096-size tokenizer fitted to the TinyStories 2M dataset such that the name `Mario` now takes two tokens.

When we test the untrained mixer on both self- and non-self token representation, we find that the model size requirements for accurate input representation appear to be similar to the transformers, or perhaps slightly more : for example, given the output of all tokens (`[:, :, :]`) of the input

**Mario, the Idea, versus Mario, the Man**

For untrained models, one mixer block exhibits a perfect input representation of `Mario, the Idea, versus Mario, the Man` for $d_{model}=32$ and larger. For the mixer with expanded convolutions between tokens, non-self representation is less impressive: an untrained mixer with $d_{model}=512$ with an expansion factor of 1 yields

```
it, but ario, the Idea, versus Mario, the Man
```

And the situation is not helped with training: recall for one transformer block of an untrained $d_{model}=64$ mixer, we have a perfect input representation for `Mario, the Idea, versus Mario, the Man`, but we find that non-self token representation is much worse. For `[:, 1:, :]` we have (ignoring trailing tokens) from the first mixer block

```
selessonario, the Idea, versus Mario, the Man
```

which is no better than the untrained model's representation,

```
Cario, the Idea, versus Mario, the Man
```

Similarly, a trained $d_{model}=256$ does not correctly identify the first token, and gives a representation of

```
iario, the Idea, versus Mario, the Man
```

With a non-expanded (ie 'flat') convolution between sequence elements, non-self tokens may be accurately found: for untrained mixers we have for [:, 1:, :] at $d_{model}=512$ (given the first block),

```
Mario, the Idea, versus Mario, the Man
```

This accurate self- and non-self token representation persists after training for that model, if we take the information in a slightly deeper layer (say the output `[:, 1:, :]` of the second mixer or fourth mixer block). What is more impressive is the trained flat mixer's representation given multiple masked tokens: for the output `[:, 3:, :]` of the second block we get a perfect input representation.

Reducing the hidden dimension leads to inaccurate non-self representation for the flat masked mixer, as for $d_{model}=256$ the representation is `iario, the Idea, versus Mario, the Man`. It may be wondered if a linear combination of 1-dimensional convolutions (ie in parallel rather than in series) would yield even better non-self token representation, and after some testing it does indeed: before training, if we have two parallel 1D convolutions rather than one (added together to make our linear combination) then representation is extremely acurate: given `[:, 10:, :]` of the *last* mixer block (8) for a relatively small model with $d_{model}=64$, the input **Mario, the Idea, versus Mario, the Man** is perfectly represented. 

If we look deeper into an untrained flat mixer model, on the other hand, for $d_{model}=1024$, at block 8 we find 

```
Marario, the Idea, versus Mario, the Man
```

and for $d_{model}=2048$ at block 8 we do find both accurate self and non-self token representation. This is also true if we scale the depth of the model without increasing the width, for example for $d_{model}=1024$ and $n=24$ layers we have also have a perfect self- and non-self representation for an untrained model (note that the n=8 layer version above was not as capable). What is more remarkable is that even if the first three tokens are masked, the representation is still perfect.

### Model Invertibility

At the end of the last section, we have seen that an untrained masked mixer's final layer representation is sufficiently powerful to identify nearly all or all of a short prompt's input tokens. 

Masked mixer input representation ability decreases somewhat upon training. For example, the flat $d_{model}=1024$ explored above retains perfect input representation of **Mario, the Idea, versus Mario, the Man** for one mixer block, but after 8 blocks we have for `[:, 1:, :]`

```
Mgiftio, the Idea</s>versus Mario, the Man
```

which is slightly worse than the perfect representation present in this model before training commenced. Note, however, that the non-self token 'M' was accurately found.

This flat mixer representation is more accurate than that obtained from a similarly sized transformer (even when using the same 4096-size tokenizer and training dataset): a trained $d_{model}=256, \; n=8$ transformer (llama style) model yields for `[:, 1:, :]` the input *Mario, the Idea, versus Mario, the Man*

`pictureario, the Idea, th let fterMarioiMan`

where some self- and the non-self token are incorrectly identified, although for a trained transformer with $d_{model}=512$ we have

`s. They whistch whistsat panstayou're snowpatophch whistsat Man`

From these results it is apparent that masked mixers remain at least somewhat invertible after training, whereas transformers are less so.
## The boundary of the logistic map

### The logistic map is a conjugate of a Julia set

The logistic equation

$$
x_{n+1} = rx_n (1 - x_n) \\
\tag{1} \label{eq1}
$$

is a model for growth rate that displays many features of nonlinear dynamics in a nice one-dimensional form. For a summary on some of these interesting properties, see [here](/logistic-map.md).

Why aren't values for r>4 shown in any of the graphs for the logistic equation? This is because these values head towards infinity: they are unbounded.  Which values are bounded? This question is not too difficult to answer: for r values of 0 to 4, initial populations anywhere in the range $[0, 1]$ stay in this range.  

What if we iterate \eqref{eq1} but instead of $x$ existing on the real line, we allow it to traverse the complex plane? Then we have

$$
z_{n+1} = rz_n (1 - z_n) \\
\tag{2} \label{eq2}
$$

Where $z_n$ (and thus $z_{n+a}$ for any $a$) and $r$ are points in the complex plane using the Gaussian plane:

$$
z_n = x + yi \\
r = x + yi
$$

Now which points remain bounded and which head towards infinity?  Let's try fixing $r$ in place, say at $3$ and seeing what happens to different starting points in the complex plane after iterations of \eqref{eq3}.  

![complex map]({{https://blbadger.github.io}}/logistic_map/logistic_boundary_3_fixed_r.png)

The result looks very similar to a [Julia set](/julia-sets.md), is defined as the set of points bordering initial points whose subsequent iterations of 

$$
z_{n+1} = z_n^2 + c
\tag{3} \label{eq3}
$$

that diverge (head to infinity) and points whose subsequent interations do not diverge.

For $c = -0.75 + 0i$, this set is shown below (with an inverted color scheme to that used for the logistic map for clarity)

![julia map]({{https://blbadger.github.io}}/logistic_map/julia_-0.75.png)

These maps look extremely similar, so could they actually be the same?  They are indeed!  The logistic map \eqref{eq1} and the quadratic map \eqref{eq3} which forms the basis of the Julia sets are conjugates of one another: they contain identical topological properties for a certain $a$ value, or in other words transforming from one map to another is a homeomorphism.  

This can be shown as follows: a linear transformation on any variable is the same as a (combination of) stretching, rotation, translation, or dilation.  Each of these possibilities does not affect the underlying topology of the transformed space, which one can think of as being true because the space is not broken apart in any way.  

This being the case, if it is possible to transform the logistic map \eqref{eq1} into the quadratic map \eqref{eq3} with the linear transformation $h(x) = ax+b$, then these maps are topologically equivalent.  To test this, with the following definitions for the logistic and quadratic equations,

$$
f(x) = rx(1-x) \\
g(x) = x^2 + c \\
$$

is there some linear transformation

$$
h(x) = ax+b 
$$

that makes

$$
h(f(x)) = g(h(x))
$$

true?

This can be expanded for clarity:

$$
a(rx_n(1-x_n)) + b = (ax_n + b)^2 + c \\
arx_n - arx_n^2 + b = a^2x_n^2 + 2abx_n + b^2 + c \\
$$

and now  substituting useful values for $a$ and $b$ to remove terms,

$$
b = r/2 \implies -arx_n^2 + b = a^2x_n^2 + b^2 + c \\
a = -r \implies b = b^2 + c \\
c = b - b^2
$$

putting these expressions together, the conjugacy is valid whenever

$$
c = \frac{r}{2} \left( 1-\frac{r}{2} \right)
$$

Therefore the quadratic map is by a homeomorphism (in particular, a linear transformation) equivalent to the logistic map, and thus the two are topologically equivalent.  Now the necessity of the seemingly arbitrary value of $a=-0.75$ for the Julia set above is clear: $r=3$ was specified for the logistic map, and by our homeomorphism then $c= \frac{3}{2}(1-\frac{3}{2}) = -3/4$.  

All this is to say that for any $r$ value, the logistic map is equivalent to a Julia set where $c=\frac{r}{2}(1-\frac{r}{2})$. Just for fun, let's zoom in on the logistic boundary map above, focusing on the origin.

 
![complex map]({{https://blbadger.github.io}}/logistic_map/logistic_bound_fixed_r.gif)
 
An aside: for most decreases in scale, more iterations are required in order to determine if close-together coordinates will diverge towards infinity or else remain bounded.  But this is not the case for a zoom towards the origin: no more iterations are required for constant resolution even when the scale has increased by a factor of $2^{20}$.

### The logistic map and the Mandelbrot set

Julia sets may be connected or disconnected.  As the logistic map \eqref{eq1} is a homeomorphism of the quadratic map Julia set \eqref{eq3}, identical set topologies should be observed for the logistic map. Observing which values $z_0$ border bounded and unbounded iterations of the logistic map in the complex plane

$$
z_{n+1} = rz_n(1-z_n)
\tag{2}
$$

from $r=2 \to r=4$ (both on the real line) for the logistic map, we have

 
![complex map]({{https://blbadger.github.io}}/logistic_map/logistic_boundary_fixed_r.gif)
 

This is analagous to observing Julia sets where $c=0 \to c=-2$.  

What happens if we instead fix the starting point and instead plot the points $r = x + yi$ for which the logistic map \eqref{eq2} diverges versus points that do not diverge? For the starting point $z_0 = 1/2 + 0i$, we get 

![complex map]({{https://blbadger.github.io}}/logistic_map/logistic_bound_0.5.png)

This figure resembles a double-sided [Mandelbrot set](/mandelbrot-set.md).  When we zoom in, we can find many little mandelbrot sets (in reversed x-orientation).  The Mandelbrot set is the map where the initial value $z_0$ is fixed at the origin and the points for which $c$ diverge or not are potted.  This is analagous to fixing the starting point for the logistic equation and then looking at which $r$ values cause future iterations to diverge using the $c=r/2(1-r/2)$ found above:

$$
z_{next} = z_n^2 + \frac{r}{2} \left( 1-\frac{r}{2} \right)
\tag{4}
$$

For $z_0 = 0+0i$, this is identical to the logistic map above (which stipulates the starting point $z_0 = 1/2 + 0i$ is not at the origin). 

Why are the starting points different for these maps, even though the output is the same?  If one iterates the logistic map from point $x_0 = 0+0i$, no value of $r$ will cause the trajectory to diverge (as it stays at the origin).  Observing the Julia set and logistic set above, it is clear that the map has been translated by $1/2 + 0i$ units and has also been compressed.  This is true for any Julia set with an equivalent logistic set, which is why we have to start at $1/2 + 0i$ for the latter to make an identical Mandelbrot set to the former.

To make identical maps for any one starting point $z_0 = x + yi$, we may use the homeomorphic functions (see above) before the change of variables which are

$$
a(rx_n(1-x_n))+b \\
(ax_n+b)^2 + c \\
a=r/2, \; b=-r,\; c=\frac{r}{2} \left( 1-\frac{r}{2} \right)
$$

Iterating either equation gives the same map of converging versus diverging iterations for various $r$ values in the complex plane for any starting value $z_0$.  For example, $z_0 = 1/4$ gives

![complex map]({{https://blbadger.github.io}}/logistic_map/logistic_mandelbrot.png)

### Zoom and extended logistic set

The Mandelbrot set is well known for its fractal structure that yields interesting patterns at arbitrary scale.  The logistic map boundary is the same in this respect: for example, increasing scale at the point 

$$
(3.58355 + 0i)
$$

yields

![complex map]({{https://blbadger.github.io}}/logistic_map/logistic_bound_zoom.gif)

Note the identical objects found in a Mandelbrot set zoom on the point 

$$
(-1.633 + 0i)
$$

A programmatic aside: the previous and following zooms were made using a different method than elaborate on previously.  Rather than modifying the ogrid directly, `a_array` and `z_array` are instead positioned (using addition and subtraction of real and imaginary amounts) and then scaled over time as follows:

```python
def mandelbrot_set(h_range, w_range, max_iterations, t):
	y, x = np.ogrid[1.4: -1.4: h_range*1j, -1.8: 1:w_range*1j] # note that the ogrid does not scale

	a_array = x/(2**(t/15)) - 1.633 + y*1j / (2**(t/15)) # the array scales instead
	z_array = np.zeros(a_array.shape)
	iterations_till_divergence = max_iterations + np.zeros(a_array.shape)
  
  ...
```

This method requires far fewer iterations at a given scale for an arbitrary resolution relative to the method of scaling the ogrid directly, although more iterations are required for constant resolution as the scale decreases.  The fewer iteration number necessary is presumably due to decreased round-off error: centering the array and zooming in on the origin leads to approximately constant round-off error, whereas zooming in on a point far from the origin leads to significant error that requires more iterations to resolve.  I am not completely certain why a constant number of iterations are not sufficient for constant resolution using this method, however. 

![mandelbrot map]({{https://blbadger.github.io}}/logistic_map/mandelbrot_zoom_frame.gif)


Also as for the Mandelbrot set, we can change the map by changing the starting point of the logistic map \eqref{eq2}. For example, moving the starting point from $x_0 = 0 \to x_0 = 2$ yields

![complex map]({{https://blbadger.github.io}}/logistic_map/logistic_boundary_fixed_start.gif)
## Logistic Map

### Introduction: periodic trajectories in the logistic map

The logistic map was derived from a differential equation describing population growth, popularized by Robert May. The dynamical equation is as follows:

$$
x_{n+1} = rx_n (1 - x_n) 
\tag{1}
\label{eq1}$$

where r can be considered akin to a growth rate, $x_{n+1}$ is the population next year, and $x_n$ is the current population.  Population ranges between 0 and 1, and signifies the proportion of the maximum population.

Let's see what happens to population over time at a fixed r value.  To model \eqref{eq1}, we will employ numpy and matplotlib, two indispensable python libraries.

```python
#! python3
# import third-party libraries
from matplotlib import pyplot as plt 
import numpy as np 

# initialize an array of 0s and specify starting values and r constant
steps = 50
x = np.zeros(steps + 1)
y = np.zeros(steps + 1)
x[0], y[0] = 0, 0.4

r = 0.5

# loop over the steps and replace array values with calculations
for i in range(steps):
	y[i+1] = r * y[i] * (1 - y[i])
	x[i+1] = x[i] + 1

# plot the figure!
fig, ax = plt.subplots()
ax.plot(x, y, alpha=0.5)
ax.set(xlabel='Time (years)', ylabel='Population (fraction of max)')
plt.show()
```

When $r$ is small (less than one, to be specific), the population heads towards 0.  Here $r=0.5$:

![towards 0]({{https://blbadger.github.io}}/logistic_map/logistic_time_r0.8.png)


As $r$ is increased to 2.5, a stable population is reached:

![stable iterations]({{https://blbadger.github.io}}/logistic_map/logistic_time_r2.5.png)

This is called a period one trajectory because it takes one iteration to return to the starting population size.  Because period one occurs when $x_{n+1} = x_n$, we can algebraically determine the population values reached.  At $r=2.5$ we have

$$
x_n = 2.5x_n(1-x_n) \\
0 = -2.5x_n^2 + 1.5x_n = x_n \left( -\frac{5}{2}x_n+\frac{3}{2} \right) \\
x_n = 0, \; x_n = 3/5 \\
$$

where the value of $x_n = 3/5$ is an attractor for all initial values except for 0, and $x_{n+1} = 0$ only occurs for $x_0 = 0$. An attractor in a dynamical system such as this is a point that other points head towards, and in this $x_n = 3/5$ is an attractor for all starting points $x_0 \in (0, 1)$.

How can one tell that $x_n = 3/5$ is an attractor but $x_n = 0$ is not?  One way is simply to start from a number of different points and see where future iterations end up: this was used above.  But this method can be difficult and imprecise when many values are to be tested, so it is often best to try to determine whether or not a point is an attractor by using the following analytical method.

Stability for one-dimensional systems may be determined using linearization, and for iterated equations like the logistic map this involves determining whether or not the absolute value slope of the derivative of $f(x)$ is less than one, or $\lvert f'(x) \rvert < 1$.  If so, the point is stable but if $\lvert f'(x) \rvert > 1$, it is unstable (and if it equals one, we cannot say).  For the logistic equation, $f'(x) = r-2rx$ and so at $r=2.5$,

$$
f'(x) = r - 2rx_n \\
f'(x) = 3.5-7x_n \\
f'(0) = 3.5-0 > 1
$$

and therefore $x=0$ is unstable.  On the other hand, $f'(3/5) = 3.5-4.2 = -0.7$ which means that $x=3/5$ is a stable point of period 1.  As we shall see below, stable points of finite period act as attractors, such that points in $(0, 1)$ eventually end up on the point $x=3/5$ given sufficient iterations.

Returning to models of \eqref{eq1{, at $r = 3.1$ the population fluctuates, returning to the starting point every other year instead of every year.  This is called 'period 2':

![period 2 logistic]({{https://blbadger.github.io}}/logistic_map/logistic_time_r3.1.png)

At this r value, can find points where $x_{n+1} = x_n$ (meaning points of period one),

$$
x_n = rx_n(1-x_n)  \\
0 = x_n(2.1-3.1x_n) \\
x_n = 0, x_n \approx 0.677
$$

but some quick numerical experimentation shows that these points are unstable: any slight deviation from the values (and indeed any finite approximation of 0.677...) will not result in $x_{n+1} = x_n$.  

Points of period 2 may be found as follows:

$$
f(f(x)) = f^2(x) = x \\
x = r^2x(1-x)(1-rx(1-x)) \\
0 = x(-r^3x^3 + 2r^3x^2 - (r^3+r^2)x + r^2 - 1)
$$

which when $r=3.1$ has a root at $x=0$.  The other roots may be found using the rather complicated cubic equation, and are $x\approx 0.76457, x \approx 0.55801, x \approx 0.67742$.  Note that the two unstable period-1 points are included (see below), but that there are two new points.  To see if they are stable,  we can check if $\lvert (f^2)' \rvert < 1$, and indeed as $(f^2)'(x) = -4r^3x^3 + 6r^3x^2 - 2\left(r^3+r^2\right)x + r^2$, and substituting both $x=0.55801$ and $x=0.76457$ yields a number around $\lvert 0.5899.. \rvert < 1$, meaning that both points are stable.  Therefore both are attractors, as seen in the previous numerical map.  On the other hand, $(f^2)'(0.67742) = 1.21$ and $(f^2)'(0) = 9.61$, neither of which are between positive and negative one and thus both points are unstable.  The point $x\approx 0.67742$ would be stable if the period had not increased: this is a period-doubling bifurcation. 

Note that 'period' on this page signifies what is elsewhere sometimes referred to as 'prime period', or minimal period.  Take a fixed point, for example at $r=2.5$ this was found to be located $x=3/5$.  This has period 1 because $x_{n+1} = x_n$, but it can also be thought to have period 2 because $x_{n+2} = x_n$ and period 3, and any other finite period because $x_{n+k} = x_n \forall k$.  This is why we found period-1 points when trying to compute period-2 values above.

But there is a clear difference between this sort of behavior and that where $x_{n+2} = x_n$ but $x_{n+1} \neq x_n$, where the next iteration does not equal the current but two iterations in the future does.  The last sentence is true for the logistic map where $r=3.1$, and we can call this 'prime period 2' to avoid ambiguity.  But for this and other pages on this site, 'prime' is avoided as any specific value referred to by 'period' is taken to mean 'prime period'.  

at $r = 3.5$, the trajectory is period 4, as it takes 4 iterations for the population to return to its original position:

![period 4 logistic]({{https://blbadger.github.io}}/logistic_map/logistic_time_r3.5.png)

As $r$ increases, trajectories of $8,\; 16,\; 32,\; 64...$ exist: given any natural number that is the power of two, there is some $r$ value range for which the logistic map trajectory has that number as a periodic cycle.

### Aperiodic trajectories in the logistic map

For $r=3.7$, the (prime) period is longer than the iterations plotted and is actually infinite, and therefore the system is called aperiodic.  To restate, this means that previous values of the logistic equation are never revisited for an aperiodic $r$ value.  The ensuing plot has points that look random but are deterministic.  The formation of aperiodic behavior from a deterministic system is termed mathematical chaos.

![aperiodic logistic]({{https://blbadger.github.io}}/logistic_map/logistic_time_r3.7.png)

Are there any non-prime periodic points?  Looking for points of period 1 (keeping $r=3.7$), we find two:

$$
x_n = rx_n(1-x_n) \\
0 = x_n(2.7-3.7x_n) \\
x_n = 0, \; x_n = 27/37
$$

Both are unstable, as $f'(0) = 3.7 - 2 \cdot 3.7 \cdot 0 > 1$ and $f'(27/37) = 3.7 - 54/37 > 1$.  For points of period 2, the cubic formula can be used to find

$$
0 = x_n(-r^3x_n^3 + 2r^3x_n^2 - (r^3+r^2)x + r^2-1) \\
x_n = 0, \; x_n = 27/37 \; x_n \approx 0.88, \; x_n = 0.39
$$

the fixed points are found again, and in addition two more points are found.  All are unstable (which can be checked by observing that all points fail the test of $\lvert (f^2)'(x_n) \rvert < 1$).  What makes the $r=3.7$ value special is that for any given integer $k$, one can find periodic points with period $k$.  But as $k+1$ also exhibits periodic points, the logistic map with $r=3.7$ only has a prime periodic point at $x_n = \infty$, which is to say that the map has no finite (prime) periodic points and therefore aperiodic.

As demonstrated by Lorenz in his [pioneering work on flow](https://journals.ametsoc.org/doi/abs/10.1175/1520-0469(1963)020%3C0130:dnf%3E2.0.CO;2), nonlinear dissipative systems capable of aperiodic behavior are extremely sensitive to initial conditions such that long-range behavior is impossible to predict.  

Observe what happens when the starting population proportion is shifted by a factor of one ten-millionth with $r=3.7$:

![aperiodic logistic shift]({{https://blbadger.github.io}}/logistic_map/logistic_time_r3.7_comp.png)

The two trajectories are nearly identical but then diverge.  This is sensitivity to initial conditions, and has been shown by Lorenz to be implied by and to imply aperiodicity (more on this below).

In contrast, a relatively large shift of a factor of one hundreth (3 to 3.03) in initial population leads to no change to periodicity or exact values at $r=2.5$ (period 1):

![periodic logistic shift]({{https://blbadger.github.io}}/logistic_map/logistic_time_2.5_hundreth.png)


or at $r=3.5$, period 4, the same change does not alter the pattern produced:

![period 4 logistic shift]({{https://blbadger.github.io}}/logistic_map/logistic_time_3.5_hundreth.png)


even a large change in starting value at $r=3.55$ (period 8), from $x_0=0.3$ to $x_0=0.5$ merely shifts the pattern produced over by two iterations but does not change the points obtained or the order in which they cycle:

![period 8 logistic shift]({{https://blbadger.github.io}}/logistic_map/logistic_large.png)


### Patterns in the aperiodic orbit map

Information from iterating \eqref{eq1} at different values of $r$ may be compiled in what is called an orbit map, which displays the stable points at each value of $r$.  These may also be though of as the roots of the equation with specific $r$ values. 

To do this, let's have the output of the logistic equation on the y-axis and the possible values of $r$ on the x-axis, incremented in small units.  
```python
#Import third-party libraries
import numpy as np 
import matplotlib.pyplot as plt 

def logistic_map(x, y):
	'''a function to calculate the next step of the discrete map.  Inputs
	x and y are transformed to x_next, y_next respectively'''
	y_next = y * x * (1 - y)
	x_next = x + 0.0000001
	yield x_next, y_next
```
For high resolution, let's use millions of iterations.  Note that the logistic equation explodes to infinity for $r > 4$, so starting at $r = 1$ let's use 3 million iterations with step sizes of 1/100,000 (as above)
```python
steps = 3000000

Y = np.zeros(steps + 1)
X = np.zeros(steps + 1)

X[0], Y[0] = 1, 0.5

# map the equation to array step by step using the logistic_map function above
for i in range(steps):
	x_next, y_next = next(logistic_map(X[i], Y[i])) # calls the logistic_map function on X[i] as x and Y[i] as y
	X[i+1] = x_next
	Y[i+1] = y_next
```

Let's plot the result! A dark background is used for clarity.
```python
lt.style.use('dark_background')
plt.figure(figsize=(10, 10))
plt.plot(X, Y, '^', color='white', alpha=0.4, markersize = 0.013)
plt.axis('on')
plt.show()
```

![logistic orbit map]({{https://blbadger.github.io}}/logistic_map/logistic_period.png)


By looking at how many points there are at a given $r$ value, the same patter of period doubling may be observed. The phenomenon that periodic nonlinear systems become aperiodic via period doubling at specific ratios was found by Feigenbaum to be a [near-universal feature](https://www.ioc.ee/~dima/mittelindyn/paper4.pdf) of the transition from periodicity to chaos.

Let's take a closer look at the fuzzy region of the right. This corresponds to the values of $r$ which are mostly aperiodic, but with windows of periodicity.  There are all kinds of interesting shapes visible even in the aperiodic sections:

![aperiodic logistic orbit map]({{https://blbadger.github.io}}/logistic_map/logistic_period_zoom2.png)

What do these shapes mean? It is worth remembering what this orbit diagram represents: a collection of single iterations of \eqref{eq1} with very slightly different $r$ values, the previous iteration population size being the input for the current iteration. This is why the chaotic regions appear to be filled with static: points that are the result of one iteration of the logistic equation are plotted, but the next point is mostly unpredictable and thus may land anywhwere within a given region.  The shapes, ie regions of higher point density, are values that are more common to iterations of changing $r$ values.

Are these same values more common if $r$ is fixed and many iterations are performed at various starting population size values? Let's take $ r \approx 3.68$, where the orbit diagram exhibits higher point density at population size $p \approx 0.74$.  THe proportion of iterations near each value may be plotted in R,

```R
# data taken from 900 iterations of the logistic equation starting at 3, 6, and 9 (+ 0.0000000000000001) at r=3.68
library(ggplot2)
data1 = read.csv('~/Desktop/logistic_0.3_4.csv', header=T)
a <- ggplot(data1, aes(value))

a + geom_dotplot('binwidth' = 0.01, col='blue', fill='red') +
    xlim(0, 1) +
    theme_bw(base_size = 14) 
```

Here the x-axis denotes the population size, and the y-axis denotes the proportion of iterations in this region:

![logistic proportions]({{https://blbadger.github.io}}/logistic_map/logistic_probs_3.68.png)

and thus there are are indeed more iterations near $0.74$ than elsewhere, holding $r$ constant and iterating from a few different starting points. 

Why are certain points more common than others, given that these systems are inherently unpredictable?  Iterating \eqref{eq1} at $r=3.68, x_0=0.3$ as shown above provides a possible explanation: the population only slowly changes if it reaches $x \approx 0.728$ such that many consecutive years (iterations) contain similar population values.

![r=3.68 iterations]({{https://blbadger.github.io}}/logistic_map/logistic_time_3.68.png)

Each point in the aperiodic logistic map's trajectory is unstable, meaning that every value arbitrarily close to the point of interest will eventually become arbitrarily far away (within the bounds of the function's image).  But these numerical test suggest that points near $0.7282...$ move comparatively less than others.

Fixed points for $r=3.68$ may be found:

$$
x_n = rx_n(1-x_n) \\
x_n = 0, x_n = 0.728261\\
$$

Note that our relatively stable point above is very close to the (unstable) point of period 1. These points have linear stabilities of 

$$
f'(0) = 3.86\\
f'(0.72826) = -1.68
$$

Now $\lvert -1.68 \rvert > 1$ and so the point is unstable, but note that it is not very unstable as it is close to 1.  This means that subsequent iterations will diverge, just slowly, which explains why points near $x_n=0.7282...$ tend to have subsequent iterations near their current value.

This does not really explain why these points are more likely to be visited by any trajectory because points of every periodicity are possible, not just period 1.  To address this, one can ask how stable is this point compared to (unstable) points of other periodicities.  For period two points, 

$$
x = 0, \; x \approx 0.39, \; x \approx 0.72, \; x \approx 0.87 \\
(f^2(0))' = -113.2145 \\
(f^2(0.393))' = -79.06 \\
(f^2(0.728))' = -31.63 \\
(f^2(0.878))' = 17.61 \\
$$

But as there are infinitely many periodic points, this approach is flawed because we will never be done comparing stabilities at different points.  Instead, the following technique presented by Strogatz uses a geometric analytic argument: where does $x_{n+1}$ change the least for any given change in $x_n$?  This occurs near $x_n = 0.5$, as the absolute value of derivative of the logistic map $f'(x) = r(1-2x)$ is minimized when $x=1/2$. Therefore the most stable $x_n$ values exist for iterations following this value.  

This is best seen using a cobweb plot, which plots the equation of interest (here $rx(1-x)$) and the line $y=x$ in order to follow iterations geometrically.  The principle is that $y=x$ is used to reflect y-values onto the x-axis, thereby allowing iterations to be plotted clearly.  The procedure is as follows: given any point on the x-axis, find the y-value that corresponds to $rx(1-x)$.  This is the value of the next iteration of \eqref{eq1}, and this value is reflected back onto the x-axis by travelling horixontally until the $y=x$ line is reached.  The x-value of this point is the same as the y-value we just found, and therefore we can repeat the process of finding a subsequent iteration value by again finding the y-value of the curve $rx(1-x)$ and travelling horizontally to meet $y=x$.

For example, if $r=3.6$, $x_n = 0.5$ gives $x_{n+1} \approx 0.91$.  

![logistic analysis]({{https://blbadger.github.io}}/logistic_map/logistic_analysis_1.png)

and another iteration gives $x_{n+2} \approx 0.32$,

![logistic analysis]({{https://blbadger.github.io}}/logistic_map/logistic_analysis_2.png)

After six iterations (red, orange, yellow, green, blue, indigo respectively), there is

![logistic analysis]({{https://blbadger.github.io}}/logistic_map/logistic_analysis_full.png)

Now as one ranges over r values, the first four iterations after $x_n=0.5$ are in red, yellow, green, and blue respectively:

![logistic map semistable traces]({{https://blbadger.github.io}}/logistic_map/logistic_traces.png)

Overlayed onto the logistic map with successive iterations after $x_n=1/2$ in red, orange, yellow, green, blue, indigo, and violet respectively,

{% include youtube.html id='hUZ6s0AAXq0' %}

Aperiodicity with sensitivity to initial values, also called mathematical chaos, results when points are everywhere unstable.  But as this section has demonstrated, they are not necessarily equally unstable everywhere which illustrates a feature of chaos that differs from its English usage: mathematical chaos is not completely disordered. A more descriptive word might be 'mysterious' because these systems are unpredictable, even if they are partially ordered or are bounded by spectacular patterns, as seen in the following section.

This phenomenon of patterns arising amidst aperiodicity is also found in [prime gaps](https://blbadger.github.io/unpredictable-primes.html).


### Prediction accuracy

Consider the logistic map for, $r = 3.6$ and $r = 4$.  In contrast to complete disorder, short-range prediction is possible with chaotic systems even if long-range prediction is impossible.  Does relatively restricted aperiodicity (as seen for $r=3.6$) lead to an extension in prediction range?  Let's compare iterations of two starting values at a factor of a ten-thousanth apart (0.3 and 0.30003) to find out:

$r=3.6$
![logistic iterations shifted]({{https://blbadger.github.io}}/logistic_map/logistic_time_3.6_small.png)

$r=4$
![logistic iterations shifted]({{https://blbadger.github.io}}/logistic_map/logistic_time_4_small.png)

Observe that the divergence in values occurs later for $r=3.6$ than for $r=4$, implying that longer-range prediction is possible here.  Iterations of \eqref{eq1} at both values of $r$ are chaotic, but they are not equally unpredictable.

To illustrate this more clearly, here is a plot of the first iteration of divergence (100 iterations mazimum) of \eqref{eq1} at varying $r$ values, with $p_{01} = 3, p_{02} = 3.0003$.

To do this, the first step is to initialize an array to record $r$ values from 3.5 to 4, taking 500 steps.  The next step is to count the number of iterations it takes to diverge (with a maximum interation number of 100 in this case).  Arbitrarily setting divergence to be a difference in value greater than 0.15, 
```python
steps = 500
y = np.zeros(steps + 1)
r = np.zeros(steps + 1)
r[0] = 3.5
y[0] = 100

for i in range(500):
	X1 = 0.3
	X2 = 0.3 + 0.3**(t/30)

	for j in range(100):
		X1 = r[i] * X1 * (1-X1)
		X2 = r[i] * X2 * (1-X2)

		if np.abs(X1 - X2) > 0.15:
			break

	y[i+1] = j + 1
	r[i+1] = r[i] + 0.001
```

which when plotted yields

![logistic divergence]({{https://blbadger.github.io}}/logistic_map/logistic_divergence_3.0003.png)

Prediction ability (in length until divergence) tends to decrease with increasing $r$ values, but the exact relationship is unpredictable: some small increases in $r$ lead to increased prediction ability.

Increasing the accuracy of the initial measurement would be expected to increase prediction ability for all values of $r$ for \eqref{eq1}.  Is this the case? Let's go from $\Delta x_0 = 1 \to \Delta x_0 \approx 3.5 \times 10^{-11}$.  

{% include youtube.html id='7MGpoV2x2Hc' %} 

Thus prediction power does increase with better initial measurements, but not always: the benefit is unpredictable.  Notice that for certain values of $r$ the number of iterations until divergence actually increases with a decrease in $\Delta x_0$: this means that paradoxically increased accuracy can lead to decreased prediction accuracy!  

Ranging $r=3.5 \to r=4$, small changes in $r$ lead to little change to iterations of \eqref{eq1} with $x_0 = 0.3$ if the trajectory is periodic.  But when aperiodic, small changes in $r$ lead to large changes in the population trajectory.

{% include youtube.html id='WlbN2ZD34HU' %}

This sensitivity to constant values as well as initial population is important because in the real world, no measurement is perfect but is merely an estimation: the gravitational constant is not 9.8 meters per second squared but simply close to this value.  Necessarily imperfect measurements mean that not only would one have to take infinitely long to predict something at arbitrarily (infinitely) far in the future, but beyond a certain point the predictions will be inaccurate.  

This was first shown in Lorenz's [pioneering work](https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml?tab_body=pdf) mentioned above, in which a deterministic model of air convection was observed to exhibit unpredictable behavior.  A simplified proof for the idea that aperiodicity implies sensitivity to initial values, based on Lorenz's work, is found [here](https://blbadger.github.io/chaotic-sensitivity.html). 

### Nonlinear maps are often fractals

One of the most striking features of this map is that it is a self-similar fractal.  This means that smaller parts resemble the whole object.  Observe what happens when we zoom in on the upper left section of the aperiodic region in the logistic map: A smaller copy of the original is found.

![self-similar logistic map]({{https://blbadger.github.io}}/logistic_map/logistic_period_zoom3.png)

If we take this image and again zoom in on the upper left hand corner, again we see the original!

![self-similar logistic map]({{https://blbadger.github.io}}/logistic_map/logistic_period_zoom4.png)

Zooming in by a factor of $2^{15}$ on the point $(3.56995, 0.8925)$, we have

{% include youtube.html id='exUA33y8ZVI' %}

An infinite number of smaller images of the original are found with increasing scale.  Far from being unusual, the formation of fractals from nonlinear systems is the norm provided that they are dissipative and do not explode towards infinity nor exhibit a point or line attractor.  

### Period three

As $r$ increases, the periodicity increases until it becomes infinite, and infinite periodicity is equivalent to aperiodicity.  This occurs via period doubling: as can be seen clearly from the logistic map, one period splits into two, which split into four, which split into eight etc.  This period doubling occurs with less and less increase in $r$ such that an infinite period is reached within a finite increase in $r$, and at this point the map is aperiodic. This occurs whenever there is a transition from periodicity to aperiodicity.  

Looking closely at the orbit map, it seems that there are regions where an aperiodic trajectory transitions into a periodic one, for example at around $x = 3.83$ where there are three values where that $f(x)$ is located.  Is this really a transition away from aperiodicity?

For $r=3.83$, most initial values in $(0.3, 0.8)$ when iterated with the logistic map are numerically attracted to the following period 3 orbit:

$$
0.50466649, \; 0.9574166, \; 0.15614932, \; 0.50466649 ...
$$

Checking that these points are indeed prime period 3 is a task of finding the roots of $f^3(x) = x$ when $r = 3.83$.  $f^3(x)$ is the rather complicated expression

$$
f^3(x) = -r^7x^8 + 4r^7x^7 \\
- (6r^7+2r^6)x^6 + (4r^7+6r^6)x^5 \\
- (r^7+6r^6+r^5+r^4)x^4 \\
+ (2r^6 + 2r^5 +2r^4)x^3 \\
-(r^5 + r^4+r^3)x^2 + r^3x \\
$$

so finding the solutions to $f^3(x)-x = 0$ is best done using a computer algebra evaluator, which yields

$$
0 \approx -12089.028955x^8+48356.11582x^7 \\
-78846.982583x^6+67294.542381x^5 \\
-32066.758626x^4+8391.415073x^3 \\
-1095.484996x^2+55.181887x 
$$

This is a greater-than fourth order equation, meaning that there is no formula akin to the quadratic or cubic formulas that allow us to evaluate roots.  Instead roots can be found using an analytic technique, such as [Newton's or Halley's method](https://blbadger.github.io/polynomial-roots.html).  Using Halley's method (code [here](https://github.com/blbadger/polynomial_roots)) and selecting for real roots between 0 and 1, there is

```python
[(0.504667), (0.16357), (0.156149), (0.524), (0.738903), (0), (0.957418), (0.955293)]
```

Each of the points found numerically are found, along with five others.  The stability of each point may be checked by evaluating $abs (f^3(x))' < 1 $, which can be done using the program above, which results in the ungainly equation

```python
f^3(x). = -96712.23164x^7+338492.81074x^6-473081.895498x^5+336472.71190500003x^4-128267.034504x^3+25174.245219x^2-2190.969992x+56.181887
```

Evaluating this equation with class `Calculate` gives, respectively,

```python
[0.329918, 1.652343, 0.329823, 1.652285, -6.12846, 56.181887, 0.328975, 1.652814]
```

and this means that as expected, only the roots that were attractors in the numerical approach are stable, and the rest are unstable.  As an aside, note that the root at 0 has the largest value of $(f^3)'(x)$, and numerically is the most unstable of all points.

But there is something different about $r=3.83$ that yields (prime) period 3 points compared to those observed above for prime period 1 or 2.  Let's try to find points of period 4, which involves finding the roots of the equation $f^4(x) - x = 0$ which for $r=3.83$ is the unfortunate

```python
-559733898.714433x^16+4477871189.715462x^15-16257127648.301172x^14+35437147718.087624x^13-51706394045.119659x^12+53298583067.632263x^11-39921349801.953377x^10+22008774729.800488x^9-8946369923.674864x^8+2660008486.237636x^7-568040584.628146x^6+84473618.951678x^5-8330423.126762x^4+503584.653117x^3-16284.736485x^2+214.176627x
```
which Halley's method estimates real roots at

```python
[(0.60584), (0.738903), (0.803014), (0.299162), (0.369161), (0), (0.891935), (0.914596)]
```

Some of these points are not prime period 4, but rather prime period 1 or 2.  But some (eg. x = 0.2991621) are truly prime period 4, which is suprising indeed!  How can there be prime period points of period 4, given that we already found points of prime period 3 and 4 is greater than 3?

This observation is not specific to the logistic map: any (discrete) dynamical system of a continuous function with period 3 also contains (unstable) points of every other integer period.  This means that if period 3 is observed, one can find unstable trajectories of period 5, period 1, period 4952 and so on.  This result is due to Sharkovskii, who found the complete order on the integers:

$$
3, \; 5, \; 7, \; 9, \; 11, ... \\
6, \; 10, \; 14, \; 18, \; 22, ... \\
12, \; 20, \; 28, \; 36, \; 44, ... \\
\vdots \; 
\; \\
... 16, \; 8, \; 4, \; 2, \; 1
$$

where any prime period also gives points of prime period for any number to the right or bottom of the first.

As observed by Li and Yorke, period three also implies an uncountably infinite number of (unstable) aperiodic trajectories.  

### Exits from aperiodicity

The conclusion from the last section is that for values of $r$ of the logistic map such that there is a period-3 orbit, there are simultaneously orbits of any finite period as well as infinite-period (which are by definition aperiodic) orbits for this same $r$ value.  These are unstable, whereas the period-3 orbit is stable which is why we see points attracted to this particular orbit.

Now consider the transition from an area in the orbit map where no orbit is stable (and thus the trajectory is aperiodic) to an an area of a stable period-3 orbit (but still with points of aperiodicity, albeit unstable ones). Here is a closer view at the transition to period 3 near $r=3.8285$ (right click to view image in higher resolution):

![logistic aperiodicity to periodicity]({{https://blbadger.github.io}}/logistic_map/logistic_closeup.png)

Every third iteration of the logistic map starting from the relatively stable point of $x_0=1/2$, ie 

$$
x_3, x_6, x_9, x_{12}, x_{15}, x_{18}, x_{21}
$$ 

is also added and color coded as red, orange, yellow, green, blue, indigo, and violet respectively.  There are three points of period 3 near $r=3.83$, and so a third of all iterations from $x_0 = 1/2$ go to each point.

Here we see that every third iteration from $x_0 = 1/2$ is less unstable than the surrounding points, and so each has a higher point density.  The iterations approach one another as $r$ increases until they meet at around $r=3.82$.  A quick look at the logistic map suggests that the meeting of less-unstable points, particularly a meeting where these points approach at a decreasing rate (ie $\frac{d}{dr}x_3 - x_0 \to 0$) occurs for practically all transitions from aperiodicity to periodicity, whatever the period (3, 5, 7, 6, 12, etc.)  

Does a meeting of iterations of $x_0 = 1/2$ necessarily lead to a transition from aperiodicity to periodicity?  Yes, and here is why: 

First note that if two points $f(x_0), f^k(x_0), k > 2$ approach one another, then another point $f^{2k}(x_0)$ will also approach because $f(x_0)$ is near-periodic with period $k$.  This means that, for the region mapped above, there are not only 7 but really a countably infinite number of less-unstable points that approach one another, and all are iterations from $x_0 = 1/2$ (and therefore orbits from this starting value).  There are a countably infinite number of periodic orbits possible (see [here](https://blbadger.github.io/aperiodic-irrationals.html) for more on this), and therefore there are as many possible periodic orbits as there are less-unstable orbits that converge.  Then the value the less-unstable orbits converge upon becomes stable with respect to those periodic orbits because they are all accounted for.  

Now note that all points plotted using any computer are, strictly speaking, periodic: there are only so many possible values because computers use finite decimal precision.  Therefore eventually all values repeat: thus each point plotted is really a member of a periodic orbit.  As there are countably many periodic orbits possible and as countably many periodic orbits converge whenever two or more meet, the convergence value becomes stable for points that are plotted by a computer.

### Orbit map revisited

It is interesting to consider what the logistic orbit map represents: periodic points that are stable, not periodic points that are unstable.  With very accurate arithmetic, the logistic map looks quite different.

This can be plotted using the `Decimal` class in python, which implements arbitrary (within memory bounds, that is) decimal precision arithmetic.  For a constant $300000$ steps starting at $r=2.95$ and ending at $r=4$, increasing from $8$ to $28208$ digits of decimal precision yields

{% include youtube.html id='CcTqmILTRX8' %}

And with perfect precision, the orbit map would display only the points that are periodic at the start: an orbit map beginning at $r=0$ with a periodic point at $x_n=0$ would remain at $x_n=0$ throughout (until $r>4$).  An orbit map starting at $r=1$ would retain a period 1 attractor for the orbit map, which can be modeled by increasing decimal accuracy from 8 to 5587 digits, $r \in [1, 4]$, with 100000 steps as follows:

{% include youtube.html id='iqFLjFJwpZ4' %}

And an orbit map starting at period 2 would remain in period 2, and the same for 4, 8, etc.

But what would happen if we began the orbit map in the aperiodic region $r > 3.56995...$, assuming we had the ability to perfom infinite precision arithmetic?  If one were able to calculate with perfect precision, what would the orbit map look like?  Clearly a substantial change would occur in the periodic windows in the aperiodic region, because these periodic windows are stable orbits that coexist with many unstable ones.  With perfect precision, iterations in any of these unstable orbits would remain in that orbit rather than head to a stable point.  

For example, earlier in this page it was found that the (stable) period three region at $r=3.83$ also exhibited a period 4 trajectory that contains $x_n = 0.2991621...$.  This means that if one were to begin iterating at this point with perfect precision, the orbit map would exhibit a period 4 region for $r=3.83$ instead.  From Sharkovskii's total order above, this is also true for any other integer in this period three region: given any desired period, we can find a point such that $r=3.83$ exhibits a period of that value there.  

This is not all: from Sharkovskii's as well as Li and Yorke's work, it is shown that there are uncountably many unstable aperiodic orbits for period 3 (or 6 or 12 or 5...), it follows that nearly all trajectories in the aperiodic region are unstable but aperiodic.  With perfect precision, instability is irrelevant and therefore most trajectories through these regions would remain aperiodic.  Thus if one were to pick a value ($x_0 \in \Bbb R$ and in the domain of the logistic map) at random for the period three region $r=3.83$ (or any aperiodic region $r> 3.565995...$), it is almost certain to exist on an aperiodic but unstable trajectory such that the periodic windows we see with the orbit map would no longer be visible.  

### Exact solution for r=4

At r=4, the logistic map exhibits a fascinating conjuction of properties: aperiodicity with solvability.  By this is meant that we can express the location of any future point $x_{n+a}$ in a closed form, non-recursive expression: we no longer need to iterate \eqref{eq1} in order to find out where a future iteration will be.  And yet this map is also aperiodic, and furthermore it is sensitive to initial conditions such that any arbitrarily close $x_0, x_{0'}$ will eventually diverge given enough iterations.  

The relationship between solvability and aperiodicity, which are intuitively opposing ideas, is [explored elsewhere](/uncomputable-aperiodics.html).





## Mandelbrot set with variations

The Mandelbrot set $\mathscr M$ is the set of points $a$ in the complex plane for which the Julia sets are connected.  This happens to be the same set of points that iterations of the equation

$$
z_{n+1} = z_n^2 + a
\tag{1}
$$

do not diverge (go to positive or negative infinity) but instead are bounded upon many iterations at a starting value of $z_0 = 0$.  Thus the Mandelbrot set is very similar to the Julia set but instead of fixing $a$ and varying $z_0$ at different points in the plot, instead the starting value of $z_0$ is fixed at 0 and the value of $a$ at different points in the complex plane are plotted.

Let's compute $\mathscr M$. As for the [Julia sets](/julia-sets.html), the simplest way to do this is to initialize a complex plane as an `ogrid` array in numpy, with the difference being that `a` is assigned to this array, not `z`, which is instead asigned to an identically sized array of 0s.

```python
#! python3
import numpy as np 
import matplotlib.pyplot as plt 
plt.style.use('dark_background')

def mandelbrot_set(h_range, w_range, max_iterations):
	y, x = np.ogrid[-1.4: 1.4: h_range*1j, -1.8: 1: w_range*1j]
	a_array = x + y*1j
	z_array = np.zeros(a_array.shape)
	iterations_till_divergence = max_iterations + np.zeros(a_array.shape)
```

Now we can iterate at each position of `z_array` until the value diverges, using the value of `a` in `a_array`.  Finally, call the function and plot the results!

```python
	for h in range(h_range):
		for w in range(w_range):
			z = z_array[h][w]
			a = a_array[h][w]
			for i in range(max_iterations):
				z = z**2 + a
				if z * np.conj(z) > 4:
					iterations_till_divergence[h][w] = i
					break

	return iterations_till_divergence

plt.imshow(mandelbrot_set(800, 800, 30), cmap='twilight_shifted')
plt.axis('off')
plt.show()
plt.close()
```

![mandelbrot image]({{https://blbadger.github.io}}fractals/mandelbrot_custom_800x800x30.png)
  
This method is perfectly good, but slow.  Luckily we can use the numpy `ogrid` to compute divergence much faster! (see the [Julia sets page](/julia-sets.html) for more information)

```python
import numpy as np 
import matplotlib.pyplot as plt 
plt.style.use('dark_background')


def mandelbrot_set(h_range, w_range, max_iterations):
	# top left to bottom right
	y, x = np.ogrid[1.4: -1.4: h_range*1j, -1.8: 1: w_range*1j]
	a_array = x + y*1j
	z_array = np.zeros(a_array.shape)
	iterations_till_divergence = max_iterations + np.zeros(a_array.shape)

	for i in range(max_iterations):
		# mandelbrot equation
		z_array = z_array**2 + a_array

		# make a boolean array for diverging indicies of z_array
		z_size_array = z_array * np.conj(z_array)
		divergent_array = z_size_array > 4

		iterations_till_divergence[divergent_array] = i

		# prevent overflow (numbers -> infinity) for diverging locations
		z_array[divergent_array] = 0 
    
	return iterations_till_divergence

plt.imshow(mandelbrot_set(2000, 2000, 70), cmap='twilight_shifted')
plt.axis('off')
plt.savefig('mandelbrot.png', dpi=300)
plt.close()
```

This code is perfectly valid for mapping $\mathscr M$, itself (the dark region), but the colors look strange: there is a banding pattern that is not seen in the plot from the other program for $\mathscr M$.  

![ mandelbrot]({{https://blbadger.github.io}}fractals/mandelbrot_diverging2.png)

A little retrospection can convince us that there is a problem with how we compute the first iteration of divergence, mainly that sometimes this program does not actually store the first iteration but instead a later one! Remember that $a$ is being added to $z$ for every iteration even if $z = 0$, as we have set it to stop the values from getting out of hand.  This could cause a later iteration to become larger than 2 (see [here](/julia-sets.md), and the code as it stands would record this later value as the iteration of divergence. This can be remedied by introducing another boolean array `not_already_diverged` that keeps track of which points in the plane have previously headed off towards infinity as follows:

```python
def mandelbrot_set(h_range, w_range, max_iterations, t):
	y, x = np.ogrid[1.6: -1.6: h_range*1j, -2.2: 1: w_range*1j]
	a_array = x + y*1j		
	z_array = np.zeros(a_array.shape) 
	iterations_till_divergence = max_iterations + np.zeros(a_array.shape)

	# make an array with all elements set to 'True'
	not_already_diverged = a_array < 1000
	
	for i in range(max_iterations):
		# mandelbrot equation
		z_array = z_array**2 + a_array 

		# make a boolean array for diverging indicies of z_array
		z_size_array = z_array * np.conj(z_array)
		divergent_array = z_size_array > 4
		diverging_now = divergent_array & not_already_diverged

		iterations_till_divergence[diverging_now] = i
		# prevent overflow (numbers -> infinity) for diverging locations
		z_array[divergent_array] = 0

		# prevent the a point from diverging again in future iterations
		not_already_diverged = np.invert(diverging_now) & not_already_diverged

	return iterations_till_divergence
```

The colors are accurate now! The above code (except with slightly larger x and y ranges used to initialize the ogrid) may be called with the kwarg `extent` in order to provide accurate axes makers as follows:

```python
plt.imshow(mandelbrot_set(2000, 2000, 70), cmap='twilight_shifted', extent=[-2.2, 1, -1.6, 1.6])
plt.axis('on')
plt.show()
```
which yields 

![mandelbrot_set]({{https://blbadger.github.io}}fractals/mandelbrot_corrected.png)

To reorient ourselves, the dark area in the center is composed of all the points that do not diverge (head towards positive or negative infinity) after the specified maximum number of iterations.  The light areas bordering this are the points that diverge but not immediately, and the purple region that surrounds the shape is the region that quickly heads towards infinity.

The Mandelbrot set is a very rich fractal. Here is a zoom on the point - 0.74797 - 0.072500001i (see [here](/julia-sets.md) for a description of how to make the video)

![disappearing mandelbrot]({{https://blbadger.github.io}}fractals/mandelbrot_zoom1.gif)

And here is the same point, increasing scale to a factor of $2^{42}$ (over four trillion)

{% include youtube.html id='0qrordbf7WE' %}

What happens if we change the exponent of (2) such that $z^1 \to z^4$ ?  At $z^1$, the equation is linear and a circular region about the origin remains bounded.  But as the system becomes nonlinear, intricate shapes appear.  Here we go from $z^1 \to z^4 \to z^1$, and note that the positive real axis is pointed up instead of to the right.

![extended mandelbrot]({{https://blbadger.github.io}}fractals/mandelbrot_slow.gif)

### Translations and rotations

What happens if there is a small amount $b$ added upon each iteration?  Then we have $z_{n+1} = z_n^2 + a + b$, which is equivalent to changing $a$ by a constant factor for all values in the complex plane.  This results in the map being translated in the complex plane, but not otherwise changed.  

The effect is quite different if the starting value $z_0 \neq 0 + 0i$. We are now departing from a true Mandelbrot set, which requires the initial value to be $0$, but a small change like setting $z_0 = a$ will result in a set that mostly resembles the Mandelbrot set $\mathscr M$.  But if some value $b$ is added upon each iteration, the set of non-diverging points changes unpredictably, reflecting the irregularity of $\mathscr M$ itself. 

To summarize, the following equation shall be investigated:

$$
z_{n+1} = z_n^2 + a + b \\
z_0 = a \\
\tag{2}
$$

Let's look at the bounded iterations of (2) with many real values of $b$, going from $b=0 \to b=1.3 \to b=0$ as follows:

```python
def mandelbrot_set(h_range, w_range, max_iterations, t):
	...
	# make an array with all elements set to 'True'
	not_already_diverged = a_array < 1000
	z_array = a_array # set initial z_array values to a_array points
	
	for i in range(max_iterations):
		# mandelbrot equation
		z_array = z_array**2 + a_array + (1.3/300)*t 
		...
```

![disappearing mandelbrot]({{https://blbadger.github.io}}fractals/mandelbrot_disappeared.gif)

In the other direction, $b=0 \to b = -2.5$ yields

![disappearing mandelbrot]({{https://blbadger.github.io}}fractals/mandelbrot_disappeared_reversed.gif)


How about if we move to a complex number? The set from $b = 0 \to b = 1 + i$ looks like

![disappearing complex mandelbrot]({{https://blbadger.github.io}}fractals/mandelbrot_complex_disappeared.gif)

Instead of moving from the origin to a given point $b$, let's try rotating about the origin at a radius $r$.  Luckily we are already working with complex numbers so this can be done using Euler's formula

$$
e^{i y} = \cos(y) + i \sin(y)
$$

so if we want one complete rotation ($2\pi$ radians) after 300 images (the usual length of the videos on this page) of a point centered at a radius of $1/3$,
```python
	...

	for i in range(max_iterations):
		# mandelbrot equation
		z_array = z_array**2 + a_array + np.exp(3.1415j * (t/150))/3
```

which yields

![disappearing complex mandelbrot]({{https://blbadger.github.io}}fractals/mandelbrot_swirl_0.3r.gif)

Euler's formula can be found using infinite series, which are sums of infinitely long sequences.  Power series centered at $0$ (also called Maclaurin series) can be expressed as

$$
f(x) = c_0 + c_1(x) + c_2(x)^2 + c_3(x)^3 + \cdots
$$

and have coefficients equal to

$$
c_n = \frac{f^{(n)}(0)}{n!}
$$

where $f^{(n)}$ corresponds to the nth derivative of $f$, which can be found by noting that $f'(0) = 1c_1$ and $f'' (0) = 2 \cdot 1c_2$ and $f''' (0) = 3 \cdot 2 \cdot 1c_3$ etc.

Therefore if a power series exists for any function $f(x)$, it has the form

$$
f(x) = \sum_{n = 0}^\infty \frac{f^{(n)}(x)}{n!} x^n 
$$

Checking Taylor's inequality, it can be verified that $e^x$ can be represented by a power series, which is

$$
e^x = \sum_{n=0}^\infty \frac{z^n}{n!} = 1 + z + \frac{z^2}{2!} + \frac{z^3}{3!} + \cdots 
$$

and the same is true for sine and cosine, which are

$$
\sin(x) = \sum_{n=0}^\infty (-1)^n\frac{z^{2n+1}}{(2n+1)!} = z - \frac{z^3}{3!} + \frac{z^5}{5!} - \cdots \\
\cos(x) = \sum_{n=0}^\infty (-1)^{n}\frac{z^{2n}}{(2n)!} = 1 - \frac{z^2}{2!} + \frac{z^4}{4!} - \cdots
$$

Expressing $e^{iz}$ as an infinite sum proceeds by substituting $x = iz$ and remembering that successive powers of $i$ yield the 4-cycle $(i, \; -1, \; -i,\; 1, \dots)$  which altogether is

$$
e^{iz} = 1 + iz + i^2\frac{z^2}{2!} + i^3\frac{z^3}{3!} + i^4\frac{z^4}{4!} + \cdots \\
e^{iz} = 1 + iz - \frac{z^2}{2!} - i\frac{z^3}{3!} + \frac{z^4}{4!} + \cdots \\
$$

and now splitting the series by taking every other term,

$$
e^{iz} = \left( 1 - \frac{z^2}{2!} + \frac{z^4}{4!} - \cdots \right) + \\
i \left( z - \frac{z^3}{3!} + \frac{z^5}{5!} - \cdots \right) \\
e^{iz} = \cos(z) + i \sin(z)
$$

Evaluating Euler's formula with $x= i\pi$ gives the beautiful identity

$$
e^{i\pi} + 1 = 0
$$

which relates two of the best known transcendental numbers with the two arithmetic identities.



### Continuity and aperiodicity

### There are $2^{\Bbb Q} = \Bbb R$ continuous functions from $\Bbb Q \to \Bbb Q$ (1)

This theorem has been established and may be found in texts on real analysis, and one proof will suffice here.

First we define a 'unique' function to define a single trajectory in finite dimension $D$.  In other words, there is a one-to-one and onto mapping of a function to a trajectory.  The coordinates for this trajectory are defined by members of the set of rational numbers $\Bbb Q$ for any continuous function.  Now note that at any one point along a continuous trajectory, the next point is one of three options: it is slightly larger, slightly smaller, or the same. Graphically in two dimensions: 

![continuous function next value]({{https://blbadger.github.io}}misc_images/continuous_function_next.png)

and precisely, $f$ may increase by $\delta$, decrease by $\delta$, or stay the same. Here $\delta$ is defined as 

$$ 
\lvert x_1 - x_2 \rvert < \epsilon \implies \lvert f(x_1) - f(x_2) \rvert < \delta 
$$ 

for any arbitrarily small value $\epsilon$. Note that this is identical to the standard definition for continuity, which is $\lvert f(x_1) - f(x_2) \rvert < \epsilon$ for any $\epsilon > 0$ implies that there exists some $\delta$ such that $\lvert x_1 - x_2 \rvert < \delta$, but with $\epsilon$ and $\delta$ reversed.

Thus for $\Bbb Q$ there are three options, meaning that the set of continuous functions is equivalent to the set of all sets of $\Bbb Q$ into $\{0, 1, 2\}$

$$
\{f\} \sim 3^{\Bbb Q} \sim \Bbb R
$$

or the set of all continuous functions defined on $\Bbb Q$ is equivalent to the set of real numbers. 

### There are $2^{\Bbb R}$ discontinuous functions from $\Bbb Q \to \Bbb Q$ (2)

Discontinuous trajectories in $Q$ may do more than increase or decrease by $\delta$ or else stay the same: the next point may be any element of $\Bbb Q$.   The size of the set of all discontinuous functions is therefore the size of the set of all subsets of continuous functions. As we have established that the set of all continuous functions from $\Bbb Q \to \Bbb Q$ is equivalent to $\Bbb R$, 

$$
{2^{\Bbb Q}}^{\Bbb Q} = 2^{\Bbb R} = \Bbb R ^{\Bbb R}
$$

Another, perhaps more direct, proof of this statement is found [here](https://blbadger.github.io/aperiodic-irrationals.html).

### Discontinuous maps cannot be defined on the rationals (3)

Functions are simply subsets of the Cartesian product of one set into another, meaning that a function mapping $\Bbb Q \to \Bbb Q $ is a subset of 

$$
\Bbb Q^{\Bbb Q} = 2^{\Bbb Q}
$$

Thus there can be at most $\Bbb R$ functions mapping $\Bbb Q \to \Bbb Q$, but we have already seen that the size of the set of discontinuous functions is $2^{\Bbb R}$.  This means that discontinuous functions cannot be defined on the set of rational numbers $Q$ for any finite dimension $D$.

### Aperiodic maps are discontinuous (4)

Sensitivity to inital values can be defined such that an arbitrarily small difference $\varepsilon$ bewteen $x_0$ and $x_1$ leads to a difference $\Delta$ where

$$
\exists n : \;\Delta =  \lvert f^n(x_0) - f^n(x_1) \rvert 
$$

such that $\Delta$ is not arbitrarily small. Sensitivity to initial values [implies](https://blbadger.github.io/aperiodic-irrationals.html) aperiodicity in bounded trajectories, as first recognized by Lorenz. Therefore aperiodic maps are necessarily sensitive to initial values.

Function $f$ is defined to be a continuous function from $\Bbb R^3 \to \Bbb R^3$ such that future values do not match previous ones.  An example of $f$ could be an ordinary differential equation system for the Lorenz attractor.  Now define a function $g$ that is equivalent to the composition of function $f \circ f \circ f ...$.  We can call $g$ the map of $f$, in that it yields the final position of an initial value $x_0$ in $\Bbb R$ after $n$ compositions. One $f$ yields one $g$ after an infinite number of compositions 

$$
f \circ f \circ f \circ f...
$$

$g$ is necessarily sensitive to initial values, due to the definition of $f$. 

Corollary: $g$ for an arbitrary aperiodic map is discontinuous

Proof: Suppose, for the sake of contradiction, that $g$ is continuous.  By definition, for all points $x_a, x_b$ and for any finite $\epsilon > 0$, there exists a $\delta > 0$ such that

$$
\lvert g(x_a) - g(x_b) \rvert < \epsilon \\
\mathtt{whenever} \;  \lvert x_a - x_b \rvert < \delta 
$$

But if this was true of $g(x_a)$ and $g(x_b)$ for all $g$ then $\lvert g(x_a) - g(x_b) \rvert < \epsilon \; \forall \epsilon$ given a finite $\delta$ such that $\lvert x_a - x_b \rvert < \delta$.  But then if $x_a - x_b < \delta$ then $g(x_a)$ would stay arbitrarily close to $g(x_b)$ and $g(x)$ would not be sensitive to arbitrarily small changes in initial points $x_a, x_b...$.  This is a contradiction by the definition of $g$ above, and therefore $g(x)$ is discontinuous. $\square$

Finally, note that we can define the map of an aperiodic, bounded function $f$ based on $g$ for an infinite number of compositions $f$.  Taking $g$ as our aperiodic map, the result is that aperiodic, bounded phase space maps are discontinuous.

### Aperiodic, bounded maps cannot be defined on $\Bbb Q$ (5)

This results from (3) and (4). 

And now we conclude the (informal but hopefully accurate) theorem-proof portion of this page.

### How a composition of continuous functions can be discontinuous

Given a continuous function, say $f(x) = x^2$, the composition of this function with itself is 

$$
f \circ f =f(f(x)) = (x^2)^2 = x^4
$$

and $x^4$ is also continuous. Indeed any finite number of compositions of $f$ will be continuous.  But consider that there are an infinite number of compositions required for a map of an ordinary differential equation system over any finite length of time.  This means that no finite number of compositions of $f$, or an ability to show that this composition is continuous can convince us that $g$ is continuous.

For a discontinous functions, an infinitely small change in the input does not correspond to an infinitely small change in the output but rather a certain finite amount.  For our composition map $g$, this finite change in the output is the result of multiplication of infinitely small changes in outputs of $f$ (as $f$ is continuous) by an infinite number (of compositions).  This is reminiscent of integral calculus, where the multiplication of an infinite number of infinitely small quantities yields a finite positive output. 

This is conceptually the mirror image of the idea that nonlinear ordinary differential equations can lead to evaluation to infinity in a finite amount of time. An example of this is for the differential equation

$$
f(x) = 1 + x^2 \\
x_0 = 0
$$

then

$$
dx/dt = 1 + x^2 \\
\frac{dx}{1+x^2} = dt
$$

and integrating for time,

$$
\int \frac{dx}{1+x^2} = \int dt \\
tan^{-1}(x) = t + C \\
$$

For the definition of $g$ as the composition of $f$ at finite time $t$, $g(t) = tan^{-1}(x) + C$ meaning that $g$ reaches $\pm \infty$ when $t \ge \pi/2$.  Note that $f(x_0) = 1$, meaning that an infinite value has been reached in finite time for compositions of this $f$. 

### Smale's horseshoe map

Stephen Smale introduced the horseshoe map to describe a common aperiodic attractor topology.  This model maps a square into itself such that the surface is stretched, folded and then re-stretched over and over like a baker's dough.  The horseshoe map is everywhere discontinuous as points arbitrarily close together to start are separated (the surface is stretched at each iteration and there are infinite iterations) such that there is a distance greater than some $e > 0$ between them.  The process of folding prevents divergence to infinity.  

As an aside, it is also interesting to note that stretching and folding is the most efficient way to mix physical substances that have internal cohesion.  For example, take two substances that are malleable solids.  Being solids, they have internal cohesion strong enough to prevent mixing due to gravity if someone puts the solids in the same container.  This self-attraction makes it difficult to mix the solids by jostling or otherwise adding energy to the container.  Rather, iteratively stretching out each solid (maintaining some internal attraction while increasing surface area) and then folding them together uses less energy (cohesive forces do not need to be overcome).





## Training Memory

## The Importance of Randomization

It is standard practice to perform pseudo-random transformations on the inputs and parameters of neural networks before and during training. Shuffling or random selection of training data, initialization of weights and biases on a Gaussian (or Poisson etc.) distribution, randomized neuron drouput, and most objective function optimization procedures like stochastic gradient descent or Adaptive moment estimation readily spring to mind when one considers current methods.

It is worth asking the following question: why do all these procedures involve randomization, and why not simply arrange the input examples in any arbitrary order and maintain that during training instead?  Each case is considered in the following sections.

Some experimentation will convince one that randomization leads to better performance: for example, failing to randomize the initial weights $w_0$ and biases $b_0$ often leads to poor training and test results.  But if randomization 'works', why is this the case?

### Stochastic Gradient Descent

To begin, let's consider gradient descent.

Stochastic gradient descent can be thought of as the foundation upon which most optimization procedures are built upon, and the online (one training example at a time, that is) algorithm is as follows:

Given a vector of a network's weights and biases $v_0$, an objective function $F$, a learning rate $\eta$, and shuffled dataset $\mathscr S$,

$$
v_{i+1} = v_i - \eta \nabla F_j(v_i) \\
\forall j \in \mathscr S
\tag{1}
$$

When $i$ is equal to the size of $\mathscr S$, one training epoch is completed and the entire process is repeated until $F$ is approximately minimized (or some other constraint is met).  

The randomization step is that the dataset is shuffled.  Why take that set in the first place?  The objective function $F$ that we are trying to minimize is usually thought of as a multidimensional 'landscape', with gradient descent being similar to finding a valley while looking at the ground under one's feet while walking.
The local gradient, if sampled repeatedly, provides the necessary information to find the approximate minimum point if the objective function is smooth enough.  

Gradient descent may take many iterations towards optimization of the loss function.  To speed up this process, gradient descent may be modified to include parameters such as momentum, or extended into optimizers like RMSprop or AdaGrad.

It has been asserted in the past that a problem with gradient-based optimization algorithms is that they are prone to becoming 'trapped' in local minima or saddle points that are non-optimal.  Approaching a minimal point in any direction, the gradient vanishes, $ \nabla F_j(v_i) \to 0$ such that any sufficiently small step $v_{i+1} - v_i$ is attracted to the minimal point.  If minimal points were actually a problem for gradient-based optimization of neural networks or similar models, we should see their gradients disappear at some point during training.  Saddle points exhibit a zero gradient but are not attractive to all basis vectors.  Do we find minimal or saddle points when training deep learning implementations?

This can easily be tested, and for this author the results are as follows: with a wide array of objective functions and neural net architectures, disappearing gradients have not been observed to occur and thus it seems that even for very non-convex objective functions there is virtually no likelihood of getting 'stuck' at a local minima, or even landing on a saddle point.  The important thing to note here is that a function can easily be non-convex without containing any local minima, or even any saddle points, as all that is required is for some $ \nabla F_j(v_i)$ vector to intersect the cost functions surface.

This leads to an intuitive explanation as to why objective functions in high-dimensional space do not contain many if any local minima: to be a minimal (or saddle) point, $\nabla F_j(v_i)$ must decrease in **every** direction, which is equivalent to saying that it must increase in every basis vector space (aka dimension).  Neural network models are commonly $>100,000$ -dimensional, and each dimension near any point may increase or decrease $\nabla F_j(v_i)$.  Thus the probability of all dimensions decreasing the gradient without prior knowledge is one in $2^{100,000}$, an extremely huge number (that funnily enough evaluates to infinity on google's calculater).  

Therefore it is almost impossible for a high-dimensional objective functions to have true local minima (or true saddle points), and therefore regularizers and randomization are generally not needed to combat this possibility.

Now back to the original question: why does stochastic gradient descent work best if it is stochastic?  Why not enumerate all training examples and iterate through them in order, updating the network via gradient descent using $1$?  If each training example is fed to the network during one epoch, from an informational perspective would it matter whether or not one example comes before another?

### Training memory

This section will require some familiarity with [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network), which will be given briefly here.  A defining characteristic of recurrent neural networks is that they are updated sequentially as time passes.  The simplest form, a fully recurrent network, has one hidden layer that is updated at each sequential element, and each element is viewed at one time step.  For example, a text classification network performing sentiment analysis (finding if a statement expresses positive or negative emotions) of the following sentance:

"The dog went to the store."

would simply iterate through the sentance ('The' then 'dog' then 'went' etc.), and at each word the activation in each hidden neuron would be updated.  At the end of the sentance, the hope is that some information from early elements is retained in the hidden layer's activations, as these are then used to generate some output.  Stochastic gradient descent (or some other optimization procedure) and backpropegation are then performed on the network, updating the network's weights and biases in accordance with the output that was a result of the activations 

Unfortunately, in practice recurrent neural networks suffer from the same problems as very deep networks of all kinds: unstable gradients.  

Now consider the process of training a non-recurrent neural network, perhaps a convolutional net performing image classification.  The first training example (or batch of examples) is fed to the network with initial weights and biases (represented as $v_0$), resulting in some output $o_1$.  The output is then input into the objective function, and the gradient wrt output is calculated before backpropegation is used to update the network's weights and biases to $v_1$ via $(1)$.  The process is then repeated with the second (batch) example, where the network's weights and biases configuration $v_1$ generates an output $o_1$ which then leads to updating $v_1 \to v_2$ and so on.

Therefore the sequence of weight and bias configurations is

$$
(v_0, o_0), (v_1, o_1), (v_2, o_2), \; \cdots \; , (v_n, o_n)
$$

Now note that the final configuration $v_n$ depends not only on the prior configuration $v_{n-1}$ but also on all previous configurations and outputs as follows:

$$
v_0 + i_0 \to o_0 \to v_1 \\
v_1 + i_1 \to o_1 \to v_2 \\
\vdots \\
v_{n-1} + i_n \to o_n \to v_n
$$

Therefore successive configuration and outputs form a directed graph 

$$
v_0 \to o_0 \to v_1 \to o_1 \to \cdots \to v_n 
$$

But if we replace 'node' with 'configuration', the definition of a recurrent neural network is arrived at. This means that the final configuration $v_n$ can be thought of as retaining a 'memory' of $v_0, v_1...$ in that these weight and bias configurations were updated by future outputs, but not erased by them.  Instead of updating neuronal activations for each element in an input sample (as occurs in traditional rNNs), the configuration weights and biases are updated for each sample in an epoch (and activations do not persist between samples).  But a network's configuration determines its activations, such that the same result of memory over time exists.

To summarize, training any neural network is not instantaneous but instead occurs over a certain time interval, and therefore may be thought of as a dynamical system.  In this system, the final network configuration $v_n$ depends not only on the network inputs but also which input is fed to the network in what order.  

Considered carefully, therefore, any network network undergoing training via updating weights and biases over time is a type of recurrent neural network, with training states $v_0, v_1, ... , v_n$ forming a directed graph along a temporal sequence across training examples (rather than across a elements of one input, as is the case for rnns). 

It may be beneficial, therefore, to develop a theory of extending memory gates similar to those used in LSTMs and GRUs to the training of any network, such that a network's configuration at any point $v_m$ is influenced not only by $v_{m-1}$ but also $v_{m-2}$.  

### Input randomization and training memory

It was asserted in the previous section that the sequence of neural network configurations, inputs, and outputs form a directed, acyclic graph during training.  Because the graph is directed, there is no guarantee that reversing the order of $i_0, i_1, i_2, ..., i_{n-1}$ (and thus the order of outputs too) would result in the same sequence of configurations $v_0, v_1, v_2$.  This is to say that the order of training examples matters, such that the ability to minimize an objective function in the final configuration $v_n$ will vary depending on the order of training examples provided because the graph path taken is changed.

Now imagine training for one epoch only, meaning that all training examples are fed to the network exactly once (singly or in batch form).  What happens if we first group the training examples by similarity, perhaps using latent space vectorization, and then feed the resulting sequence to the network?  For simplicity, suppose there are inputs two types (in equal number), those belonging to set $\mathscr A$ and other belonging to set $\mathscr B$ such that all $a \in \mathscr A$ are seen by the network before $b \in \mathscr B$.  What happens to $v_n$?

Assuming that the network's hyperparameters (non-trainiable variables) are not changed during training, $v_n$ will reflect $\mathscr B$ moreso than $\mathscr A$, for the simple reason that the configuration updates for $a \in \mathscr A$ are 'overwritten' by $b \in \mathscr B$.  More precisely, the minimum distance between $v_n$ and an earlier configuration $v_m$ resulting from an update from either set is much greater for $a$ than for $b$: there are $\lvert \mathscr B \rvert$ nodes (updates) between $v_{ma}$ and $v_{mb}$.  

This is to say that the network after training would be expected to classify $a \in \mathscr A$ worse than $b \in \mathscr B$.  Instead, alternating $a, b, a, b, ...$ results in a distance $v_m(a) - v_m(b) = 1$.  Without knowing a priori which elements of the training set belong to $\mathscr A$ or $\mathscr B$, simply shuffling the training set minimized the expected value of $v_{ma} - v_{mb}$ and would therefore present the best solution to the problem of recency in training memory.

An example: 

Suppose one were to attempt to infer the next letter in a word using a character level recurrent neural net, and that all inputs were slight variations on the phrase 

$$
abababab
$$

and the network was trained on the task of returning the final letter. Assuming the training was effective, the expected output given an input 

$$
abababa
$$

would be $b$.  Indeed, relatively simple tasks such as these are well-accomplished by recurrent neural nets with no fancy gating.  But now suppose that a non-recurrent (in the traditional sense) neural network were trained on a sequence of $a \in \mathscr A$ and $b \in \mathscr B$ as above.  The expected next output given sequential alternating inputs

$$
a, \; b, \; a, \; b, \; a ,..., \;a
$$

would be $b$. If the above argument (that any network while training over many inputs behaves as a recurrent neural net for one input) is accepted, the alternation in input type contributes to $v_n$ minimizing its objective function.

Thus although alternating input types minimized the value of $v_{ma} - v_{mb}$, this alternation would be expected to cause the network to 'learn' that output type is alternating, and if this were not general to a test set then accuracy would necessarily follow. 

### Why randomize inputs during or between epochs

Randomization of inputs between or during training epochs is also effective for minimizing objective function loss. Why is this the case: once training inputs have been randomized, what further purpose would shuffling the inputs again serve?  

In the above sections, it was claimed that a neural net configuration $v_i$ depends not only on the inputs, but on the sequence of those inputs.  In dynamical system terms, a periodic training input 'trains' the network to expect periodic inputs in a test set regardless of whether or not the input is periodic. 

Suppose we have three elements per training set, and we want to perform three epochs of training.  After randomizing the inputs, there is

$$
a, c, b
$$

such that the full training session is

$$
a, c, b, a, c, b, a, c, b
$$

But this again is a periodic sequence, with a periodicity being the size of the dataset.  Over many epochs, this periodicity becomes reflected in the final network configuration $v_n$ just as a shorter period would be.  

### A possible benefit from randomized inputs during training

Returning to the successive configurations $v_0, v_1, ...$ the network takes over time during training, the directed graph representing these configurations given inputs and outputs is as follows:

$$
v_0 + i_0 \to o_0 \to v_1 + i_1 \to o_1 \to \cdots \to v_n 
$$

Now suppose one epoch is completed, and $v_n$ has decreased the objective function's output but not to the global minimum.  If one wishes to begin another training epoch and further minimize the objective function's output, what sequence of inputs should be used?

One option would be to use the same sequence of inputs as for the first epoch, that is, $i_0, i_1, i_2, i_3, ..., i_{n-1}$.  Now the initial configuration for the second epoch is not expected to be the same as it was before the first epoch, or $v_{02} \neq v_{0}$ and therefore the sequence $o_{02}, o_{12}, o_{22}, ..., o_{n2}$ would not be the same either.  But it would be more similar than if $i_0, i_1, i_2, ...$ were reordered.

To see why this is, observe that we can assign a vector to the input sequence $I = i_0, i_1, i_2, ... i_{n-1}$.  There are many vector additions and multiplications and other operations involved in updating $v_0 \to v_n$, but we can combine all of these into one operation $\circ$ to give $v_0 \circ I = v_n$.  

Finally, is it very likely that the ideal path from $v_{00} \to v_{nn}$ such that $v_{nn}$ minimized the objective function $F$ was achieved using the initial ordering $i_0, i_1, i_2 ... i_{n-1}$?  No, given that there are $(n-1)!$ ways of ordering $n-1$ inputs, without prior knowledge then the chance of choosing the best initial path is $1/(n-1)!$ assuming each path is unique.  Reordering the input sequence between epochs increases the chances of choosing a better path for one epoch (as well as a worse path).  

If many paths are of similar 'quality', but some paths are much better, then reordering the input sequence can act to minimize the objective function in $v_n$ by increasing the chances of landing on a better path.

### Backpropegation and subspace exploration
 
Neural networks were studies long before they became feasible computational models, and one of the most significant breakthroughs that allowed for this transition was the development of backpropegation.

Backpropegation can be thought of as an optimal graph traversal method (or a dynamic programming solution) that updates a network's weights and biases with the fewest number of computations possible.  The goal of training is to find a certain combination of weights and biases (and any other trainable parameters) that yield a small objective function, and one way this could be achieved is by simply trying all possible weights and biases.  That method is grossly infeasible, and even miniscule networks with neurons in the single digits are unable to try all possible (assuming 32 bit precision) combinations.






## Structured Sequence Inputs

This page is part I of II, for part II see [here](https://blbadger.github.io/nn_interpretations.html).

Relying on the ability of deep learning to form representations of an input, we explore neural networks' ability to learn from structured abstract sequential inputs.  

### Introduction and background

Deep learning here is defined as a type of machine learning in which the input is represented in successive layers, with each layer being a distributed representation of the previous.  This allows for the approximation of very complicated functions by 

Deep learning approaches are often touted for having greater accuracy than other machine learning approaches, and for certain problems this is indeed true.  Tasks such as visual object recognition, speech transcription or language translation are among those problems at which deep learning approaches are currently the most effective at among all machine learning techniques, and for some problems the efficacy of deep learning is far beyond what has been achieved by other methods.

The kinds of tasks mentioned in the last paragraph are sometimes called 'AI-complete', which is an analogy to the 'NP-complete' problems that are as hard as all other NP-complete problems with regards to computational complexity reduction. These tasks are quite difficult for traditional machine learning, but are often  approachable using deep learning methods, and indeed most research on the topic since the seminal papers of 2006 have been focused on the ability of deep learning to out-perform other methods on previously-unseen data.

This page focuses on a different but related benefit of deep learning: the ability to learn an arbitrary representation of the input, allowing one to use one  model for a very wide array of possible problems.  The interpretability, or the ability to understand how the model arrives at any prediction, of this flexible approach is then explored.

### Abstract sequence modeling

Deep learning approaches such as neural networks are able to learn a wide array of functions (although there are certainly limits to what any machine learning approach is able to approximate, see [here](https://blbadger.github.io/nn-limitations.html) for more details

Work on this topic was in part inspired by the finding that the large language model GPT-3 was able to [learn arithmetic](https://openai.com/blog/grade-school-math/).  The research question: given that a large language model is able to learn simple functions on sequential character inputs, can a smaller model learn simple and complex functions using language-like inputs as well?  

The importance of this question is that if some model were to be able to learn how to approximate certain functions on an input comprising something as general as a sequence of characters, a machine learning practitioner would be able to avoid the often difficult process of formatting each feature for each dataset of interest. In the following sections, we shall see that the answer to this question is yes.

### Input Flexibility 

To start with, we will use a small dataset and tailor our approach to that specific dataset before then developing a generalized approach that is capable of modeling any abitrary grid of values.  Say we were given the following training data in the form of a design matrix:


|Market	|Order Made|	Store Number	|Cost |Total Deliverers	|Busy Deliverers	|Total Orders	|Estimated Transit Time	|Linear Estimation	|Elapsed Time|
| ----- | ---------| -------------- | --- | --------------- | --------------- | ----------- | --------------------- | ----------------- | ---------- |
|1	|2015-02-06 22:24:17	|1845	|3441	|33	|14	|21	|861	|3218	|3779|
|2	|2015-02-10 21:49:25	|5477	|1900	|1	|2	|2	|690	|2818	|4024|
|3	|2015-01-22 20:39:28	|5477	|1900	|1	|0	|0	|690	|3090	|1781|
|3	|2015-02-03 21:21:45	|5477	|6900	|1	|1	|2	|289	|2623	|3075|

We have a number of different inputs and the task is to predict **Elapsed Time** necessary for a delivery. The inputs are mostly integers except for the **Order Made** feature, which is a timestamp.  The names of the third-to-last and second-to-last columns indicate that we are likely developing a boosted model, or in other words a model that takes as its input the outputs of another model.

For some perspective, how would we go about predicting the **Elapsed Time** value if we were to apply more traditional machine learning methods? As most features are numerical and we want an integer output, the classical approach would be to hand-format each feature, apply normalizations as necessary, feed these into a model, and recieve an output that is then used to train the model.  This was how the **Linear Estimation** column was made: a multiple ordinary least squares regression algorithm was applied to a selection of formatted and normalized features with the desired output.  More precisely, we have fitted weight $w \in \Bbb R^n$ and bias (aka intercept) $b \in \Bbb R^n$ vectors such that the predicted value $\hat {y}$ is

$$
\hat {y} = w^Tx + b
\tag{1}
$$

where $x$ is a (6-dimensional) vector containing the formatted inputs of the relevant features we want to use for predicting **Elapsed Time**, denoted $y$.  The parameters of $w, b$ for linear models usually by minimizing the mean squared error

$$
MSE = \frac{1}{m} \sum_i (\hat {y_i} - y_i)^2
$$

Other more sophisticated methods may be employed in a similar manner in order to generate a scalar $\hat y$ from $x$ via tuned values of parameter vectors $w$ and $b$.  Now consider what is necessary for our linear model to perform well: first, that the true data generating distribution is approximately linear, but most importantly for this page that the input vector $x$ is properly formatted to avoid missing values, differences in numerical scale, differences in distribution etc.

When one seeks to format an input vector, there are usually many decision that one has to make.  For example: what should we do with missing values: assign a placeholder, or simply remove the example from our training data? What should we do if the scale of one feature is orders of magnitude different than another?  Should we enforce normality if the distribution for each feature, and if so what mean and standard deviation should we apply? 
How should we deal with categorical inputs, as only numerical vectors are accepted in the linear model?  For categorical inputs with many possible options, should we perform dimensionality reduction to attempt to capture most of the  information present in a lower-dimensional form? If so, what is the minimum number of options we should use and what method should we employ: a neural network-based embedding, sparse matrix encoding, or something else?

These questions and more are all implicitly or explicitly addressed when one formats data for a model such as this: the resulting vector $x$ is composed of 'hand-designed' features.  But how do we know that we have made the optimal decisions for formatting each input?  If we have many options of how to format many features, it is in practice impossible to try each combination of possible formats and therefore we do not actually know which recipe will be optimal.

The approach of deep learning is to avoid hand-designed features and instead allow the model to learn the optimal method of representing the input as part of the training process.  A somewhat extreme example of this would be to simply pass a transformation (encoding) $f$ of the raw character input as our feature vector $x$. For the first example in our training data above, this equates to some function of the string as follows:


$$
x = f(1 \;2015-02-06 \; 22:24:17 \;1845 \;3441	\;33 \;14 \;21 \;861 \;3218)
$$

There are a number of different functions $f$ may be used to transform our raw input into a form suitable for a deep learning program.  In the spirit of avoiding as much formatting as possible, we can assign $f$ to be a one-hot character encoding, and then flatten the resulting tensor if our model requires it. One-hot encodings take an input and transform this into a tensor of size $\vert n \vert$ where $n$ is the number of possible categories, where the position of the input among all options determines which element of the tensor is $1$ (the rest are zero).  For example, a one-hot encoding on the set of one-digit integers could be

$$
f(0) = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] \\
f(1) = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] \\
f(2) = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] \\
\vdots \\
f(9) = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
$$

Note that it does not matter which specific encoding method we use, and that any order of assignment will suffice (for example, we could swap $f(0)$ and $f(1)$).  Nor does it particularly matter that we place a $1$ in the appropriate position for our tensor: any constant $c$ would work, as long as this constant is not changed between inputs.  What **is** important is that each category of input is linearly independent of all other categories, meaning tat we cannot apply some compression such as

$$
f(0) = [1, 0] \\
f(1) = [0, 1] \\
f(2) = [1, 1] \\
\vdots
$$

or even simply assign each value to a single tensor,

$$
f(0) = [0] \\
f(1) = [1] \\
f(2) = [2] \\
\vdots
$$

These and any other methods of encoding such that each input category is no longer linearly independent of each other (and not normed to one constant $c$) are generally bad because it introduces information about the input $x$ that is not likely to be true: if we were encoding alphabetical characters instead of integers, each character should be 'as different' from each other character but this would not be the case if we applied a linearly dependent encoding.

For some perspective, we can compare this input method to a more traditional approach that is currently used for deep learning.  The strategy for that approach is as follows: inputs that are low-dimension may be supplied as one-hot encodings (or as basis vectors of another linearly independent vector space) and inputs that are high-dimensional (such as words in the english language) are first embedded using a neural network, and the result is then given as an input for the chosen deep learning implementation.

Embedding is the process of attempting to recapitulate the input on the output, while using less information that originally given. The resulting embedding, also called a latent space, is no longer generally linearly independent but instead contains information about the input itself: for example, vectors for the words 'dog' and 'cat' would be expected to be more similar to each other than the vectors 'dog' and 'tree'.  

Such embeddings would certainly be possible for our character-based approach, and may even be optimal, but the following intuition may explain why they are not necessary for successful application of our sequential encoding: we know that any deep learning model will reduce dimensionality to 0 (in the case of regression) or $n$ (for $n$ categories) depending on the output, so we can expect the model itself to perform the same tasks that an embedding would.  This is perhaps less efficient than separating out the embedding before then training a model on a given task, but it turns out that this method can be employed quite successfully.  

To summarize, we can bypass not only the need for hand-chosen formatting and normalization procedures but also for specific embeddings when using a sequential character-based input approach.

### Weaknesses of recurrent neural net architectures

One choice remains: how to deal with missing features.  For models such as (1), this may be accomplished by simply removing training examples that exhibit missing features, and enforcing the validation or test datasets to have some value for all features. The same approach could be applied here, but such a strategy could lead to biased results.  What happens when not only a few but the majority of examples both for training and testing are missing data on a certain feature?  Removing all examples would eliminate the majority of information in our training dataset, and is clearly not the best way to proceed.

Perhaps the simplest way to proceed would be to assign our sequence-to-tensor mapping function $f$ to map an empty feature to an empty tensor.  For example, suppose we wanted to encode an $x_i$ which was missing the first feature value of $x_c$:

$$
x_c = f(1 \;	2015-02-06 \; 22:24:17 \;1845 \;3441	\;33 \;14 \;21 \;861 \;3218) \\
x_i = f( \;	2015-02-06 \; 22:24:17 \;1845 \;3441	\;33 \;14 \;21 \;861 \;3218)  
$$

in this case $x_c = x_i$ except for the first 10 elements of $x_c$, which $x_i$ omits.  The non-concatenated tensors would be

$$
x_c = [[0,1,0,0,0,0,0,0,0,0],[0,0,1,...] ...]\\
x_i = [[],[0,0,1,...] ...]
$$

which means that the concatenated versions begin as $[0,1,0...]$ for $x_c$ and $[0,0,1,...]$ for $x_i$, resulting in a variable-length $x$.

A specific class of deep learning models have been designed for variable-length inputs. These are recurrent neural networks, and they operate by examining each input sequentially (ie for $x_c$ the network would first take an input as $[0,1,0,0,0,0,0,0,0,0]$), using that input to update the activations for each neuron according the the weights $w$ and biases $b$ present in that network, and then perhaps generating an output which can be viewed as an observation of the activations of the final layer of neurons. 

For our regression problem, the only output that we would need to be concerned with is the output after the last input was recieved by the network. This output would then be used for loss back-propegation according to the gradient of the loss.

One difficulty with recurrent neural networks is that they may suffer from exploding gradients: if each input adds to the previous, so can the gradient from each output leading to exceptionally large gradients for long sequences and thus difficulties during stochastic gradient descent.  This particular problem may be remedied a number of different ways, one of which involves gating the activations for each sequential input.  Another problem is that because network activations are added together, pure recurrent networks often have difficulty 'remembering' information fed into the network at the start of the sequence whilst nearing the end.

Both of the problems in the last paragraph are addressed using the Long Short-term Memory recurrent architecture, which keeps both short-term as well as a long-term memory modules in the form of neurons that are activated and gated. However, even these and other modified recurrent nets do not address perhaps the most difficult problem of recurrence during training: the process has a tendancy to 'forget' older examples relative to non-recurrent architectures, as there is $n^2$ distance between the last update and first update with respect to parameter activations.  See [this page](https://blbadger.github.io/neural-networks2.html) for more informatino on this topic.

That said, recurrent neural net architectures can be very effective deep learning procedures.  But there is a significant drawback to using them for our case: we lose information on the identity of each feature.  To explain why this is, imagine observing the following tensor as the first input: $[0,1,0...]$. There is not way a priori for you to know which feature this input tensor corresponds to, whereas in our grid of values above, we certainly would know which column (and therefore which feature) an empty value was located inside. 

The importance of maintaining positional information for tasks such as these has been borne out by experiment: identical LSTM-based neural networks perform far better with respect to minimization of test error using input functions $f$ that retains positional information compared to $f$ that do not.

### Structured sequence input encoding 

How do we go about preserving positional information for a sequence in order to maintain feature identity for each input element?  A relatively simple but effective way of doing this is to fix the number of elements that are assigned to be a constant value $c$, and then to provide a place-holder value $v$ for however many elements that a feature is missing.  In our example above, this could be accomplished by adding an eleventh element to each tensor (which we can think of as denoting 'empty') and performing one-hot encoding using this expanded vocabulary,

$$
x_c = [[0,1,0,0,0,0,0,0,0,0,0],[0,0,1,...] ...]\\
x_i = [[0,0,0,0,0,0,0,0,0,0,1],[0,0,1,...] ...]
$$

The important thing here is to keep the element denoting 'empty' to be one that is rarely if ever used to denote anything else.  For example, if we were to use the tensor for $f(0)$ to denote an empty character, we would lose information if $0$ were found in any of the features because a priori the model cannot tell if the tensor $f(0)$ denotes a zero element or an empty element.

This structured input can be implemented as follows: first we import relevant libraries and then the class `Format` is specified.  Note that source code for this page may be found [here](https://github.com/blbadger/nnetworks/tree/master/interprets).

```python
# fcnet.py
# import standard libraries
import string
import time
import math
import random

# import third-party libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd
from sklearn.utils import shuffle

import torch
import torch.nn as nn

class Format:

	def __init__(self, file, training=True):

		df = pd.read_csv(file)	
		df = df.applymap(lambda x: '' if str(x).lower() == 'nan' else x)
		df = df[:10000]
		length = len(df['Elapsed Time'])
		self.input_fields = ['Store Number', 
				'Market', 
				'Order Made',
				'Cost',
				'Total Deliverers', 
				'Busy Deliverers', 
				'Total Orders',
				'Estimated Transit Time',
				'Linear Estimation']

		if training:
			df = shuffle(df)
			df.reset_index(inplace=True)

			# 80/20 training/validation split
			split_i = int(length * 0.8)

			training = df[:][:split_i]
			self.training_inputs = training[self.input_fields]
			self.training_outputs = [i for i in training['positive_two'][:]]

			validation_size = length - split_i
			validation = df[:][split_i:split_i + validation_size]
			self.validation_inputs = validation[self.input_fields]
			self.validation_outputs = [i for i in validation['positive_two'][:]]
			self.validation_inputs = self.validation_inputs.reset_index()

		else:
			self.training_inputs = self.df # not actually training, but matches name for stringify
```
Then in this class we implement a method that converts each row of our design matrix into a sequence of characters, which will be used as an argument to $f()$.

```python
class Format:
  ...
  
  def stringify_input(self, index, training=True):
		"""
		Compose array of string versions of relevant information in self.df 
		Maintains a consistant structure to inputs regardless of missing values.

		Args:
			index: int, position of input

		Returns:
			array: string: str of values in the row of interest

		"""


		taken_ls = [4, 1, 8, 5, 3, 3, 3, 4, 4]

		string_arr = []
		if training:
			inputs = self.training_inputs.iloc[index]
		else:
			inputs = self.validation_inputs.iloc[index]

		fields_ls = self.input_fields
		for i, field in enumerate(fields_ls):
			entry = str(inputs[field])[:taken_ls[i]]
			while len(entry) < taken_ls[i]:
				entry += '_'
			string_arr.append(entry)

		string = ''.join(string_arr)
		return string
```
Now we implement another class method which will perform the task of $f()$, ie of converting a sequence of characters to a tensor.  In this particular example, we proceed with concatenating each character's tensor into one in the `tensor = tensor.flatten()` line because we are going to be feeding our inputs into a simple fully-connected feed-forward neural network architecture.

```python
	@classmethod
	def string_to_tensor(self, input_string):
		"""
		Convert a string into a tensor

		Args:
			string: str, input as a string

		Returns:
			tensor: torch.Tensor() object
		"""
		
		places_dict = {s:i for i, s in enumerate('0123456789. -:_')}
		
		# vocab_size x batch_size x embedding dimension (ie input length)
		tensor_shape = (len(input_string), 1, 15) 
		tensor = torch.zeros(tensor_shape)

		for i, letter in enumerate(input_string):
			tensor[i][0][places_dict[letter]] = 1.

		tensor = tensor.flatten()
		return tensor 
```
and now we can assemble a neural network. Here we implement a relatively simple fully-connected network by inheriting from the `torch.nn.Module` library and specify a 5-layer (3 hidden layer) architecture.
 
 ```python
 class MultiLayerPerceptron(nn.Module):

	def __init__(self, input_size, output_size):

		super().__init__()
		self.input_size = input_size
		hidden1_size = 500
		hidden2_size = 100
		hidden3_size = 20
		self.input2hidden = nn.Linear(input_size, hidden1_size)
		self.hidden2hidden = nn.Linear(hidden1_size, hidden2_size)
		self.hidden2hidden2 = nn.Linear(hidden2_size, hidden3_size)
		self.hidden2output = nn.Linear(hidden3_size, output_size)
		self.relu = nn.ReLU()
		self.dropout = nn.Dropout(0.3)

	def forward(self, input):
		"""
		Forward pass through network

		Args:
			input: torch.Tensor object of network input, size [n_letters * length]

		Return: 
			output: torch.Tensor object of size output_size

		"""

		out = self.input2hidden(input)
		out = self.relu(out)
		out = self.dropout(out)

		out = self.hidden2hidden(out)
		out = self.relu(out)
		out = self.dropout(out)

		out = self.hidden2hidden2(out)
		out = self.relu(out)
		out = self.dropout(out)

		output = self.hidden2output(out)
		return output
 ```
 
Note that for the positive control experiments below, dropout is disabled during training.

This architecture may be understood as accomplishing the following: the last layer is equivalent to a linear model $y=w^Tx' + b$ on the final hidden layer $x'$ as an input, meaning that the hidden layers are tasked with transforming the input vector $x$ into a representation $x'$ that is capable of being modeled by (1).

Finally we can choose an objective (loss) function and an optimization procedure.  Here we use L1 loss rather than MSE loss as our objective function because it usually results in less overfitting (as it fits a $\hat y$ to the median rather than the mean of an appropriate input $x$)

$$
L^1_{loss} = \frac{1}{m} \sum_i \vert \hat {y_i} - y_i \vert
$$

where $m$ is the number of elements indexed by $i$. Here we use Adaptive moment estimation, a variant of stochastic gradient descent, as our optimization procedure.  Also employed is gradient clipping, which prevents gradient tensors with poor condition number from adversely affecting optimization.

```python
	def __init__(self,...):
	self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)
	...
	
	def train_minibatch(self, input_tensor, output_tensor, minibatch_size):
		"""
		Train a single minibatch

		Args:
			input_tensor: torch.Tensor object 
			output_tensor: torch.Tensor object
			optimizer: torch.optim object
			minibatch_size: int, number of examples per minibatch
			model: torch.nn

		Returns:
			output: torch.Tensor of model predictions
			loss.item(): float of loss for that minibatch

		"""

		output = self.model(input_tensor)
		output_tensor = output_tensor.reshape(minibatch_size, 1)
		loss_function = torch.nn.L1Loss()
		loss = loss_function(output, output_tensor)

		self.optimizer.zero_grad() # prevents gradients from adding between minibatches
		loss.backward()

		nn.utils.clip_grad_norm_(self.model.parameters(), 0.3)
		self.optimizer.step()

		return output, loss.item()
```

### Controls

Before applying our model to the dataset in question, we can directly assess the efficacy or our sequential character encoding method by applying the model to what in experimental science is known as a positive control.  This is a (usually synthetic) dataset in which the desired outcome is known beforehand, such that if the experimental apparatus were successful in being able to perform its function then we would be able to arrive at some specific output.  

In this case, we are interested in determining whether or not this relatively small neural network is capable of learning defined functions, in contrast to the as-yet undefined function that determines the actual delivery time (together with any Bayesian error). Our first control is as follows:

|Market	|Order Made|	Store Number	|Cost |Total Deliverers	|Busy Deliverers	|Total Orders	|Estimated Transit Time	|Linear Estimation	|Control Output|
| ----- | ---------| -------------- | --- | --------------- | --------------- | ----------- | --------------------- | ----------------- | ---------- |
|1	|2015-02-06 22:24:17	|1845	|3441	|33	|14	|21	|861	|3218	|330|
|2	|2015-02-10 21:49:25	|5477	|1900	|1	|2	|2	|690	|2818	|10|
|3	|2015-01-22 20:39:28	|5477	|1900	|1	|0	|0	|690	|3090	|10|
|3	|2015-02-03 21:21:45	|5477	|6900	|1	|1	|2	|289	|2623	|10|

where the function is

$$
y = 10d
$$

where $d$ is the **Total Deliverers** input. We can track test (previously unseen) accuracy during training on any regression by plotting the actual value (x-axis) agains the model's prediction (y-axis): a perfect model will have all points aligned on the line $y=x$.  Plotting the results of the above model on a test (unseen during training) dataset every 23 epochs, we have

{% include youtube.html id='KgCuK6v_MgI' %}

so clearly for this control function, the model arrives at near-perfect accuracy fairly quickly.  It is important here to note that this is a much more difficult problem for the model than at first it may seem because of the encoding method: rather than simply assign an appropriate weight to an input neuron encoding $d$ instead the model must learn to decode the encoding function, and then apply the appropriate weight to the decoded value.  In effect, we are enforcing a distributed representation of the input because the encoding method itself is distributed (here $4*15=60$ tensor values rather than $1$).

We can also experiment with the network learning a more complicated rule, this time the non-linear (with respect to the input) mapping

$$
y = (c/100) * b
$$

where $b$ is the **Busy Deliverers** input and $c$ is the **Cost** input field.  Once again, the model is capable of very accurate estimations on the test dataset (actual outputs on the x-axis and expected outputs on the y-axis)

{% include youtube.html id='rZRQa3ExzTU' %}

with the trained model achieving $R^2>0.997$, meaning that less than $0.3%$ of the variation in the actual value was not accounted for by the model's prediction. Further observation shows that a small number of points are poorly predicted by the model, to be specific the trained model yields far lower expected values compared to the actual $y$ value.  Why could this be?  On explanation is that this nonlinear function is more difficult to fit for our model, but this does not seem likely given that the model was capable of fitting a quite complicated nonlinear function to the first control.  This is because the input encoding requires the model to be able to decode a sequence of characters into a number, such that the model must learn a far more complicated function than $y=10d$.

If the model is capable Observing the **Cost** input, we find that a small number of examples contain 5 digit cost values.  Our encoding scheme only takes 4 characters from that input, which results in ambiguous information being fed to the model, as $13400$ and $1340$ would be indistinguishable.  We can rectify this by assigning the **Cost** input to take 5 characters as follows:

```python
taken_ls = [4, 1, 8, 5, 3, 3, 3, 4, 4]
```
which yields 

{% include youtube.html id='Obmzk-_MUhw' %}

and indeed accuracy has been greatly increased for these examples.  There is some experimental evidence that the estimation accuracy for both positive controls diminishes as the number of training examples increases, or in symbols $\sum_i \hat {y_i} - y_i \to 0$ as $i \to \infty$.

These examples show that certain known functions on the input are capable of being approximated quite well by a structured sequence -based encoding method when fed to a relatively small fully connected feedforward neural network.

### Generalization and language model application

The greatest advantage of the structured sequence input is its flexibility: because all inputs are converted to a single data type automatically, the experimentor does not need to determine the method by which each input is encoded.  Thus we are able to combine heterogeneous inputs not only consisting of integers and timestamps but also categorical variables, language strings, images, and even audio input (provided that it has been digitized).  

Up to now, the input method has been designed with only one problem in mind.  The number of elements per input field was specified as

```python
taken_ls = [4, 1, 15, 5, 4, 4, 4, 4, 4]
```

which is only applicable to that particular dataset.  This may be generalized a number of different ways, but one is as follows: we take either all or else a portion (`short=True`) of each input field, the number of elements of which is denoted by `n_taken` (which must be larger than the longest element if all characters in an input field are to be used)

```python
	def stringify_input(self, input_type='training', short=True, n_taken=5, remove_spaces=True):
		"""
		Compose array of string versions of relevant information in self.df 
		Maintains a consistant structure to inputs regardless of missing values.

		kwargs:
			input_type: str, type of data input requested ('training' or 'test' or 'validation')
			short: bool, if True then at most n_taken letters per feature is encoded
			n_taken: int, number of letters taken per feature
			remove_spaces: bool, if True then input feature spaces are removed before encoding

		Returns:
			array: string: str of values in the row of interest

		"""
		n = n_taken

		if input_type == 'training':
			inputs = self.training_inputs

		elif input_type == 'validation':
			inputs = self.val_inputs

		else:
			inputs = self.test_inputs

		if short == True:
			inputs = inputs.applymap(lambda x: '_'*n_taken if str(x) in ['', '(null)'] else str(x)[:n])
		else:
			inputs = inputs.applymap(lambda x:'_'*n_taken if str(x) in ['', '(null)'] else str(x))

		inputs = inputs.applymap(lambda x: '_'*(n_taken - len(x)) + x)
		string_arr = inputs.apply(lambda x: '_'.join(x.astype(str)), axis=1)

		return string_arr
```

We assemble the strings into tensors in a similar manner as above for the `string_to_tensor` method, except that the dataset's encoding may be chosen to be any general set.  For example, if we were  except encoding to all ascii characters rather than only a subset. This makes the dimension of the model's encoding rise from 15 to 128 using `places_dict = {s:i for i, s in enumerate([chr(i) for i in range(128)])}`.  

A boolean argument `Flatten` may also be supplied, as some deep learning models are designed for language-like inputs

```python
	@classmethod
	def string_to_tensor(self, string, flatten):
		...
		if flatten:
			tensor = torch.flatten(tensor)
		return tensor
```

Structured sequence inputs are in some way similar to natural languages: both contain a string of characters, of which only a small subset of all possible sequences ever appears as an example.  One may therefore ask the question: can we apply specialized deep learning architectures developed for natural language processing to our structured sequence modeling tasks?

One architecture that is currently in use for large-scale language modeling is the transformer, which is a feedforward style network developed from recurrent neural network architectures that incorperated a concept called 'self-attention'.  In a self-attention module, each input (usually a word but here will be a letter) is associated with three vectors $K, Q, V$ for Key, Query, and Value that are produced from learned weight matricies $W^K, W^Q, W^V$.  Similarity between inputs to the first element (denoted by the vector $\pmb{s_1}$) is calculated by finding the dot product (denoted $*$) of one element's query vector with all other element's key vectors 

$$
\pmb{s_1} = (q_1*k_1, q_1*k_2, q_1*k_3,...)
$$

before a linear function is applied to each element followed by a softmax transformation to the vector $\pmb{s_1}$ to make $\pmb{s_1'}$.  Finally each of the resulting scalar components of $s$ are multiplied by the corresponding value vectors for each input $V_1, V_2, V_3,...$ and the resulting vectors are summed up to make the activation vector $\pmb{z_1}$ that is the same dimension as $V_1$

$$
\pmb{s_1'} = \mathbf{softmax} \; ((q_1*k_1)/ \sqrt d, (q_1*k_2)/ \sqrt d, (q_1*k_3)/ \sqrt d,...) \\
\pmb{s_1'} = (s_{11}', s_{12}', s_{13}',...) \\
\pmb{z_1} = V_1 s_{11}' + V_2 s_{12}' + V_3 s_{13}'+ \cdots
$$

The transformer is based on multi-head attention, which means that multiple self-attention $z_1$ vectors are obtained (and thus multiple $W^K, W^Q, W^V$ vectors are learned) for each input. The multi-head attention is usually followed by a layer normalization and fully connected layer (followed by another layer normalization) to make one transformer encoder. Multiple encoder modules are usually stacked sequentially, and for this page we will be using the following architecture that employs 3 encoder modules that feed into a single fully connected layer.

The transformer encoder and decoder modules are reportedly not particularly effective at retaining positional information from the input.  This is presumably due to the addition step used to make $\pmb{z_1}$, as after this step the resulting vector contains information of which other element is most similar to $s_1$, but not where that element was found.  To remedy this, a positional encoding is usually applied to the input prior to feeding the input to the transformer encoder.  The positional encoding may be performed a number of different ways, but one that is most common is to simply add the values of periodic functions such as $\sin, \cos$ to the input directly.  See the [code repo](https://github.com/blbadger/nnetworks/blob/master/interprets/transformer.py) for an implementation of this kind of positional encoding.

An aside: it has been claimed that positional encoding is necessary for transformer-style architectures because they lose practically all positional information during the multi-head attention stage.  This is not strictly true, as can be shown experimentally: simply permuting any given input usually yields a different output from the transformer encoder, meaning that order does indeed determine the output.  Indeed, simply removing positional encoding does not result in significant reductions in test accuracy, suggesting that for this application positional encoding is superfluous.

All together, the architecture is as follows:

![transformer]({{https://blbadger.github.io}}neural_networks/transformer.png)

and the implementation for this architecture is as follows: 

```python
class Transformer(nn.Module):
	"""
	Encoder-only tranformer architecture for regression.  The approach is 
	to average across the states yielded by the transformer encoder modules before
	passing this to a single hidden fully connected linear layer.
	"""
	def __init__(self, output_size, n_letters, d_model, nhead, feedforward_size, nlayers, minibatch_size, dropout=0.3):

		super().__init__()
		self.n_letters = n_letters
		self.posencoder = PositionalEncoding(d_model, dropout)
		encoder_layers = TransformerEncoderLayer(d_model, nhead, feedforward_size, dropout, batch_first=True)
		self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)
		self.transformer2hidden = nn.Linear(n_letters * d_model, 50)
		self.hidden2output = nn.Linear(50, 1)
		self.relu = nn.ReLU()
		self.minibatch_size = minibatch_size

	def forward(self, input_tensor):
		"""
		Forward pass through network

		Args:
			input_tensor: torch.Tensor of character inputs

		Returns: 
			output: torch.Tensor, linear output
		"""

		# apply (relative) positional encoding
		input_encoded = self.posencoder(input_tensor)
		output = self.transformer_encoder(input_encoded)

		# output shape: same as input (batch size x sequence size x embedding dimension)
		output = torch.flatten(output, start_dim=1)
		output = self.transformer2hidden(output)
		output = self.relu(output)
		output = self.hidden2output(output)

		# return linear-activation output
		return output
```

The transformer encoder by default applies a dropout of probability $0.1$ to each layer (multi-head attention or fully connected) before layer normalization.  As normalization via dropout has been disabled for the other positive controls on this page, batch normalization was disabled by calling `model.eval()` after the first minibatch trained.  Applied to the control function

$$
y = 10d
$$

over 200 epochs we have the following results on 2000 test examples (predicted output on the y-axis and actual on the x-axis):

{% include youtube.html id='9VxxOfFA_TA' %}

including an extra layer of 10 neurons before the output, we have a slightly better approximation

{% include youtube.html id='h6EaKP_6EXE' %}

The transformer encoder architecture was designed to yield a representation of the input that is then fed into a decoder to gives probabilistic outputs over a set of discrete variables, usually words. In some respects, having the representation from a transformer encoder feed into a fully connected network in order to perform regression is quite a different task because the function we wish to approximate is best understood as being continuous rather than discrete.  

Furthermore, the structured sequence inputs are one-hot encodings but the transformer was design to take in medium (~512) dimensional embeddings of words, which would usually not be one-hot encoded.  These embeddings (one for each word) would attempt to capture the most important aspects of that word in a sentance, 

### Justification of sequence-based character encodings

We have seen that the above encoding approach is effective, but why is this? It seems that we are simply feeding in raw information to the network and then are more or less by luck meeting with success.  Happily there is a clear explanation as to why this method is successful.

Consider the network architecture above: in this case the input is a tensor of concatenated one-hot vectors, and the output after three hidden layers is a single neuron that performs a linear regression on the second-to-last layer.  Omitting most components, the architecture may be visualized as follows:

![deep learning architecture]({{https://blbadger.github.io}}neural_networks/nn_architecture.png)

Now let's compare this to the normal approach to encoding categorical inputs that may be of high dimensionality.  Rather than employ large one-hot vectors, a better way is to perform embeddings on each of the features in question.  This may be done with autoencoder neural networks as follows: first each feature is separated and then one hidden layer of smaller dimensionality than that input feature is trained to return the input as well as possible.  The hidden layers contain what are normally called embeddings of the input.  

Broadly speaking, an embedding is a structure-preserving map of one abstract object in another, for example the natural numbers in the integers. In the context of machine learning, embeddings are usually defined as vector spaces in which some distance metric of points in that space corresponds to a metric of those points outside the space in some significant measure.  This means that more 'similar' examples (using some measure of significance to the machine learning task) will tend to be more 'similar', or in other words closer together, in the embedding.  Useful embeddings are usually of lower dimension than the space of the inputs.

These hidden layers may then be used as the input layer of the deep learning model that will then produce an output.  For a visualization of this process see below.

![deep learning architecture]({{https://blbadger.github.io}}neural_networks/nn_embeddings.png)

Consider what happens when we feed the input directly into the model, bypassing the embedding stage.  If successful, each hidden layer of the model will learn one or more distributed representations of the input, meaning that the input will be represented accross each hidden layer such that the final representation allows for the final neuron to perform a linear regression $\hat {y} = w^Tx + b$ and obtain an accurate result, regardless of the true function one attempts to model.

In the model above, it is clear that each successive representation of the input will be of lower dimension than the last because each layer contains fewer parameters than the last.  But in the general case, we can also say that between the input and output there is a representation that is of lower dimension than the input itself (otherwise training would be impossible, see the last section of [this page](https://blbadger.github.io/nn-limitations.html) for more details).  

Containing input information in a space of smaller dimension than the input itself does not mean that deep learning models contain embeddings. But because we know that each layer is composed of a relatively simple continuous non-linear transformation of the prior layer, the second-to-last layer of an accurate deep learning model must be able to provide information necessary for the final layer to carry out its task with only one non-linear transformation.  We know that the final layer's vector space must reflect the measure of significance placed on the input, and therefore we can expect that the second-to-last layer's vector space will also reflect this same measure of the input as most single non-linear used in deep learning applications transformations preserve relative distance.

Thus we are allowed to bypass an embedding stage because the model will be expected to obtain its own embeddings in each layer. 

![deep learning architecture]({{https://blbadger.github.io}}neural_networks/nn_including_embeddings.png)

The traditional approach of first training embeddings using autoencoders before proceeding to use those as inputs into the model is analagous to greedy layer-wise pretraining for the first layer of the network, with the objective function being a distance measurement from the input rather than the model's objective function.  

Despite the theoretical reasons behind why an embedding would be expected to exist in a trained deep learning model, it may be wondered if there is any evidence for the claim that a feed-forward neural network learns a set of embeddings on the input.  Happily we can simply use the (machine learning) definition of an embedding in order to test for the presence of this in a layer of a trained neural network.

Taking a trained version of the fully connected architecture shown in a previous section of this page, applied to the dataset with the output being a linear function of one input,

$$
y=10d
$$

we can test whether any layer forms an embedding on the inputs of this dataset by observing the correlation between the desired metric ($y$) for pairs of input examples versus some distance metric, say $L^1$, on the activations of a hidden layer.  Choosing the last hidden layer, we have the following 

![deep learning embedding]({{https://blbadger.github.io}}neural_networks/trained_linear_embedding.png)

{% include youtube.html id='YZJ3iokAzgk' %}

Thus we observe a correlation between the $l^1$ distance and the desired metric ($y$) and the final hidden layer for pairs of examples, which indicates that indeed the last hidden layer has formed a useful embedding on the input. We can also test whether the same model learns an embedding of the same inputs where the desired metric is

$$
y = (c/100) * b
$$

Before training, there is no correlation between pairs of examples in terms of actual distance versus embedding distance (both $L^1$ metric)

![deep learning embedding]({{https://blbadger.github.io}}neural_networks/untrained_embedding.png)

But after training, the correlation is found 

![deep learning embedding]({{https://blbadger.github.io}}neural_networks/trained_nonlinear_embedding.png)

{% include youtube.html id='KenWfEU2SLQ' %}







## Image Classification with Convolutional Neural Networks

### Introduction

In scientific research it is often important to be able to categorize samples accurately. With experience, the human eye is in many instances able to accurately discern and categorize many important features from microsopic observation. 

Another way to accomplish this task is to employ a computational method.  One could imagine attempting to program a machine to recognize the same features that an expert individual would take note of in order to replicate that individual's performance.  Historically this approach was taken in efforts resulting in what were called 'expert systems', but now such approaches are generally no longer attempted.  This is for a couple reasons: firstly because they are extremely tedious and time-consuming to implement, and secondly because once implemented these methods generally fail to be very accurate. The underlying cause for both of these points of failure is that it is actually very difficult to explain complicated tasks like image recognition precisely yet generally enough to be accurate.  

The failure of attempts to directly replicate complex tasks programatically has led to increased interest in the use of probability theory to make estimations rather than absolute statements, resulting in the expansion of a field of statistical learning.  Particularly difficult tasks like image classification often require particularly detailed computational methods for best accuracy, and this computationally-focused statistical learning is called machine learning.

The current state-of-the-art method for classifying images via machine learning is achieved with neural networks.  These are programs that use nodes called artificial 'neurons' that have associated weights and biases that are adjusted in accordance to some loss function in order to 'learn' during training.  These neuron are arranged in sequential layers, each representing the input in a potentially more abstract manner before the output layer is used for classification.  This sequential representation of the input in order to accomplish a machine learning task is the core idea behind the field of deep learning, encompasses artifical neural networks along with other models such as Boltzmann machines.

A hands-on introduction to the theory and utility of neural networks for image classification is found in Nielsen's [book](http://neuralnetworksanddeeplearning.com/), and the core algorithms of stochastic gradient descent and backpropegation that are used to train neural nets on this page are explained there.  For a deeper and more comprehensive study of this and related topics, perhaps the best resource is the [classic text](https://www.deeplearningbook.org/) by Goodfellow, Bengio, and Courville.  This page will continue with the assumption of some familiarity with the fundamental ideas behind deep learning.  Note thatexplanation of the utility of convolutions is given below, as this topic is particularly important to this page and is somewhat specialized to image-based deep learning models.

Neural networks are by no means simple constructions, but are not so prohibitively complicated that they cannot be constructed from scratch such that each operation to every individual neuron is clearly defined (see [this repo](https://github.com/blbadger/neural-network/blob/master/connected/fcnetwork_scratch.py) for an example).  But this approach is relatively slow: computing a tensor product element-wise in high-level programming languages (C, C++, Python etc) is much less efficient than computation with low-level array optimized libraries like BLAS and LAPACK.  In python, `numpy` is perhaps the most well-known and powerful library for general matrix manipulation, and this library can be used to simplify and speed up neural network implementations, as seen [here](https://github.com/blbadger/neural-network/blob/master/connected/fcnetwork.py). 

As effective as numpy is, it is not quite ideal for speedy computations with very large arrays, specifically because it does not optimize memory allocation and cannot make use of a GPU for many simultaneous calculations. To remedy this situation, there are a number of libraries were written  for the purpose of rapid computations with the tensors, with Tensorflow and PyTorch being the most popular.  Most of this page references code employing Tensorflow (with a Keras front end) and was inspired by the [Keras introduction](https://www.tensorflow.org/tutorials/keras/classification) and [load image](https://www.tensorflow.org/tutorials/load_data/images?hl=TR) tutorials.  

### Convolutions Explained

A convolution (in the context of image processing) is a function in which a pixel's value is added to that of its neighbors according to some given weight.  The set of weights are called the kernal or filter, and are usually denoted as $\omega$. Arguably the simplest example is the uniform kernal, which in 3x3 is as follows:

$$
\omega = 
\frac{1}{9}
\begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1 \\
\end{bmatrix}
$$

To perform a convolution, this kernal is applied to each pixel in the input to produce an output image in which each pixel corresponds to the average of itself along with all adjacent pixels.  To see how this kernal blurs an input, say the following pixel values were found somewhere in an image $f(x, y)$:

$$
f(x, y) = 
\begin{bmatrix}
1 & 2 & 3 \\
0 & 10 & 2 \\
1 & 3 & 0 \\
\end{bmatrix}
$$

The convolution operation applied to this center element of the original image $f(x_1, y_1)$ with pixel intensity $10$ now is

$$
\omega * f(x_1, y_1) =
\frac{1}{9}
\begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1 \\
\end{bmatrix} 
*
\begin{bmatrix}
1 & 2 & 3 \\
0 & 10 & 2 \\
1 & 3 & 0 \\
\end{bmatrix}
$$

The two-dimensional convolution operation is computed in an analagous way to the one-dimensional vector dot (scalar) product.  Indeed, the convolutional operation is what is called an inner product, which is a generalization of the dot product into two or more dimensions.  Dot products convey information of both magnitude and the angle between vectors, and similarly inner products convey both a generalized magnitude and a kind of multi-dimensional angle between operands.  

The calculation for the convolution is as follows:

$$
\omega * f(x_1, y_1) = 1/9 (1 \cdot 1 + 1 \cdot 2 + 1 \cdot 3 + \\
1 \cdot 0 + 1 \cdot 10 + 1 \cdot 2 + 1 \cdot 1 + 1 \cdot3 + 1 \cdot 0) \\
= 22/9 \\
\approx 2.44 < 10
$$

so that the convolved output $f'(x, y)$ will have a value of $2.4$ in the same place as the unconvolved input $f(x, y)$ had value 10.  

$$ f'(x, y) = 
\begin{bmatrix}
a_{1, 1} & a_{1, 2} & a_{1, 3} \\
a_{2, 1} & a_{2, 2} = 22/9 & a_{2, 3} \\
a_{3, 1} & a_{3, 2} & a_{3, 3} \\
\end{bmatrix}
$$

Thus the pixel intensity of $f'(x, y)$ at position $m, n = 2, 2$ has been reduced.  If we calculate the other values of $a$, we find that it is more similar to the values of the surrounding pixels compared to the as well. The full convolutional operation simply repeats this process for the rest of the pixels in the image to calculate $f'(x_1, y_1), ..., f'(x_m, y_n)$.  A convolution applied using this particular kernal is sometimes called a normalized box blur, and as the name suggests it blurs the input slightly. 

But depending on the kernal, we can choose to not blur an image at all. Here is the identity kernal, which gives an output image that is identical with the input.

$$
\omega = 
\begin{bmatrix}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0 \\
\end{bmatrix}
$$

We can even sharpen an input, ie decrease the correlation between neighboring pixels, with the following kernal.

$$
\omega = 
\begin{bmatrix}
0 & -1/2 & 0 \\
-1/2 & 2 & -1/2 \\
0 & -1/2 & 0 \\
\end{bmatrix}
$$

Why would convolutions be useful for deep learning models applied to images?

Feed-forward neural networks are composed of layers that sequentially represent the input in a better and better way until the final layer is able to perform some task.  This sequential representation is learned by the model and not specified prior to training, allowing neural networks to accomplish an extremely wide array of possible tasks.

When passing information from one layer to the next, perhaps the simplest approach is to have each node ('artificial neuron') from the previous layer connect with each node in the next.  This means that for one node in the second layer of width $n$, the activation $a_{m, n}$ is computed by adding all the weighted activations from the previous layer of width $m$ to the bias for that node $b_n$

$$
a_{m, n} =  \left( \sum_m w_m z_m \right ) + b_n
$$

where $z_m$ is the result of a (usually nonlinear) function applied to the activation $a_m$ of the previous layer's node to result in an output.  The same function is then applied to $a_{m, n}$ to make the output for one neuron of the second layer, and the process is repeated for all node in layer $n$.

This is called a fully connected architecture, as each node from one layer is connected to all the nodes in the adjacent layers.  For a two-layer network,  it is apparent that the space complexity of this architecture with regards to storing the model's parameters scales quadratically with $mn$, meaning that for even moderately sized inputs the number of parameters becomes extremely large if the second layer is not very small.

One approach to dealing with this issue is to simply not connect all the nodes from each layer to the next.  If we use a convolution, we can learn only the parameters necessary to define that convolution, which can be a very small number (9 + 1 for a 3x3 kernal with a bias parameter).  The weights learned for a kernal are not changed as the kernal is scanned accross the input image (or previous layer), although it is common practice to learn multiple kernals at once in order to allow more information to pass between layers.  

The convolutional operation is slightly different when applied to inputs with many sub-layers, usually termed 'features'.  One example of this would be a natural image of 3 colors, each with its own sub-layer (R, G, B). It is standard practice to have each kernal learn a set of weights for each feature in the input, rather than share the weights for all features. Precisely, this means that for an image with three color channel and one output feature and a 3x3 kernal will learn $3 * 3 * 3$ weight and one bias parameters, which are used to calculate the output value 

$$
a_{m, n} =  \sum_f \sum_k \sum_l w_{f, k, l} z_{f, m+k, n+l} + b_n
$$

Each feature of the output learns a new kernal, meaning that for an input image with $3$ features $f_i$ and an output of $6$ features $f_o$ for the same kernal with $k$ parameters (here 3x3), there are $k * f_i * f_o = ( 3* 3) * 3* 6 = 162$ weight parameters in total.  Note that the number of parameters required increases quadratically with the number of features in the input and output of the convolution, meaning that practical convolutional layers are limited in terms of how many features they can possibly learn.

Practicality aside, convolutions are very useful for image-based deep learning models.  In this section, we have seen how different kernals are able to sharpen, blur, or else do nothing to an input image.  This is not all: kernals can also transform an input to perform edge detection, texture filtering, and more.  The ability of a neural network to learn the weights of a kernal allows it to learn which of these operations should be performed across the entire image.


### Implementing a neural network to classify images

The first thing to do is to prepare the training and test datasets.  Neural networks work best with large training sets composed of thousands of images, so we split up images of hundreds of cells into many smaller images of one or a few cells.  We can also performed a series of rotations on these images:  rotations and translation of images are commonly used to increase the size of the dataset of interest.  These are both types of data augmentation, which is when some dataset is expanded by a defined procedure.  Data augmentation should maintain all invariants in the original dataset in order to be representative of the information found there, and in this case we can perform arbitrary translations and rotations.  But for other image sets, this is not true: for example, rotating a 6 by 180 degrees yields a 9 meaning that arbitrary rotation does not maintain the invariants of images of digits.

Code that implements this procedure is found [here](https://github.com/blbadger/nnetworks/blob/master/NN_prep_snippets.py). Similar image preparation methods are also contained in Tensorflow's preprocessing module.

Now let's design the neural network architecture.  We shall be using established libraries for this set, so reading the documentation for [Keras](https://keras.io/) and [Tensorflow](https://www.tensorflow.org/api_docs) may be useful.  First we write a docstring for our program stating its purpose and output, and import the relevant libraries.


```python
# Standard library
from __future__ import absolute_import, division, print_function, unicode_literals
import os
import pathlib

# Third-party libraries
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from PIL import Image 
import numpy as np 
import matplotlib.pyplot as plt 

```

Now we can assign the directories storing the training and test datasets assembled above to variables that will be called later.  Here `data_dir` is our training dataset and `data_dir2` and `data_dir3` are test datasets, which link to folders I have on my Desktop.

```python
def data_import():
	"""
	Import images and convert to tensorflow.keras.preprocessing.image.ImageDataGenerator
	object

	Args:
		None

	Returns:
		train_data_gen1: ImageDataGenerator object of image for training the neural net
		test_data_gen1: ImageDataGenerator object of test images
		test_data_gen2: ImageDataGenerator object of test images
	"""

	data_dir = pathlib.Path('/home/bbadger/Desktop/neural_network_images',  fname='Combined')
	data_dir2 = pathlib.Path('/home/bbadger/Desktop/neural_network_images2', fname='Combined')
	data_dir3 = pathlib.Path('/home/bbadger/Desktop/neural_network_images3', fname='Combined')
```

Now comes a troubleshooting step.  In both Ubuntu and MacOSX, `pathlib.Path` sometimes recognizes folders or files ending in `._.DS_Store` or a variation on this pattern.  These folders are empty and appear to be an artefact of using `pathlib`, as they are not present if the directory is listed in a terminal, and these interfere with proper classification by the neural network.  To see if there are any of these phantom files,

```python
	image_count = len(list(data_dir.glob('*/*.png')))

	CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') 
				if item.name not in ['._.DS_Store', '._DS_Store', '.DS_Store']])

	print (CLASS_NAMES)
	print (image_count)
```
prints the number of files and the names of the folders in the `data_dir` directory of choice.  If the number of files does not match what is observed by listing in terminal, or if there are any folders or files with the `._.DS_Store` ending then one strategy is to simply copy all files of the relevant directory into a new folder and check again.

Now the images can be rescaled (if they haven't been already), as all images will need to be the same size. Here all images are scaled to a heightxwidth of 256x256 pixels.  Then a batch size is specified, which determines the number of images seen for each training epoch.

```python
	### Rescale image bit depth to 8 (if image is 12 or 16 bits) and resize images to 256x256, if necessary
	image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
	IMG_HEIGHT, IMG_WIDTH = 256, 256

	### Determine a batch size, ie the number of image per training epoch
	BATCH_SIZE = 400

```
From the training dataset images located in `data_dir`, a training dataset `train_data_1` is made by calling `image_generator` on `data_dir`.  The batch size specified above is entered, as well as the `target_size` and `classes` and `subset` kwargs.  If shuffling is to be performed between epochs, `shuffle` is set to `True`.  

`image_generator` also has kwargs to specify rotations and translations to expand the training dataset, although neither of these functions are used in this example because the dataset has already been expanded via rotations.  The method returns a generator that can be iterated to obtain images of the dataset.

```python
	train_data_gen1 = image_generator.flow_from_directory(
		directory=str(data_dir),
		batch_size=BATCH_SIZE, 
		shuffle=True, 
		target_size=(IMG_HEIGHT,IMG_WIDTH), 
		classes=list(CLASS_NAMES), 
		subset = 'training')
```

The same process is repeated for the other directories, which in this case contain test image datasets.  An image set generation may be checked with a simple function that plots a subset of images. This is particularly useful when expanding the images using translations or rotations or other methods in the `image_generator` class, as one can view the images after modification.

```python
train_data_gen1, test_data_gen1, test_data_gen2 = data_import()
image_batch, label_batch = next(train_data_gen1)
```

Assigning the pair of labels to each iterable in the relevant generators,

```python
(x_train, y_train) = next(train_data_gen1)
(x_test1, y_test1) = next(test_data_gen1)
(x_test2, y_test2) = next(test_data_gen2)
```

Now comes the fun part: assigning a network architecture! The Keras `Sequential` module is a straightforward, if relatively limited, class that allows a sequential series of network architectures to be added into one model. This does not allow for branching or recurrent architectures, in which case the more flexible functional Keras `tensorflow.keras.Model` should be used, and below is an example of the network implemented with this module. After some trial and error, the following architecture was found to be effective for the relatively noisy images here.

For any neural network applied to image data, the input shape must match the image x- and y- dimensions, and the output should be the same as the number of classification options.  In this case, we are performing a binary classification between two cells, so the output layer of the neural network has 2 neurons and the input is specified by `IMG_HEIGHT, IMG_WIDTH` which in this case is defined above as 256x256.  

The first step when using `keras.Model` is to create a class that inherits from `keras.Model`, and it is a good idea to inherit the objects of the parent class using `super().__init__()` as well

```python
class DeepNetwork(Model):

    def __init__(self):
        super(DeepNetwork, self).__init__()
        self.entry_conv = Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(28, 28, 1), data_format='channels_last')
        self.conv16 = Conv2D(16, 3, padding='same', activation='relu')
        self.conv32 = Conv2D(32, 3, padding='same', activation='relu')
        self.conv32_2 = Conv2D(32, 3, padding='same', activation='relu')
        self.conv64 = Conv2D(64, 3, padding='same', activation='relu')
        self.conv64_2 = Conv2D(64, 3, padding='same', activation='relu')
        self.max_pooling = MaxPooling2D()

        self.flatten = Flatten()
        self.d1 = Dense(512, activation='relu')
        self.d2 = Dense(10, activation='softmax')
        
```

The way to read the `Conv2D` layer arguments is as follows: `Conv2D(16, 3, padding='same', activation='relu')` signifies 16 convolutional (aka filter) layers with a kernal size of 3, padded such that the x- and y-dimensions of each convolutional layer do not decrease, using ReLU (rectified linear units) as the neuronal activation function.  The stride length is by default 1 unit in both x- and y-directions.

Now the class method `call` may be defined.  This is a special method for classes that inherit from `Module` and is called every time an input is fed into the network and specifies way the layers initialized by `__init__(self)` are stitched together to make the whole network.

```python
    def call(self, model_input):
        out = self.entry_conv(model_input)
        for _ in range(2):
            out = self.conv16(out)
            out = self.max_pooling(out)
	    
        out2 = self.conv32(out)
        out2 = self.max_pooling(out2)
        for _ in range(2):
            out2 = self.conv32_2(out2)
	    
        out3 = self.max_pooling(out2)
        out3 = self.conv64(out3)
        for _ in range(2):
            out3 = self.conv64_2(out3)
            out3 = self.max_pooling(out3)
	    
        output = self.flatten(out3)
        output = self.d1(output)
        final_output = self.d2(output)
	
        return final_output

```
The output being 10 neurons corresponding to 10 class types, which could be the genotypes of cells or some similar attribute.  

Now the class `DeepNetwork` can be instantiated as the object `model`

```
model = DeepNetwork()
```

`model` is now a network architecture that may be represented graphically as

![neural network architecture]({{https://blbadger.github.io}}/neural_networks/neural_network.png)

For optimal test classification accuracy at the expense of longer training times, an extra dense layer of 50 neurons was added to the above architecture. This may be accomplished by initializing `self.d2` and adding this to `call()` 

```python
class DeepNetwork(Model):

    def __init__(self):
    ...
    
        self.d1 = Dense(512, activation='relu')
	self.d2 = Dense(50, activation='relu')
        self.d3 = Dense(10, activation='softmax')


	def call(self, model_input):
	...
	    output = self.flatten(out3)
	    output = self.d1(output)
	    output = self.d2(output)
	    final_output = self.d3(output)
	    return final_output
])
```

Now the network can be compiled, trained, and evaluated on the test datasets. `model.summary()` prints the parameters of each layer in our model for clarity.

```python
model.compile(optimizer='Adam', 
	loss = 'categorical_crossentropy', 
	metrics=['accuracy'])

model.summary()
model.fit(x_train, y_train, epochs=9, batch_size = 20, verbose=2)
model.evaluate(x_test1, y_test1, verbose=1)
model.evaluate(x_test2, y_test2, verbose=1)

```

A few notes about this architecture: first, the output is a softmax layer and therefore yields a probability distribution for an easy-to-interpret result.  The data labels are one-hot encoded, meaning that the label is denoted by a vector with one-hot tensor, ie instead of labels such as `[3]` we have `[0, 0, 1]`.  In Tensorflow's lexicon, categorical crossentropy should be used instead of sparse categorical crossentropy because of this.  

Second, as there are only two categories used for the experiments below, the final layer was changed to have a 2-neuron output.  Softmax was again used as the output activation function, which is not particularly advisable being that a 2-category softmax will tend to be unstable and result in high confidences for various predictions compared to the use of a sigmoid activation combined with a single-unit output (which is similar to a logistic regression for the final layer). 

Softmax was retained in the final layer in the experiments below in order to purposefully add some instability to the final layer and force the network to choose between options confidently.

### Accurate image classification

We follow a test/train split on this page: the model is trained on ~80% of the sample data and then tested on the remaining 20%, which allows us to estimate the model accuracy using data not exposed to the model during training. For reference, using a blinded approach by eye I classify 93 % of images correctly for certain dataset, which we can call 'Snap29' after the gene name of the protein that is depleted in the cells of half the dataset (termed 'Snap29') along with cells that do not have the protein depleted ('Control').  There is a fairly consistent pattern in these images that differentiates 'Control' from 'Snap29' images: depletion leads to the formation of aggregates of fluorescent protein in 'Snap29' cells.

The network shown above averaged >90 % binary accuracy (over a dozen training runs) for this dataset.  We can see these test images along with their predicted classification ('Control' or 'Snap29'), the confidence the trained network ascribes to each prediction, and the correct or incorrect predictions labelled green or red, respectively.  The confidence of assignment is the same as the activation of the neuron in the final layer representing each possibility.  

![neural network architecture]({{https://blbadger.github.io}}/neural_networks/nn_images_1.png)

Let's see what happens when the network is applied to an image set without a clear difference between the 'Control' and experimental group (this time 'Snf7', named  after the protein depleted from these cells in this instance).  After being blinded to the true classification labels, I correctly classified 71 % of images of this dataset.  This is better than chance (50 % classification accuracy being that this is a balanced binary dataset) and how does the network compare? The average training run results in 62 % classification accuracy.  We can see the results of one particular training run: the network confidently predicts the classification of nearly all images, but despite this confidence it is incorrect for many. Note that the confidence is a result of the use of softmax in the final layer.

![snf7 test accuracy]({{https://blbadger.github.io}}/neural_networks/nn_images_2.png)

Each time a network is trained, there is variability in how effective the training is even with the same datasets as inputs.  Because of this, it is helpful to observe a network's performance over many training runs (each run starting with a naive network and ending with a trained one)  The statistical language R (with ggplot) can be used to summarize data distributions, and here we compare the test accuracies of networks trained on the two datasets.

```R
# R
library(ggplot2)

data1 = read.csv('~/Desktop/snf7_vs_snap29color_deep.csv')
attach(data1)
fix(data1)
l <- ggplot(data1, aes(comparison, test_accuracy, fill=comparison))
l + geom_boxplot(width=0.4) +
    geom_jitter(alpha=0.5, position=position_jitter(0.1)) +
    theme_bw(base_size=14) + 
    ylim(50, 100) +
    ylab('Test accuracy') +
    xlab('Dataset')
    
```

This yields

![neural network architecture]({{https://blbadger.github.io}}/neural_networks/nn_images_3.png)

### Generalization and training velocity

Why does the network confidently predict incorrect answers for the Snf7 dataset?  Let's see what happens during training.  One way to gain insight into neural network training is to compare the accuracy of training image classification at the end of each epoch.  This can be done in R as follows:

```R
# R
library(ggplot2)
data7 = read.csv('~/Desktop/snf_training_deep.csv')
data6 = read.csv('~/Desktop/snap29_training_deep_2col.csv')

q <- ggplot() +
  # snf7
  geom_smooth(data=data7, aes(epoch, training.accuracy), fill='blue', col='blue', alpha=0.2) +
  geom_jitter(data=data7, aes(epoch, training.accuracy), col='blue', alpha=0.4, position=position_jitter(0.15)) +
  # snap29
  geom_smooth(data=data6, aes(epoch, training.accuracy), fill='red', col='red', alpha=0.2) +
  geom_jitter(data=data6, aes(epoch, training.accuracy), col='red', alpha=0.4, position=position_jitter(0.15))

q + 
  theme_bw(base_size=14) +
  ylab('Training accuracy') +
  xlab('Epoch') +
  ylim(50, 105) + 
  scale_x_continuous(breaks = seq(1, 9, by = 1))
  
```

As backpropegation lowers the cost function, we would expect for the classification accuracy to increase for each epoch trained. For the network trained on the Snap29 dataset, this is indeed the case: the average training accuracy increases in each epoch and reaches ~94 % after the last epoch.  But something very different is observed for the network trained on Snf7 images: a rapid increase to 100 % training accuracy by the third epoch is observed.

![neural network architecture]({{https://blbadger.github.io}}/neural_networks/nn_images_4.png)

The high training accuracy (100%) but low test accuracy (62 %) for the network on Snf7 dataset images is indicative of a phenomenon in statistical learning called overfitting.  Overfitting signifies that a statistical learning procedure is able to accurately fit the training data but fails to generalize previously-unseen test dataset.  Statistical learning models (such as neural networks) with many degrees of freedom are somewhat prone to overfitting because small variations in the training data are able to be captured by the model regardless of whether or not they are important for true classification.  

Overfitting is intuitively similar to memorization: both lead to an assimilation of information previously seen, without this assimilation necessarily helping the prediction power in the future.  One can memorize exactly how letters look in a serif font, but without the ability to generalize then one would be unable to indentify many letters in a sans-serif font.  The goal for statistical learning is to make predictions on hitherto unseen datasets, and thus to avoid memorization which does not guarantee this ability.

Thus the network overfits Snf7 images but is able to generalize for Snap29 images.  This makes intuitive sense from the perspective of manual classification, as there is a relatively clear pattern that one can use to distinguish Snap29 images, but little pattern to identify Snf7 images.  

A slower learning process for generalization is observed compared to that which led to overfitting, but perhaps some other feature could be causing this delay in training accuracy.  The datasets used are noticeably different: Snap29 is in two colors whereas Snf7 is monocolor.  If a deeper network (with the extra layer of 50 dense neurons before the output layer) is trained on a monocolor version of the Snap29 or Snf7 datasets, the test accuracy achieved is nearly identical to what was found before,

![neural network architecture]({{https://blbadger.github.io}}/neural_networks/nn_images_5.png)

And once again Snap29 training accuracy lags behind that of Snf7.

![neural network architecture]({{https://blbadger.github.io}}/neural_networks/nn_images_8.png)

### AlexNet revisited

To see if the faster increase in training accuracy for Snf7 was peculiar to the particular network architecture used, I implemented a model that mimics the groundbreaking architecture now known as [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), and the code to do this may be found [here](https://github.com/blbadger/neural-network/blob/master/AlexNet_sequential.py).  There are a couple differences between my recreation and the original that are worth mentioning: first, the original was split across two machines for training, leading to some parallelization that was not necessary for my training set.  More substantially, AlexNet used a somewhat idiosyncratic normalization method that is related to but distinct from batch normalization, which has been substituted here.  

Using this network, it has previously been seen that overfitting is the result of slower increases in training accuracy relative to general learning (see [this paper](https://arxiv.org/abs/1611.03530) and [another paper](https://dl.acm.org/doi/10.5555/3305381.3305406)).  With the AlexNet clone, once again the training accuracies for Snf7 increased faster than for Snap29 (although test accuracy was poorer for both relative to the deep network above).  This suggests that faster training leading to overfitting in the Snf7 dataset is not peculiar to one particular network architecture and hyperparameter choice.

![neural network architecture]({{https://blbadger.github.io}}/neural_networks/nn_images_10.png)

![neural network architecture]({{https://blbadger.github.io}}/neural_networks/nn_images_11.png)

There are a number of interesting observations from this experiment.  Firstly, the multiple normalization methods employed by AlexNet (relative to no normalization used for our custom architecture) are incapable of preventing severe overfitting for Snf7, or even for Snap29.  Second, with no modification to hyperparameters the AlexNet architecture was able to make significant progress towards classifying Snap29 images, even though this image content is far different from the CIFAR datasets that AlexNet was designed to classify.  The third observation is detailed in the next section.

### How learning occurs

At first glance, the process of changing hundreds of thousands of parameters for a deep learning model seems to be quite mysterious. For a single layer it is perhaps simpler to understand how each component of the input influences the output such that the gradient of each component wrt. a function on the output may be computed, and once that is done the value of that parameter may be changed accordingly.  But with many layers of input representation between the input and output, how does a gradient update to one layer not affect other layers? How accurate is gradient computation using backpropegation for deep learning models?

The answer to the latter question is that it is fairly accurate and not too difficult to compute the gradient itself, but the question of how the successive layers influence each other during gradient update is an outstanding one in the field of deep learning and has no clear answer as yet.

This does not mean that we cannot try to understand how learning occurs regardless. One way we can do this is to observe how the accuracy or model loss function changes over time as a model is fed certain inputs, and this is done in the preceding sections on this page. A more direct question may also be addressed: what does the model learn to 'look at' in the input?

We can define what a model 'looks at' most in the input as the inputs that change the output of the model the most, which is called input attribution.  Attribution may be calculated in a variety of different ways, and here we will use a particularly intuitive method: gradient*input. The gradient of the output with respect to the input, projected onto the input, tells us how each input component changes when the output is moving in the direction of greatest ascent by definition. We simply multiply this gradient by the input itself to find how each input component influences the output.  Navigate over to [this page](https://blbadger.github.io/nn_interpretations.html) for a look at another attribution method and more detail behind how these are motivated.

Thus we are interested in the gradient of the model's output with respect to the input multiplied by the input itself,

$$
\nabla_a O(a; \theta) * a
$$

where $\nabla_a$ is the gradient with respect to the input tensor (in this case a 1x256x256 monocolor image) and $O(a; \theta)$ is the output of our model with parameters $\theta$ and input $a$ and $*$ denotes Hadamard (element-wise) multiplication.  This may be accomplished in Tensorflow as follows:

```python
def gradientxinput(features, label):
	...
	optimizer = tf.keras.optimizers.Adam()
	features = features.reshape(1, 28, 28, 1)
	ogfeatures = features # original tensor
	features = tf.Variable(features, dtype=tf.float32)
	with tf.GradientTape() as tape:
		predictions = model(features)

	input_gradients = tape.gradient(predictions, features).numpy()
	input_gradients = input_gradients.reshape(28, 28)
	ogfeatures = ogfeatures.reshape(28, 28)
	gradxinput = tf.abs(input_gradients) * ogfeatures
	...
```
such that `ogfeatures` and `gradxinput` may be fed directly into `matplotlib.pyplot.imshow()` for viewing. 

Earlier it was noted that a human may learn to distinguish between a healthy and unhealthy cell by looking for clumps of protein in the images provided.  Does a neural network perform classification the same way?  Applying our input attribution method to one particular example of an unhealthy cell image for a trained model, we have

![gradients]({{https://blbadger.github.io}}/neural_networks/snf7_gradxinput.png)

where the attributions are in purple and the original input image is in grayscale.  Here it may clearly be seen that the clumps of protein overlap with the regions of highest attribution.  It is interesting to note that the same attribution is applied to images of healthy cells: 

![gradients]({{https://blbadger.github.io}}/neural_networks/snf7_gradxinput2.png)

Across many input images, a trained model tend to place the highest attribution on exactly these clumps accross many images ('S' denotes unhealthy and 'C' denotes healthy cells)

![gradients]({{https://blbadger.github.io}}/neural_networks/snf7_gradxinput_grid.png)

As a general rule, therefore, the deep learning models place the most importance on the same features as an expert human when attempting to classify the images of this particular set. The models effectively learn how to discriminate between classification options by determining how much of a clump of protein exists.

### Learning does not equate to global minimization of a cost function during training

The above experimental results are quite interesting because they suggest that, for deep learning models of quite different architecture, a model's ability to generalize depends not only on the model's parameters $\theta$ and the family of functions that it can approximate given its possible outputs $O_n(\theta; a)$ but also on the choice of input $a$.  Furthermore, the input $a$ is capable of inducing overfitting in a model employing extensive measures to prevent this phenomenon (L2 regularization, batch normalization) but a different input results in minimal overfitting (in the same epoch number) in a model that has none of these measures (nor any other regularizers separate from the model architecture itself).

Why is this the input so important to the behavior of the network?  Neural networks learn by adjusting the weights and biases of the neurons in order to miminize a cost function, which is a continuous and differentiable representation of the accuracy of the output of the network. This is usually done with a variant on stochastic gradient descent, which involves calculating the gradient (direction of largest change) using a randomly chosen subset of images (which is why it is 'stochastic').  This is conceptually similar to rolling a ball representing the network down a surface representing the multidimensional weights and biases of the neurons.  The height of the ball represents the cost function, so the goal is to roll it to the lowest spot during training.  

As we have seen in the case for Snf7 dataset images, this idea is accurate for increases in training accuracy but not not necessarily for general learning: reaching a global minimum for the cost function (100 % accuracy, corresponding to a cost function of less than 1E-4) led to severe overfitting and confident prediction of incorrect classifications.  This phenomenon of overfitting stemming from a global minimum in the cost function during training is not peculiar to this dataset either, as it has also been observed [elsewhere](http://proceedings.mlr.press/v38/choromanska15.pdf).  

If decreasing the neural network cost function is the goal of training, why would an ideal cost function decrease (to a global minimum) not be desirable?  In our analogy of a ball rolling down the hill, something important is left out: the landscape changes after every minibatch (more accurately, after every computation of gradient descent and change to neuronal weights and biases using backpropegation).  Thus as the ball rolls, the landscape changes, and this change depends on where the ball rolls. 

In more precise terms, for any given fixed neural network or similar deep learning approach we can fix the model architecture to include some set of parameters that we change using stochastic gradient descent to minimize some objective function.  The 'height' $l$ of our landscape is the value of the objective function, $J$.

In this idealized scenario, the objective function $J$ is evaluated on an infinite number of input examples, but practically we can only evaluate it on a finite number of training examples.  The output $O$ evaluated on set $a$ of training examples parametrized by weights and biases $\theta$ is $O(a; \theta)$ such that the loss function given true output $y$ is 

$$
l = J(O(a; \theta), y)
$$

What is important to recognize is that, in a non-idealized scenario, $l$ can take on many different values for any given model configuration $\theta$ depending on the specific training examples $a$ used to evaluate the objective function $J$.  Thus the 'landscape' of $h$ changes depending on the order and identity of inputs $a$ fed to the model during training, even for a model of fixed architecture.  This is true for both the frequentist statistical approach in which there is some single optimal value of $\theta$ that minimizes $h$ as well as the Bayesian statistical approach in which the optimal value of $\theta$ is a random variable as it is unkown whereas any estimate of $\theta$ using the data is fixed.

As $j$ is finite, an optimal value (a global minimum) of $h$ can be achieved by using stochastic gradient descent without any form of regularization, provided that the model has the capacity to 'memorize' the inputs $a$. This is called overfitting, and is strenuously avoided in practice because reaching this global minimum nearly always results in a model that performs poorly on data not seen during training.  But it is important to note that this process of overfitting is indeed identical to the process of finding a global (or asymptotically global) minimum of $l$, and therefore we can also say that regularized models tend to actually effectively avoid configurations $\theta$ that lead to absolute minima of $l$ for a given input set $a$ and objective function $J$.
## Interpretable Deep Learning

### Introduction

This page is part II in a series, for part I see [this page](https://blbadger.github.io/neural-networks3.html).

It has been claimed in the past that deep learning is somewhat of a 'black box' machine learning approach: powerful, but mysterious with respect to understanding how a model arrived at a prediction. For a clear example of a machine learning technique that is the opposite of this, let's revisit the linear regression.

$$
\hat {y} = w^Tx + b
\tag{1}
$$

In some respect, training this approach is similar to training a neural network: the weight $w$ bias $b$ vectors are adjusted in order to minimize a cost function.  Note that although we could train a linear regression model with gradient descent, it is more efficient to note that the gradient of the cost function in quadratic and therefore simply finding the point with 0 gradient algebraically allows us to minimize the cost function in one step.

Linear regression is, as the name implies, linear with respect to the parameters that we train.  This means that for the regression function $f$, all parameters $w_n \in {w}$ and $b_n \in {b}$ are additive

$$
f(w_1 + w_2) = f(w_1) + f(w_2) \\ 
$$

and scaling

$$
f(a*w_1) = a*f(w_1)
$$

which is why the model is simple to optimize (as the gradient of this model is quadratic, and thus has one extremal point that can be found via direct computation of where $\nabla_w w^Tx + b = 0$ and $\nabla_b w^Tx + b = 0$.  

But most importantly for transparency, this means that there are clear interpretations of any linear model.  Say we have a regression that has 10 variables and the model assigns a weight of 0.1 to the first variable, and 2.5 to the second. From only this information, we know that the same change in the second variable as the first will lead to a 25-times increase in the response prediction.  This follows directly from the additive (which means that we can consider each parameter separately) and scaling (such that constant terms may be taken outside a function mapping) nature of linear functions.

Say one were trying to predict how many rubber duckies were in a pool, and this prediction depended on 10 variables, the first two being how many rubber duck factories were nearby and the second how many people at the pool who like rubber ducks.  If the second variable has a larger weight than the first, it is more important for predicting the output. We can even say that for every person who brings a rubber duck, we will have 2.5 more ducks in the pool.

This contrived example is to illustrate the very natural way that linear models may be interpreted: they key idea here is that one can separate out information for each input because the model itself is additive, and furthermore that we can know what the model will predict when we change a variable because all linear models are scaling. 

Nonlinear models may be non-additive, non-scaling, or both non-additive and non-scaling.  In our above example, this could mean that we have to examine all 10 variables at once to make any prediction, and furthermore that we have to know how many rubber ducks are in the pool to make a prediction of how many will be added by changing any of the variables.

Thus nonlinear models by their nature are not suited for interpretations that assume linearity.  But we may still learn a great deal about how they arrive at the outputs they generate if we allow ourselves to explore further afield than linear interpretations.  One widely used class of method to interpret nonlinear models is called attribution, and it involves determining the most- and least-important parts of any given input.  

### Attribution via Occlusion

Given any nonlinear model that performs a function approximation from inputs to outputs, how can we determine which part of the input is most 'important'?  Importance is not really a mathematical idea, and is too vague a notion to proceed with.  Instead, we could say that more important inputs are those for which the output would be substantially different if those particular inputs were missing, and vice versa for less important inputs.  

This definition leads to a straightforward and natural method of model interpretation that we can apply to virtually any machine learning approach.  This is simply to first compute the output for some input, and then compare this output to a modified output that results from removing elements of the input one by one.  This technique is sometimes called a perturbation-style input attribution, because we are actively perturbing the input in order to observe the change in the output.  More commonly this method is known as 'occlusion', as it is similar to the process of a non-transparent object occluding (shading) light on some target.

There are a few different ways one can proceed with implementing occlusion. For a complete input map $x_c$ and a map of an input missing the first value ($x_i$), 

$$
x_c = f(1 \;	2015-02-06 \; 22:24:17 \;1845 \;3441	\;33 \;14 \;21 \;861 \;3218) \\
x_i = f( \;	2015-02-06 \; 22:24:17 \;1845 \;3441	\;33 \;14 \;21 \;861 \;3218)  
$$

recall the structured input method's approach to missing values maps

$$
x_c=[[0,1,0,0,0,0,0,0,0,0,0],[0,0,1,...]...] \\
x_i=[[0,0,0,0,0,0,0,0,0,0,1],[0,0,1,...]...]
$$

We generate an occluded input map $x_o$,

$$
x_o = f(\mathscr u\;	2015-02-06 \; 22:24:17 \;1845 \;3441	\;33 \;14 \;21 \;861 \;3218)  
$$

where $\mathscr u$ signifies an occlusion (here for the first character) in a similar way by simply zeroing out all elements of the first tensor, making the start of the occluded tensor as follows:

$$
x_c=[[0,1,0,0,0,0,0,0,0,0,0],[0,0,1,...]...] \\
x_o=[[0,0,0,0,0,0,0,0,0,0,0],[0,0,1,...]...]
$$

and then calculate the occlusion value $v$ by taking the difference between the model's mapping $m()$ of this changed input and the first:

$$
v = \vert m(x_c) - m(x_o) \vert
$$

where $x_o$ is our occluded input, as this unambiguously defines an occluded input that is, importantly, different than an empty input and also different from any normally used character input as well.  This difference is important because otherwise the occluded input would be indistinguishable from an input that had a truly empty first character, such that the network's output given this occluded input might fail to be different from the actual output if simply having an empty field or not were the most important information from that field.  Thus we could also use a special character to denote an occluded input, perhaps by enlarging the character tensor size to 12 such that the mapping is given as

$$
x_o=[[0,0,0,0,0,0,0,0,0,0,0,1],[0,0,1,...]...]
$$

Note that this method is easily applied to categorical outputs, in which case the occlusion value $v$ is

$$
v = \sum_i \vert m(x_c)_i - m(x_o)_i \vert
$$

where $i \in C$ denotes a category in the set of all possible output categories $C$.

Let's implement this occlusion for a regression output in a class `StaticInterpret` that is specifically tailored to the dataset shown earlier on this page (later there will be presented a version of this class that may be applied to any sequential input regardless of length). This class will take as arguments the neural network `model`, the set of torch.tensor objects `input_tensors` and another set of torch.tensor objects `output_tensors`.  As the input format (meaning which element corresponds to which input field) is unchanged for separate input tensors, we then assign the correct fields as `self.fields_ls` and the number of elements (characters) per field as `self.taken_ls`

```python
class StaticInterpret:

	def __init__(self, model, input_tensors, output_tensors):
		self.model = model 
		self.model.eval()
		self.output_tensors = output_tensors
		self.input_tensors = input_tensors
		self.fields_ls = ['Store Number', 
				'Market', 
				'Order Made',
				'Cost',
				'Total Deliverers', 
				'Busy Deliverers', 
				'Total Orders',
				'Estimated Transit Time',
				'Linear Estimation']
		self.embedding_dim = 15
		self.taken_ls = [4, 1, 15, 4, 4, 4, 4, 4, 4] # must match network inputs

```

Now we can define a class function that will determine the occlusion value.  This function takes a keyword argument `occlusion_size`, an integer that determines the number of sequential elements that are occluded simultaneously.  This parameter is useful as many input fields have multiple characters associated with them.  If we remove only one character at once, what happens to the output?  Perhaps nothing, if the other characters in that same field are able to provide sufficient information to the model.  At the same time, too large an occlusion leads to inaccuracies in attribution as one field's occlusion cannot be separated from anothers.  An occlusion of size 2 has experimentally been found to be generally effective for this kind of task.

```python
	def occlusion(self, input_tensor, occlusion_size=2):
		"""
		Generates a perturbation-type attribution using occlusion.

		Args:
			input_tensor: torch.Tensor
		kwargs:
			occlusion_size: int, number of sequential elements zeroed
					out at once.

		Returns:
			occlusion_arr: array[float] of scores per input index
		"""
		input_tensor = input_tensor.flatten()
		output_tensor = self.model(input_tensor)
		zeros_tensor = torch.zeros(input_tensor[:occlusion_size*self.embedding_dim].shape)
		total_index = 0
		occlusion_arr = [0 for i in range(sum(self.taken_ls))]
		i = 0
		while i in range(len(input_tensor)-(occlusion_size-1)*self.embedding_dim):
			# set all elements of a particular field to 0
			input_copy = torch.clone(input_tensor)
			input_copy[i:i + occlusion_size*self.embedding_dim] = zeros_tensor

			output_missing = self.model(input_copy)
			occlusion_val = abs(float(output_missing) - float(output_tensor))

			for j in range(occlusion_size):
				occlusion_arr[i // self.embedding_dim + j] += occlusion_val

			i += self.embedding_dim

		# max-normalize occlusions
		if max(occlusion_arr) != 0:
			correction_factor = 1 / (max(occlusion_arr))
			occlusion_arr = [i*correction_factor for i in occlusion_arr]

		return occlusion_arr
```

Prior to training, the attribution values would be expected to be more or less randomly assigned. When we observe a heatmap of the attribution value per sequence position (x-axis) given fifty different test examples (y-axis) we see that indeed our expectation is the case

![readable attribution]({{https://blbadger.github.io}}neural_networks/attributions_0000.png)

and see [the source code](https://github.com/blbadger/nnetworks/tree/master/interprets) for details on how to implement a heatmap-based visualization of model occlusion.

Using one of our controls, we may observe how a network 'learns' occlusion.  Recalling that this dataset's output is defined as

$$
y = 10d
$$

where $d$ is the **Total Deliverers** input, and observing that this input occupies the 24th through the 27th position (inclusive) in our input sequence (as there are fields of size 4, 1, 15, and 4 ahead of it and we are incrementing from 0 rather than 1), we expect the occlusiong attribution to be highest for these positions for the majority of inputs.  Note that the **Total Deliverers** field is closely related to the **Available Deliverers** and **Total Orders** fields that follow it, as an increase in one is likely to affect the other two.  In statistical terms, these fields are related via the underlying data generating distribution.

This means that although we may expect to see the **Total Deliverers** field as the most 'important' as measured using occlusion, a statistical model would be expected to also place fairly large attributions on the fields most similar to **Total Deliverers**.  And if we observe occlusion values per character during a training run (200 epochs), indeed we see that this is the case (note how positions 24-27 are usually among the brightest in the following heatmap after training).  

{% include youtube.html id='HcSUH0zTexQ' %}

### Attribution via Saliency

Occlusion is not the only way to obtain insight into which input elements are more or less important for a given output: another way we can learn this is to exploit the nature of the process of learning using a neural network or other similar deep learning models.  This particular method is applicable to any model that uses a differentiable objective function, and thus is unsuitable for most tree-based machine learning models but is happily applicable to neural networks.  Gradient-based input attribution is sometimes termed 'saliency'.

The principle behind gradient based approaches to input attribution for neural network models is as follows: given some input, we can compute the model output and also the gradient of that output with respect to the weights and biases of the model.  This gradient may then be back-propegated to the input layer of the network, as if the input were a parametric component of the neural network like the other layers. We then assess how the input contributes to the output's gradient by multiplying the input gradient by the input itself.

$$
g = \nabla_i f(x) * x
$$

where $\hat{y} = f(x)$ is the model output, $x$ is the model input, $i$ is the input layer, and $g$ is the gradientxinput.  Note that $\nabla_i f(x)$ and $x$ are both tensors, and as we want a tensor of the same size we use the Hadamard (element-wise) product, which when using `torch.Tensor` objects `t1, t2`, the Hadamard product may be obtained as `t1 * t2`.

The intuition behind this method is a little confusing for the researcher or engineer normally used to finding the gradient of an objective function with respect to a tunable parameter in a model, so before proceeding further let's note the differences between this gradient used to interpret an input and the gradient used to train a model.  

To summarize: the method by which the gradient of the objective (loss) function $J$ of the output given model a configuration $\theta$ denoted as $O(\theta)$ evaluated with respect to parameter $p$ is

$$
g = \nabla_p J(O(\theta))
$$

and we can imagine a landscape of this loss function with respect to one variable as a Cartesian graph.  The opposite of the gradient tells us the direction (and partial magnitude) with which $p$ should be changed in order to minimize $J(O(\theta))$, which can be visualized as follows

![loss gradient]({{https://blbadger.github.io}}neural_networks/loss_gradient.png)

For multiple variables, we can imagine this visualization as simply extending to multiple dimensions, with each parameter forming a basis in $\Bbb R^n$ space.  The gradient's component along the parameter's axis forms the direction (with some magnitude information as well) in which that parameter should be changed.

Now instead of evaluating the gradient with respect to the objective function, to determine how the output alone changes we can evaluate the gradient of the output $O(\theta)$ with respect to parameter $p$

$$
g = \nabla_p O(\theta)
$$

which can be visualized as

![output gradient]({{https://blbadger.github.io}}neural_networks/output1_gradient.png)

now to determine which parameter is more important for determining a given output, we can compare the gradients $g_1$ and $g_2$ which are computed with respect to $p_1$ and $p_2$, respectively

$$
g_1 = \nabla_{p_1} O(\theta) \\
g_2 = \nabla_{p_2} O(\theta)
$$

These can be visualized without resorting to multidimensional space as separate curves on a two dimensional plane (for any single value of $p_1$ and $p_2$ of interest) as follows:

![output gradient]({{https://blbadger.github.io}}neural_networks/output2_gradient.png)

Where the red arrows denote $-g_1$ and $-g_2$ as gradients point in the direction of steepest ascent.  In this particular example, $\vert g_1 \vert > \vert g_2 \vert$ as the slope of the tangent line is larger along the output curve for $p_1$ than along $p_2$ for the points of interest.  In multidimensional space, we would be comparing the gradient vector's components, where each parameter $p_n$ forms a basis vector of the space and the component is a projection of the gradient vector $g$ onto each basis.

The next step is to note that as we are most interested in finding how the output changes with respect to the *input* rather than with respect to the model's true parameters, we treat the input itself as a parameter and back-propegate the gradient all the way to the input layer (which is not normally done as the input does not contain any parameters as normally defined).  Thus in our examples, $(p_1, p_2) = (i_1, i_2)$ where $i_1$ is the first input and $i_2$ is the second.

Now that we have the gradient of the output with respect to the input, this gradient can be applied in some way to the input values themselves in order to determine the relation of the input gradient with the particular input of interest.  The classic way to accomplish this is to use Hadamard multiplication, in which each vector element of the first multiplicand $v$ is multiplied by its corresponding component in the second multiplicand $s$

$$
v = (v_1, v_2, v_3,...) \\
s = (s_1, s_2, s_3,...) \\
v * s= (v_1s_1, v_2s_2, v_3s_3, ...)
$$

and all together, we have

$$
g = \nabla_i f(x) * x
$$

This final multiplication step is perhaps the least well-motivated portion of the whole procedure.  When applied to image data in which each pixel (or in other words each input) can be expected to have a non-zero value, we would not expect to lose much information with Hadamard multiplication and furhtermore the process is fairly intuitive: brighter pixels (ie those with larger input values) that have the same gradient values as dimmer pixels are considered to be more important. 

Thus we see that what occurs during the gradientxinput calculation is somewhat similar to that for the occlusion calculation, except instead of the input being perrured in some way, here we are using the gradient to find an expectation of how the output should change if one were to perturb the input in a particular way.

The following class method implements a slightly modified version of gradientxinput.  Each 15 tensor inputs in sequence corresponds to one character, and so a natural way to accumulate the gradientxinput values is to first take the absolute values of the gradient components with respect to these inputs $\nabla_{ij}f(x)$, apply Hadamard multiplication to the corresponding input values $x_j$, and then sum up the resulting vector.

$$
g = \sum_j \vert \nabla_{ij} f(x) \vert * x_j
$$

A method implementing this version is as follows:

```python
	def gradientxinput(self, input_tensor):
		"""
		 Compute a gradientxinput attribution score

		 Args:
		 	input: torch.Tensor() object of input
		 	model: Transformer() class object, trained neural network

		 Returns:
		 	gradientxinput: arr[float] of input attributions

		"""
		# enforce the input tensor to be assigned a gradient
		input_tensor.requires_grad = True
		output = self.model.forward(input_tensor)

		# only scalars may be assigned a gradient
		output_shape = 1
		output = output.reshape(1, output_shape).sum()

		# backpropegate output gradient to input
		output.backward(retain_graph=True)

		# compute gradient x input
		final = torch.abs(input_tensor.grad) * input_tensor
		saliency_arr = []
		s = 0
		for i in range(len(final)):
			if i % self.embedding_dim == 0 and i > 0: 
				saliency_arr.append(s)
				s = 0
			s += float(final[i])
		saliency_arr.append(s)

		# max norm
		maximum = 0
		for i in range(len(saliency_arr)):
			maximum = max(saliency_arr[i], maximum)

		# prevent a divide by zero error
		if maximum != 0:
			for i in range(len(saliency_arr)):
				saliency_arr[i] /= maximum

		return saliency_arr
```

which results in

{% include youtube.html id='2V3B3tuc6mY' %}

### Combining Occlusion and Gradientxinput

One criticism of pure gradient-based approaches such as gradientxinput is that they only apply to local information.  This means that when we compute

$$
\nabla_i f(x)
$$

we only learn what happens when $i$ is changed by a very small amount $\epsilon$, which follows from the definition of a gradient.  Techniques that attempt to overcome this locality challenge include integrated gradients and other such additive measures.  But those methods have their own drawbacks (integrated gradients involve modifying the input such that the gradient-based saliency may exhibit dubious relevance to the original).  

Happily, we already have a locality-free (non-gradient) based approach to attribution that we can add to gradientxinput in order to overcome this limitation. The intuition behind why occlusion is non-local is as follows: there is no a priori reason as to why a modification to the input at any position should yield an occluded input that is nearly indistinguishable from the original, and thus any results obtained from comparing the two images using our model do not apply only to the first image. Furthermore, the `occlusion_size` parameter provides a clear hyperparameter to prevent any incedental locality, as simply increasing this parameter size is clearly sufficient to increase the distance between $x_o$ and $x_c$ in the relevant latent space.

Thus it is not entirely inaccurate to think of gradientxinput as a continuous version of occlusion, as the former tells us what happens if we were to change the input by a very small amount whereas the latter tells us what would happen if we changed it by a large discrete amount. 

We may combine occlusion with gradientxinput by simply averaging the values obtained at each position, and the result is as follows:

{% include youtube.html id='-M15BxfmFRQ' %}

One may want to be able to have a more readable attribution than is obtained using the heatmaps above,  This may be accomplished by averaging the attribution value of each character over a number of input examples, and then aggregating these values (using maximums or averages or another function) into single measurements for each field. An abbreviated implementation (lacking normalization and aggregation method choice) of this method is as follows:

```python
	def readable_interpretation(self, count, n_observed=100, method='combined', normalized=True, aggregation='average'):
		"""
		...
		"""
		attributions_arr = []

		for i in range(n_observed):
			input_tensor = self.input_tensors[i]
			if method == 'combined':
				occlusion = self.occlusion(input_tensor)
				gradxinput = self.gradientxinput(input_tensor)
				attribution = [(i+j)/2 for i, j in zip(occlusion, gradxinput)]
			...
			attributions_arr.append(attribution)

		average_arr = []
		for i in range(len(attributions_arr[0])):
			average = sum([attribute[i] for attribute in attributions_arr])
			average_arr.append(average)
		...
		if aggregation == 'average':
			final_arr = []
			index = 0
			for i, field in zip(self.taken_ls, self.fields_ls):
				sum_val = 0
				for k in range(index, index+i):
					sum_val += average_arr[k]
				final_arr.append([field, sum_val/k])
				index += i
		...
		plt.barh(self.fields_ls, final_arr, color=colors, edgecolor='black')
		...
		return
```

(note that the working version may be found in the [source code](https://github.com/blbadger/nnetworks/tree/master/interprets))

For the trained network shown above, this yields

![readable attribution]({{https://blbadger.github.io}}neural_networks/readable_1.png)

and the correct input is attributed to be the most important, so our positive control indicates success.  What about a slightly more realistic function in which multiple input fields contribute to the output? Recall that in our nonlinear control function, the output $y$ is determined by the function

$$
y = (c/100) * b
$$

where $b$ is the **Busy Deliverers** input and $c$ is the **Cost** input.  During training we can see that the expected (20-23 and 28-31 inclusive) positions are the ones that obtain the highest attribution values for most inputs:

{% include youtube.html id='ARTrheoeXEI' %}

and when the average attribution per input is calculated, we see that indeed the correct fields contain the highest attribution.

![readable attribution]({{https://blbadger.github.io}}neural_networks/readable_nonlinear.png)

Now we can apply this method to our original problem of finding which input are important for predicting a delivery time.



### Inherent limitations in neural nets

### Neural networks can approximate only a subset of all functions

An oft-quoted feature of neural networks is that they are universal, meaning that they can compute any function (see [here](https://www.sciencedirect.com/science/article/abs/pii/0893608090900056) for a proof of this).  A good geometrical explanation of this is found in [Nielsen](http://neuralnetworksanddeeplearning.com/).  Being that many everyday tasks can be thought of as some kind of function, and  as neural networks are very good at a wide range of tasks (playing chess, recognizing images, language processing etc.) it can be tempting to see them as universal panacea for any problem.  This view is mistaken, however, and to see why it is best to understand what exactly universality entails before exploring the limits of neural nets or any other similar approach.

There is an important qualification to the proof that neural networks can compute (more precisely, can arbitrarily approximate) any function: it only applies to continuous and differentiable functions.  This means that neural networks should be capable of approximating any smooth and connected function, at least in theory.  The first issue with this is whether or not a certain function is 'trainable', meaning whether or not a network is able to learn to approximate a function that is it theoretically capable of approximating.  Putting this aside, the need for differentiability and continuity brings a far larger qualification to the statement of universality.  

To see this, imagine that we were trying to use a neural network to compute an arbitrary function.  What are the chances of this function being continuous and differentiable?  Let's see how many functions belong to one of three categories: differentiable (and continuous, as differentiability implies continuity), continuous but not necessarily differentiable, or not necessarily continuous or differentiable.  Visually, differentiable functions are smooth with no sharp edges or squiggles and continuous functions may have sharp edges but must be connected.  

Formally, we can define the set of all functions $f$ that map $X$ into $Y$, $f: X \to Y \; given \; X, Y \in \Bbb R$:

$$
\{f \in (X, Y)\}
$$

with the set of all continuous functions being

$$
\{f \in \mathbf C(X, Y)\}
$$

and the set of continuous and (somewhere) differentiable functions as

$$
\{f \in \mathbf C^1(X, Y)\}
$$

The cardinality of the set of all functions is equivalent to $ \Bbb R^{\Bbb R}$ and the cardinality of the set of all continuous functions is $\Bbb R$, so

$$
\lvert \{f \in \mathbf C(X, Y)\} \rvert = \lvert \Bbb R \rvert << \lvert \Bbb R^{\Bbb R} \rvert = \lvert \{f \in (X, Y)\} \rvert
$$

A small proof for this is as follows: continuous real functions map the real line onto the rationals and as the rationals are countable, the set of all continuous real functions is countable. In symbols,

$$
\lvert \{f \in \mathbf C(X, Y)\} \rvert = \lvert \Bbb R ^ \Bbb Q \rvert = \lvert \Bbb R \rvert
$$
 
 
Whereas real functions map the real line onto any point in this line (not necessarily rational points), so
 
$$
\lvert \{f \in \mathbf (X, Y)\} \rvert = \lvert \Bbb R ^ \Bbb R \rvert
$$

Set cardinality is intuitively a rather rough measure of the size of a set: the [Cantor set](https://blbadger.github.io/fractal-geometry.html), for example, has the same cardinality as $\Bbb R$ even though the probability of choosing an element in this set for all elements over the interval $[0, 1]$ is 0, and in that sense the Cantor set has a smaller measure than the reals.

More formally, as a consequence of Baire's theorem, it can be shown that the set of somewhere-differentiable functions $\{f \in \mathbf C^1(X, Y)\}$ is of the first category (negligably small, specifically a union of countably many nowhere dense subsets) in Banach space (see Hewitt & Stromberg's Real and Abstract analysis for a proof).  Thus

$$
m \{f \in \mathbf C^1(X, Y)\}  << m \{f \in \mathbf C(X, Y)\} 
$$

where $m \mathbf C$ signifies the measure of the set of $ \mathbf C$.  Expanding the definition of measure to be set cardinality inequality or Banach space inequality, we have:

$$
m \{f \in \mathbf C^1(X, Y)\}  << m \{f \in \mathbf C(X, Y)\}  << m \{f \in (X, Y)\} 
$$

Thus size of the set of all continuous and differentiable functions is far smaller than the size of the set of all continuous functions, which is in turn far smaller than the set of all functions.  The usage of 'far smaller' does not quite do justice to the idea that each set is vanishingly tiny compared to the next.

What does this mean? If we restrict ourselves to differentiable and continuous function then neural networks are indeed universal, but they are hardly so for all possible functions because differentiable and continuous functions are a tiny subset of all functions.

### Limitations for all finite programs

Could it be that a better neural network will be made in the future, and this will allow us to compute nondifferentiable functions as well as differentiable ones?  Using more concrete terms, perhaps we will be able to engineer a neural network that is arbitrarily precise at decision problems (which is equivalent to classification) in the future.  This thinking can be used to represent the idea that although not perfect not, some sort of machine learning will be able to be arbitrarily good at decision making in the future.  If not neural networks then perhaps decision trees: can some sort of machine learning program get so good at classification that it can learn how to classify anything, to arbitrary precision?

The answer is no: no matter what program we use, we cannot solve most decision problems.  To see why, first note that any program, from a simple `print (Hello World!)` to a complicated neural network, is conveyed to a computer as a finite string of bits (a list of 1s and 0s).  Natural numbers are also defined by a finite string of bits and so we can establish an equivalence between finite programs and natural numbers.  The size of the set of all natural numbers is equivalent to the size of the set of all rational numbers (both are countably infinite).  

$$
\{\mathtt {finite} \; \mathtt {programs}\} = \{finite \; strings\; of\; bits\} \sim \Bbb N 
$$

A note on the precise symbols used here: $\sim$ indicates a set-theoretic equivalence relation, which means that there is a bijective (one-to-one and onto) mapping from one set to another, or informally that there are 'as many' elements in two sets.  The equivalence relation $\sim$ is reflexive, transitive, and symmetric just like number theoretic equality $=$, but equivalence does not usually entail arbitrary replacement.

The size of the set of all natural numbers is equivalent to the size of the set of all rational numbers (both are countably infinite), so

$$
\{\mathtt {finite} \; \mathtt {programs}\} \sim \Bbb N \sim \Bbb Q
$$

Now let's examine the set of all possible decision problems.  We can restrict ourselves to binary classification without loss of generality, where we have two options: $1$ and $0$.  Now a binary decision problem has an ouput 1 or 0 for every input, and we can list the outputs as a string of bits in binary point, ie binary digits that define a number between 0 and 1.  As a decision problem must be defined for every possible input,  and as there may be infinite inputs for any given decision problem, this string of bits is infinite and can define a number between 0 and 1.


$$
\{decision\; problems\} = \{infinite \; strings\; of\; bits\} \sim \{x \in (0, 1]\}
$$


As the size of the set of all numbers in $(0, 1]$ is equivalent to the size of the set of all real numbers, 


$$
\{decision \; problems\} \sim \Bbb R
$$

and as the size of the set of all real numbers is uncountably infinite wheras the size of the set of all rational numbers is countably infinite,

$$
\{decision \; problems\} \sim \Bbb R >> \Bbb Q \sim \{\mathtt {finite} \; \mathtt {programs}\}
$$

This means that the set of all finite programs is a vanishingly small subset of the set of all decision problems, meaning that no finite program (or collection of programs) will ever be able to solve all decision problems, only a small subset of them.

### Implications

The underlying point here is that there is a difference between strict universality in computer science and the idea of universality in functions. Universality in computer science is an idea that goes back to the Church-Turing thesis that any computable function may be evaluated by a certain machine, called a 'Universal' machine (Turing machines are a class of universal machines).  Programming languages that are capable of defining procedures for all computable functions are termed 'Turing complete', and most general purpose languages that exist today are Turing complete. 

Note that the class of computable functions extends far beyond the continuous, (piecewise) differentiable functions that have shown to be approximated by neural networks.  But with more work, one can imagine that some future machine learning technique may approximate nondifferentiable and even discontinuous computable functions as well.  Would that future machine learning program then be able to compute any function?

Not in the slightest: most functions are uncomputable.  Specifically, if a random function were chosen then the probability that it is computable is 0. For why statement is true, see [here](https://blbadger.github.io/solvable-periodicity.html).  Now consider that functions describing the natural world are by no means computable.  This means that there is no guarantee that the set of all computable functions (and therefore the set of all machine learning techniques in the future) will accurately describe the natural world in any way.  For example, take the common physical constants $g$, $h$ etc.  There is no evidence to suggest that any of these values are computable, or even rational.  Even something as simple as a physical constant therefore is dubiously evaluated by a computable function.

Combining the observation that no program can help us evaluate an uncomputable expression with a consideration of what is computable and [what is not](https://blbadger.github.io/#foundations), we can list a number of problems for which neural nets (or any machine learning procedure) are certainly incapable or not likely of assisting us:

1. Undecidable functions, such as the halting or total productivity problem: given a general recursive function (or Turing machine) with a certain number of states and inputs, determining whether the function ceases (or the machine halts), as well as what the greatest value possibly returned is, are both uncomputable and therefore unapproachable.

2. Undecidably-undecidable functions: attempting to prove the Goldbach conjecture, for example.  This class of problem is not definately unapproachable via nets, but very likely is not.

3. Aperiodic long-term dynamical system behavior, such as the trajectories of three or more heavenly bodies or future iteration locations of the Henon attractor, 

4. Natural processes that are best modelled by these aperiodic nonlinear dynamical equations are unlikely to be beneffited by nets or any other machine learning process.  This category includes problems such as weather prediction, long-term stock market prediction, and turbulent flow trajectory determination.

All in all, a number of things that many people would want to predict or understand are either certainly or very probably incapable of being benefited by neural networks or any other sophistocated machine learning procedure.  This includes fields like stock market prediction where attempts to predict using neural nets is underway.

A final note: the last section equated 'arbitrary approximation' with 'equivalence', in the sense that an arbitrary approximation of a function is the same as the function itself.  But this is not true when the function is [sensitive to initial conditions](https://blbadger.github.io/chaotic-sensitivity.html), because any error increases exponentially over time.  In concrete terms, suppose one were to have a function that was in the rare computable category, and a neural net computed a close approximation of it.  If that function described a non-linear change over time and the goal is long-term prediction, the network is not useful.

### Neural networks as systems of dimension reduction: implications for adversarial examples

This section will explore an interesting consequence of precision in neural networks: adversarial examples, which are inputs that are nearly indistinguisheable from other inputs, the latter of which are accurately classified but the former are just as confidently mis-classified.

All supervised parametric statistical learning procedures, from linear regressions to neural networks, are subject to a bias-variance tradeoff.  Test dataset classification errors are the result of irreducible error plus variance (sensitivity of the model to small changes in input) plus bias (error resulting from limitations in the chosen model) squared. Models that very restrictive assumptions (such as a linear regression) have high bias because such assumptions are rarely true even if they are convenient, whereas very flexible models like random forests or neural networks have lower bias as they make fewer restrictive assumptions but often exhibit higher variance, such that small changes to an input lead to large changes in output, whether that is a regression or else a classification as it is here.  [Adversarial examples](https://arxiv.org/abs/1312.6199) can be viewed as examples of the effects of using models with high variance.  Is this necessarily the case, or in other words can someone design a procedure that is of low bias but also free from effects of high variance?

Neural networks, like any statistical learning procedure, are in the business of dimensional reduction.  This is because they take in inputs that are necessarily larger than outputs, which may seem counterintuitive if the inputs are small images and the outputs are choices between thousands of options.  Even then, dimensional reduction holds: to see this, suppose that each image were classified into its own category.  Then the network would not reduce dimension but the classification would be trivial: any program could do just as well by classifying any image to its own category.  In the process of assigning multiple inputs to the same category, dimensional reduction occurs.  To be specific, the many-dimensional training space is usually reduced to a one-dimensional cost function, and this is then used to change the network in order to decrease this cost function. This reduction is equivalent to a mapping, where one point in many dimensions is mapped to one corresponding point in one dimension.

As seen for the nonlinear attractors [such as the one here](\clifford_attractor.md), changes in dimension are not: small changes in inputs lead to large changes in attractor shape. Is a change in dimension always discontinuous?  We are most interested here in a change from many dimensions to one, so start by considering the change in dimension from two to one.

The new question is as follows: can a mapping from a two dimensional surface to a one dimensional line be continuous?  It turns out no: any mapping from two to one dimensions (with the mapping being one-to-one and onto) is discontinuous, to be specific it is everywhere discontinuous.  Here 'continuous' as a property of functions is defined topologically as follows: in some metric space $(X, d)$ where $f$ maps to another metric space $(Y, d')$, the function $f$ is continuous if and only if for any $\epsilon > 0$,

$$
\lvert b - a \rvert < \delta \implies \lvert f(b) - f(a) \rvert < \epsilon
$$

Where $\delta > 0$ is a distance in metric space $(X, d)$ and $\epsilon$ is a distance in metric space $(Y, d')$.  A discontinuous function is one where the above expression is not true for some pair $(a, b) \in X$ and an everywhere discontinous function is one in which the above expression is not true for every pair $(a, b) \in X$.

The statement is that any one-to-one and onto mapping from two dimensions to one is everywhere discontinuous. To show this we will make use of an elegant proof found in Pierce's Introduction to Information Theory (p16-17).  

Suppose we have arbitrary two points on a two dimensional surface, called $a$ and $b$.  We can connect these points with an arbitrary curve, and now we choose two other points $c$ and $d$ on the surface and connect them with a curve that travels through the curve $ab$ as follows. All four points are mapped to a line, and in particular $a \to a'$, $b\to b'$ etc.

![discontinous proof]({{https://blbadger.github.io}}/neural_networks/discontinous_proof.png)

Now consider the intersection of $ab$ and $cd$.  This intersection lies between $a'$ and $b'$ because it is on $ab$.  But now note that all other points on $cd$ must lie outside $a'b'$ in order for this to be a one-to-one mapping.  Thus there is some number $\delta > 0$ that exists separating the intersection point from the rest of the mapping of $cd$, and therefore the mapping is not continuous.  To see that it is everywhere discontinous, observe that any point on the plane may be this intersection point, which maps to a discontinous region of the line.  Therefore a one-to-one and onto mapping of a two dimensional plane to a one dimensional line is nowhere continuous $\square$.

This theorem also extends to the mappings of more than two dimensions to a line.  If one reverses the direction of $f$ such that it maps a line to a plane (while keeping the one-to-one and onto stipulations), the theorem can also be extended to show that no bijective, continuous function can map a 1-dimensional line to a 2-dimensional surface.  Plane-filling curves such as Peano or Hilbert curves (see [here](https://blbadger.github.io/fractal-geometry.html) for more) are [not continuous](https://blbadger.github.io/most-discontinuous.html) for a pre-image of finite size, and for either finite or infinite starting line lengths these curves are not one-to-one and are therefore not bijective.

Now consider the existence of adversarial examples, also called adversarial negatives, images that are by eye indistinguishable from each other but are seen by a network to be completely different.  For some explanation of how these are generated and examples on images, see [this article](https://blbadger.github.io/neural-networks.html#adversarial-examples). The authors of [this study](https://arxiv.org/abs/1312.6199) suggest that the existence of these images suggests that the input-output mapping for a neural network is 'fairly discontinuous', and it is clear to see why: if two nearly-identical images are classified as very different, then two nearly-identical points in multidimensional input space end up being far from each other in output (and for a classification problem, therefore the predicted class).  

Neural networks map many-dimensional space to one dimension, and as the proof above demonstrates this mapping must be discontinuous everywhere if the mapping is one-to-one, meaning that each different image has a different cost function associated with it.  This means that every image classified by such a neural network will have an adversarial example, an image that is extremely close to one correctly classified but that will be incorrectly and confidently mis-classified.


Can we avoid discontinuous mapping when moving from two (or more) to one dimensions? Consider the following function 

$$
f:\Bbb R^2 \to \Bbb R
$$

where

$$
f(x_1, x_2) = x_1 + x_2
$$

This mapping is continuous: arbitrarily small changes in the metric space $(\Bbb R^2, d)$ result in arbitrarily small changes in the corresponding slace $(\Bbb R, d')$, and a sketch of a proof for this will be made apparent shortly.

How is this possible, given that one-to-one functions cannot map a surface to a line continuously?  The above function is not one-to-one, instead an infinite number of starting points map to each point on $R$.  To see why this is, consider which values of $a, b \; \lvert \; a \neq b$ are equal when mapped by $f$.  Here $x_1 + x_2$ means adding coordinate values of the cartesian plane (ie $x$ value $+$ $y$ value).  Which unique points on the plane would map to the same point on the real line using this function?


Consider $a = (0, 1)$ and $b = (1, 0)$.  $f(a) = 1 = f(b)$, and indeed every point on the line $ab$ maps to the same value in $\Bbb R$, that is, $1$.  Thus this function divides up $\Bbb R^2$ into diagonal lines, each line mapping to one point in $\Bbb R$.  Now it should be easy to see why this function is continuous: it simply maps all points in $\Bbb R^2$ to the nearest position on the line $y = x$. 

![continuous proof]({{https://blbadger.github.io}}/neural_networks/continuous_map.png)

An arbitrarily small change perpendicular to this line in two dimensions yields no change in output in one dimension, and an arbitrarily small change applied along this line in two dimensions yields an arbitrarily small change in one dimension.

In sum, one-to-one (and onto) functions from two dimensions to one map discontinuously where as functions that are not one-to-one may map two dimensions to one continously.  What does this mean for neural networks?

If we judge neural networks by their ability to classify an arbitrary number of input images into a finite number of outputs, it is clear that the neural network cannot act as a one-to-one function. But the process of training a network is as yet poorly achieved by using percent accuracy for a cost function, so the relevant output for the network is the (more or less) continuous cost function.  With respect to this output, neural networks map each individual image to a specific value on the cost function and therefore act as a one-to-one mapping function during training.  As training is what determines the final network function, a network trained using a continuous cost function acts as a one-to-one mapping between many dimensions and one.

In summary, neural networks that use a continuous cost function map (more than) two dimensions to one in a one-to-one mannar, and thus the mapping itself must be discontinuous (which results in adversarial negatives).  Mappings that occur via discrete category selection are not one-to-one and therefore may be continuous, but such mappings are insufficient for training a network to be very precise.  Thus it seems that the ability of a neural network (or any dimensional reduction machine learning technique) to learn to categorize precisely is inevitably connected with the appearance of adversarial examples, meaning that a precise network will have discontinuities in mapping regardless of how exactly the network is set up.

The implications of this are as follows: given any input and any neural net mapping inputs to a cost function in an (approximate) one-to-one manner, it is possible to find an adversarial example.  To see why no input or network is safe, consider that the points $a$, $b$, $c$, and $d$ and the mapping funciton $f$ were chosen arbitrarily.  Thus the finding that adversarial examples are [inherent features of neural nets](https://arxiv.org/abs/1905.02175) may be extended to any machine learning procedure that employs a near one-to-one mapping from input space to a cost function.

### Why are adversarial examples relatively uncommon?

If adversarial examples follow from the fact that any mapping from $\Bbb R^n \to \Bbb R$ is everywhere discontinuous, why are adversarial examples not found everywhere?  It is easy to see that adversarial examples must be rather rare, for otherwise training any network would be impossible.  How can we reconcile the result that adversarial examples should occur wherever there is a discontinuity in mapping from $\Bbb R^n \to \Bbb R$ with the finding that in practice such examples are not common?

Machine learning methods relying on dimensional reduction (like deep learning via stochastic gradient descent) generally assume the the manifold hypothesis, which states that the probability distribution of relevant inputs (sound from speech, images of the natural world etc) are concentrated around low-dimensional manifolds in higher-dimensional space.  For example, the following data points in two dimensional space are concentrated around a one-dimensional manifold denoted in blue:

![manifold](/neural_networks/manifold.png)

The manifold hypothesis requires two things to be true: first that inputs occupy very little (negligably small) of all possible input space, and second that all points sufficiently close together map to the same area on the lower dimensional manifold, not vastly separate regions.

It is clear that representations of relevant inputs are indeed only a small subset of all possible inputs: for example, most possible 20x20 images resemble noise rather than everyday objects.  But while there is some experimental evidence for the second idea, the presence of adversarial negatives provides evidence against this idea as well leaving the overall accuracy of this statement ambiguous.  

But then how are neural nets able to classify many inputs so effectively if we cannot assume the manifold hypothesis and furthermore if any one-to-one mapping from many dimensions to one is not continuous?  Adversarial examples are generally of extremely low probability: if one simply adds noise with the same norm to an input before feeding that transformed input to a neural networks, a well-trained model typically does not exhibit large changes in output class estimation in nearly all cases.  In other words, adversarial examples may be considered as exhibiting very low probability.

Another answer is that in practice one uses representations of multidimensional inputs and these representations are mappable bijectively with a one-dimensional encoded string (or integer, if one wants to think in those terms).  In effect, no one has ever trained a network on a true multidimensional input because all current computational devices represent all inputs as one dimensional sequences of bytes.  For example, the following 2-dimensional array could represent the information `abc` using one-hot vectors:

```python
[[1, 0, 0, 0, ... 0],
 [0, 1, 0, 0, ... 0],
 [0, 0, 1, 0, ... 0]]
```

But this 2-dimensional array is only an abstraction for the programmer: the information is actually stored in memory by a 1-dimensional sequence of bits.  This is accomplished because the following 1D array could be thought of as being equivalent to the 2D array above:

```python
[1, 0, 0, 0, ... 0, 0, 1, 0, 0, ... 0, 0, 0, 1, 0, ... 0]
```

The same holds true of two-dimensional arrays corresponding to images of physical objects, of three-dimensional arrays representing time-series data, etc.  And most importantly for this discussion, every input ever given to an artifical neural network is only a representation of a multidimensional input, an abstraction that is useful for thought but one that does not reflect the underlying 1D structure in memory. This means that every multidimensional object has already been bijectively mapped to a 1D object in the process of encoding, and therefore is not a true multidimensional object at all.

But then why do adversarial examples exist at all if neural nets simply map 1D arrays to other 1D arrays, which does not necessitate any discontinuity?  This can be thought of as a result of increased resolution.  As networks get more sophisticated and inputs take up more memory, the respresentations of both grow in size to become prodigiously large.  

As input size increases, the input becomes more and more similar to a truly multidimensional object with respect to an output of fixed size (usually `float` or `double`), and the existance of adversarial examples becomes more common.  This is why adversarial examples are relatively rare for small neural networks trained on low-information images (and why they have been found relatively recently in the history of machine learning research).  

















## Nonlinear tranformations as routes between dimensions

Here 'nonlinear transformations' is taken to mean the set of all equations that are nonlinear (ie any non limited to unary exponents) or piecewise linear.

### Transformations from finite to infinite size

In set-theoretic geometry, there are as many points in a line of finite length than there are in the entire real number line, which is of infinite length.  This means that there can be some function $f$ that maps each point of a finite line segment to each point on the number line, or in other words that there is some function $f$ that maps a finite length onto an infinite one bijectively.  

Therefore an object in one dimension of finite length may be mapped to the entire real line with the right transformation.  Which transformations can accomplish this?  Transformations may be defined as lines or curves or points that map the finite line segment (specifically an open subset of $\Bbb R$, for example the points of $x \in (0, 1)$ into the real line $\Bbb R$ as follows:

![mapping]({{https://blbadger.github.io}}misc_images/function_mapping.png)

Only a curve (or line segments arranged to approximate a curve if corners are ignored) can map the segment to the entire real line. 

![mapping]({{https://blbadger.github.io}}misc_images/curve_mapping.png)

This can be seen most clearly by observing what happens when the starting line approaches a linear map, ie a line.  More and more of the reals may be mapped but the distance is always finite until the mapping (the transforation) is on top of the line segment (the pre-image).  
 
![mapping]({{https://blbadger.github.io}}misc_images/linear_mapping.png)

When the transformation lies on top of the pre-image, it may accurately be said that the transformation is the pre-image.  But this transformation is no longer a function at all, as it is a one-to-many transformation because any point on the pre-image line segment is in the same place as a point on the transformation, meaning that the image point may exist anywhere.  

This is a graphical illustration of a function in which the input is the same as the function itself.  If the Church-Turing thesis is accepted, these functions can be equivalently stated as Turing machines $T$ in which the program $a$ is the same as the input $i$.  It is provable that such machines never halt: the predicate 'does the turing machine $T$ with input $i$ and program $a$ where $i=a$ halt at value $x$ for some value $x$', in symbols $\exists x T(a, a, x)$, is undecidable.  If such a machine halted at any $x$, the predicate would be decidable but as it is not, a Turing machine with its program as the input never halts.

All this is to say that the transformation of a line on top of another line may map a segment to the entire real line but that this is no longer a function, nor is it computable.  Therefore, only nonlinear functions are capable of mapping a finite segment onto the entire real line.

### Increases in topological dimension require nonlinear transformations

Topological dimension and scaling (fractal) dimension are equivalent for the classical objects in which topological dimension is defined: a point is 0-dimensional, a line 1-dimensional, a plane surface two-dimensional etc.  A mapping from one topological dimension to another that is larger requires a transformation to infinite size.  A proof for this for the case of a 1-D line transformed into a 2-D plane is as follows: first suppose that a finite line could be arranged to cover a plane.  As the line is of finite size, an increase in scale will eventually lead to gaps between portions of this line: if this were not the case, 

As seen in the previous section, only nonlinear transformations map finite sizes to infinite.  Therefore transformations from one topological dimension to a larger one must be nonlinear.

### Curve dimension

Here dimension is not fractal or topological, but a definition based on the transformations that can result from a function.  Take an arbitrary line existing on a plane.  In some orientation, this line may be defined entirely on the basis of another line that exists alone, ie in one dimension.  

Now note that this is not the case for an arbitrary curve on a plane: this curve by definition is not congruent to any of the lines mentioned above. The curve may be mapped to a line, but it is not equal in that it cannot be substituted for any line.  Therefore a curve cannot be defined in one dimension, because otherwise it would be congruent with a line.

This idea extends to curved surfaces in three dimensions.  Such surfaces are not entirely planar but exist in three dimensions, and therefore are not congruent with a topologically two-dimensional plane. This goes to suggest that curves of any kind exist between topological dimensions, which provides a clear reason why nonlinear transformations are capable of providing routes between topological or fractal dimensions.

### Dimension-changing maps are uncomputable

Space-filling Hilbert or Peano curves require an infinite number of recursions.  Infinitely recursive functions are undecidable, and therefore space-filling curves are undecidable.  

Dimension-changing maps require an infinite number of recursions as well, and therefore dimension-changing maps are undecidable. For more on decidability, see [here](/solvable-periodicity.md).

### Why nonlinear transformations are not additive, from a set-theoretic perspective

Non-additivity is a defining feature of nonlinearity.  But what does it mean that nonlinear transformations are not additive, which in symbols is

$$
f(a+b) \neq f(a) + f(b)
$$

in the sense of how is this possible?  This means that the transformation specified by $f$ must be different at different locations, such that the location $a+b$ is a different transformation than that which is found at $a$ or $b$ and that simply performing the transformation at $a$ then at $b$ is not sufficient to predict the transformation at $a+b$.  In a sense, the transformations are unique for every element of the pre-image.

But this remains a somewhat counter-intuitive idea. We can define the set of all possible transformations for any given $f$ as $\mathscr S$.  As all finite quantities can be added, elements $a$ and $b$ of $\mathscr S$ are not finite.  An infinite set is equivalent to one of its proper subsets, meaning that inside elements $a$ and $b$ there are more of $a$ and $b$.  

Now recall that most nonlinear transformations are aperiodic and that bounded, aperiodic maps form self-similar fractals. The observations that $a \in a$ (proper subset) gives a clear reason why such objects exist: each fractal image $i$ contains a smaller version of itself, meaning that $i \in i$ (proper subset, as the smaller version does not contain all points of the larger). 

Here is another argument arriving at the same concusion: Sets cannot be partitioned arbitrarily.  To see that this statement is true, consider what happens if we try to partition a set $A$ into partition sets $S_1, S_2, S_3 ...$ where members are larger than the other members of the same partition.  It is not clear which elements belong with which others, or even if an element belongs in its own set.

Somewhat surprisingly, the only way to partition a set is via an equivalence class: each element in a given partition $e_1 \in P$ must be somehow equivalent to each other element $e_2, e_3, ... \in P$.  For example, suppose that we want to partition the set of natural numbers $\Bbb N$ based on size: numbers larger than 5 in one partition, numbers smaller or equal to 5 in another.  The equivalence of each partition is its relation to the number 5: either all larger or all smaller (or the same size).  Imagine if partitioning did not necessarily involve equivalence.  Then trying to partition a set into elements that are larger than each other would be possible, but it is unclear which elements belong in any given subset, or even if an element belongs in any subset, a contradiction.

Now consider a set $S$ where some elements are incomparable.  There is no chance of partitioning $S$ because one cannot determine which elements of $S$ are greater or less than others, and thus we cannot be sure of which elements are equivalently larger or smaller (or equivalent in any way).  If a set cannot be partitioned, then additivity cannot be valid from the definition given above.  Therefore any set with incomparable elements is not additive.  

Take collections of points transformed by a nonlinear function.  Are such collections comparable to other collections?  Comparability requires that for two elements $a, b$ either $a \geq b$ or $b \geq a$.  Can we choose between statements if there exists an infinite number of elements $a$ in element $b$, while at the same time there are infinitely many $b$ in $a$?  No, and therefore self-similar nonlinear maps contain collections of points that are incomparable by definition.  As certain subsets are incomparable, the transformation itself is non-additive.

### A concrete example of non-additivity

The underlying point here is that one arrives at problems when attempting to add infinite quantities, or in other words that infinite quantities are non-additive.  As we saw earlier, nonlinear transformations have the capacity to map finite regions onto infinite, and indeed any fraction of the whole finite region may be mapped onto an infinite region.  Because of the introduction of infinite quantities, nonlinear transformations are not additive. 

To show how a infinite quantities lead to the inability of additivity, consider the following problem: a hybrid car (perhaps a prius) is driving along a road that is relatively flat.  The car dislays the gas milage at each point along the trip, and we assume that this display is accurate.  In this particular trip, the gas engine runs continually such that gas is continually consumed.  How does one calculate the overall fuel economy, or distance travelled per unit of fuel consumed for the trip?  In the absence of any other information, one could simply record the fuel economy at each second of the trip and take the average of these numbers to gain a pretty good estimation. 

But now suppose that the road has a hill such that the car uses no fuel for a certain amount of time (perhaps it shuts off the engine and coasts or else uses the electric motor).  In this case, the fuel economy for this section is infinite: there is a positive distance travelled $d$ but no fuel consumed $c$, so $f_e = d/c = d/0 \to \infty \; as \; c\to 0$.  If we then use the method above for determining fuel economy for the whole trip, we find that there is infinite economy.  But this is not so: a finite amount of fuel was consumed during the trip in total, which proceeded a finite distance, meaning that the true economy must be finite.  

The problem here is the introduction of the infinite quantity. The system is no longer additive once this occurs, and therefore one cannot use fuel economy at each time point and add them together for an accurate measure of total economy.  

### Infinite sets are self-similar

Nonlinear maps often yield self-similar fractals if bounded.  Here is a reason as to why this is: if we accept that nonlinear transformations are generally decribed by infinite sets, and that $A \in A$ (proper subset) is true for any infinite set, it necessarily follows that some subset of a map of a nonlinear transformation is equal to the entire set.  

From this perspective, self-similarity is a consequence to any non-linear transformation.  Particularly interesting is non-trivial self-similarity, which may be defined as scale invariance of a non-differentiable object (which can be discontinuous, like a Cantor set, or continuous with a non-differentiable border like a Julia set). Another definition for non-trivial self-similarity is that this is a scale invariance leading to an arbitrary amount of information at an arbitrarily small scale.

Self-similarity in either case can be thought of as a form of symmetry: it is the symmetry of scale.  Unfortunately this symmetry is not necessarily helpful for computation.

### Infinite recursion in self-similar transformations

The images of fractals on this site are examples of recursion without a base case (sometime called basis case).  In the natural numbers with addition, 1 is the base case for which succession acts to make all other numbers.  A recursive base case is the smallest indivisible unit. 

For nonlinear transformations that yield fractal objects, there is no recursive base case because there is always a smaller part of the image at any scale that is equivalent to the entire image.  For this reason, none of the fractal maps presented elsewhere on these pages are entirely accurate, because they must contain a base case in order to be computed.  

### Nonlinearity and problem solving

Nonlinear systems are not additive (see above), meaning that they cannot be broken down into parts and re-assembled again.  As we have seen for fractals, attempting to isolate a small portion of a nonlinear map may simply lead to the presence of the entire map in miniature.  

Now this is a serious challenge to any attempt to understand nonlinear systems because so much of human thought (or at least current problem solving thought) operates by the principle of additivity.  For example, if one builds a house then the a foundation is made, the walls go up, and the roof is set.  Or if a computer program is written, each individual variable is defined and operations are enumerated.  The same is true of many disciplines of knowledge: smaller pieces are added together to make the final outcome.  

In the models of nonlinear equations on this webpage, computer programs were used to represent the behavior of various nonlinear transformations.  These programs work by dividing each problem into smaller versions of the same problem until a finite case is reached.  For example, a program that observed the behavior of a particle over time first divides the time period into maybe 500000 individual time steps and then proceeds to calculate the behavior as if each time step were itself indivisible.  Or perhaps we wish to understand which points in the complex plane diverge to infinity and which do not: in this case a certain resolution for the plane, perhaps 1400x2000, is chosen and then the maximum number of iterations is specified such that the program eventually halts. 

In either case, the program generates (albeit high-resolution) approximations of the actual object.  These objects are necessarily approximations, as any machine that attempts to produce an exact self-similar object (Julia set or anything similar) requires recursion without a base case, and this program will never halt.  This is why Turing's construct of a machine $T(a, a, x)$ never halts, ie the search for a solution to the problem 'Does a Turing machine with the input $a$ as the program it runs on halt at time x?' is undecidable inside the system because there is no recursive base case.

### Sensitivity to initial values as implied by self-similarity

Self-similarity here is defined to mean an invariance with respect to scale. 

Aperiodic dynamical systems are typically self-similar if bounded.  From the definition above, this means that a small potion of the output of the dynamical system (for many inputs) resembles a larger portion of the output in some way or another.  

Aperiodicity is implied by sensitivity to initial values, meaning that two points arbitrarily close together (but not located in the same place) in a starting region (phase space or any other area) will eventually diverge such that they are arbitrarily far apart, at least within the confines of the bounded region of the function image.  

Self-similarity is usually applied to the dyamical system output, but sensitivity to initial conditions can be understood as self-similarity of input distance with respect to output distance. In other words, the distance between two points $p_o, q_o$ in the image space is invariant with respect to the scale of the distance bewteen initial points $p_i, q_i$.  This implies a self-similarity in the system input (ie function pre-image) with respect to the outputs.  Thus not only is the output self-similar with respect to itself, it is also self-similar with respect to the input with regards to distances between initial points $p_i, q_i$ and final points $p_o, q_o$.  











## Pendulum map

Imagine a pendulum swinging back and forth. We can plot the position of its tip on the x-axis and the velocity of the tip on the y-axis.  This xy plane is now called a phase space, and although it does not correspond to physical space it does tell us interesting information about the system it represents.  An excellent summary of modeling differential equations by 3B1B may be found [here](https://www.youtube.com/watch?v=p_di4Zn4wz4). 

By setting up a pendulum to obey Newton's laws, we can model how the pendulum will swing using Euler's formula to model the trajectory through phase space of the differential equations governing pendulum motion as it is slowed by friction:

$$
\cfrac{dx}{dt} = y \\
\cfrac{dy}{dt} = -ay - b \cdot \sin(x) 
\tag{1} $$

Where the constant $a$ denotes friction and the constant $b$ represents the constant of gravity divided by the length of the pendulum.  This system of equations is nonlinear (due to the sine term) and dissipative (from the friction, $-ay$) which means that it takes a 2D volume of starting points down to a 0 area.  

It is helpful to view the vector plot for this differential system to get an idea of where a point moves at any given (x,y) coordinate:

![pendulum vectors]({{https://blbadger.github.io}}pendulum_map/pendulum_vectors.png)

Imagine a ball rolling around on a plane that is directed by the vectors above. We can calculate this rolling using Euler's formula (see [here](https://blbadger.github.io/clifford-attractor.html)) the change in time step $\Delta t$ is small (0.01 in this case), the following map is produced:

![pendulum image]({{https://blbadger.github.io}}pendulum_map/continuous_pendulum.png)

Now note that we can achieve a similar map with a linear dissipative differential system

$$
\cfrac{dx}{dt} = -ay \\
\cfrac{dy}{dt} = -by + x \tag{2}
$$

which at $\Delta t = 0.1 $ yields

![swirl image]({{https://blbadger.github.io}}pendulum_map/linear_swirl.png)

In either case, the trajectory heads asymptotically towards the origin.  This is also true for any initial point in the vicinity of the origin, making the point (0,0) an **attractor** of the system.  As the attractor is a point, it is a 0-dimensional attractor or point attractor.


### Increasing timestep size leads to an increase in attractor dimension

Now let's increase $\Delta t$ little by little.  At $\Delta t = 0.02$ the map looks similar to the one above just with more space betwen each point on the spiral.  This makes sense, as an increase in timestep size would lead to more motion between iterations provided a particle is in motion.

![pendulum image]({{https://blbadger.github.io}}pendulum_map/pendulum_0.2t.png)


Increasing $\Delta t$ to 0.037 leads to the appearance of ripples in the trajectory path, where the ratio between the distance between consecutive iteration (x, y) coordinates compared to the (x, y) coordinates of the next nearest neighbor changes depending on where in the trajectory the particle is.  For lack of a better word, let's call these waves.  Another way to think about the waves is to see that they are locations of apparent changes in spiral direction.

![pendulum image]({{https://blbadger.github.io}}pendulum_map/pendulum_0.37t.png)


With a slightly larger $\Delta t$ (0.04088), the waves have become more pronounced and an empty space appears around the origin (picture is zoomed slightly).

![pendulum image]({{https://blbadger.github.io}}pendulum_map/pendulum_0.04088t.png)


And by $\Delta t = 0.045$, the attractor is now a ring

![pendulum image]({{https://blbadger.github.io}}pendulum_map/pendulum_0.045t.png)


Thus an increase in $\Delta t$ leads to the transformation of the pendulum map from a 0-dimensional attractor to a 1-dimensional one. Further increases in $\Delta t$ leads to explosion towards infinity.

Here is a video of the transition from $\Delta t = 0.03 \to \Delta t = 0.045$:

![pendulum gif]({{https://blbadger.github.io}}pendulum_map/pendulum1.gif)

What happens to the linear system (2) when $\Delta t$ increases? At $\Delta t = 0.5$, the points along the spiral are slightly more spaced out

![spiral image]({{https://blbadger.github.io}}pendulum_map/spiral_map_0.5t.png)


When $\Delta t = 0.9$, there is less space between (x, y) coordinates of different rotations than of consecutive iterations:

![spiral image]({{https://blbadger.github.io}}pendulum_map/spiral_map_0.9t.png)


And when $\Delta t = 0.9999$, this effect is so pronounced that there appears to be a ring attractor,

![spiral image]({{https://blbadger.github.io}}pendulum_map/spiral_map_0.9999t.png)


But this is not so!  Closer inspection of this ring reveals that there is no change in point density between the starting and ending ring: instead, meaning that the points are still moving towards the origin at a constant rate.

![spiral image]({{https://blbadger.github.io}}pendulum_map/spiral_map_zoom.png)

Only at $\Delta t = 1$ is there a 1-dimensional attractor, but this is unstable: at small values less than or greater than 1, iterations head towards the origin or else towards infinity. The linear system yields a 1-dimensional ring map only when the starting coordinate is already on the ring, and thus it is incapable of forming a 1-dimensional attractor (ie a stable set) as was the case for the nonlinear system.

From $\Delta t = 0.9 \to \Delta t = 0.999$, 

![spiral image]({{https://blbadger.github.io}}pendulum_map/swirl1.gif)


###  Eventually periodic pendulum map 

Take $\Delta t$ to be 0.04087.  Now let's zoom in on the upper part of the map:

![pendulum image]({{https://blbadger.github.io}}pendulum_map/pendulum_0.0487t_zoom1.png)
![pendulum image]({{https://blbadger.github.io}}pendulum_map/pendulum_0.0487t_zoom2.png)
![pendulum image]({{https://blbadger.github.io}}pendulum_map/pendulum_0.0487t_zoom3.png)
![pendulum image]({{https://blbadger.github.io}}pendulum_map/pendulum_0.0487t_zoom4.png)

Notice that more and more waves are visible as the scale decreases. At a small spatial scale, many waves are seen over a very small in $\Delta t=0.040865 \to \Delta t=0.040877$:
![pendulum image]({{https://blbadger.github.io}}pendulum_map/pendulum_zoom.gif)

Waves are not observed for the linear map at any $\Delta t$ size (here at 0.9999):
![pendulum image]({{https://blbadger.github.io}}pendulum_map/swirl_map_zoom.png)

The collection of iterations in a ring suggests that the nonlinear pendulum system is eventually periodic: the attractor is a 1-dimensional circle in phase space for the parameters chosen above. Because the system is eventually periodic, it should not be sensitive to initial values as only aperiodic trajectories are sensitive to initial values (disregarding round-off error and approximation issues present in real-world computations).  This can be checked for two values shifted by an  $0.00000001$ along the x-axis as follows:

```python
#! python3

# import third-party libraries
import numpy as np 
import matplotlib.pyplot as plt 
plt.style.use('dark_background')

def pendulum_phase_map(x, y, a=0.2, b=4.9):
	dx = y
	dy = -a*y - b*np.sin(x)
	return dx, dy

# parameters
steps = 1000000
delta_t = 0.043

# initialization
X = np.zeros(steps + 1)
Y = np.zeros(steps + 1)
X1 = np.zeros(steps + 1)
Y1 = np.zeros(steps + 1)

X[0], Y[0] = 0.00000001, 1
X1[0], Y1[0] = 0, 1

# differential equation model
for i in range(steps):
	dx, dy = pendulum_phase_map(X[i], Y[i])
	X[i+1] = X[i] + dx * delta_t
	Y[i+1] = Y[i] + dy * delta_t

for i in range(steps):
	dx, dy = pendulum_phase_map(X1[i], Y1[i])
	X1[i+1] = X1[i] + dx * delta_t
	Y1[i+1] = Y1[i] + dy * delta_t

print ('p1 = ' + '(' + str(X[-1]) + ',' + str(Y[-1]) + ')')
print ('p2 = ' + '(' + str(X1[-1]) + ',' + str(Y1[-1]) + ')')

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
p1 = (-0.6195501560736936,-0.3176710683722944)
p2 = (-0.6195501540914985,-0.3176710834485138)
```
The final values are very nearly identical.  Indeed, when the final cartesian distance between the shifted points shrinks to 0 as the initial distance does:

```python
...
	for i in range(steps):
		dx, dy = pendulum_phase_map(X1[i], Y1[i])
		X1[i+1] = X1[i] + dx * delta_t
		Y1[i+1] = Y1[i] + dy * delta_t

	initial_distance.append(float('0.' + j*'0' + '1'))
	final_distance.append(((X[-1] - X1[-1])**2 + (Y[-1] - Y1[-1])**2)**0.5)
  
 for i in range(len(initial_distance)):
	print ('initial = {}'.format(initial_distance[i]) + '    ' + 'final = {:.3e}'.format(final_distance[i]))
	
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
(output)
initial = 0.1    final = 1.957e-01
initial = 0.01    final = 1.161e-02
initial = 0.001    final = 1.485e-03
initial = 0.0001    final = 1.517e-04
initial = 1e-05    final = 1.520e-05
initial = 1e-06    final = 1.521e-06
initial = 1e-07    final = 1.521e-07
initial = 1e-08    final = 1.521e-08
initial = 1e-09    final = 1.522e-09
initial = 1e-10    final = 1.528e-10
```
and to plot these points on a log/log scale, 

```python
fig, ax = plt.subplots()
ax.plot(initial_distance, final_distance)
ax.set(xlabel='Initial distance', ylabel='Final distance')
ax.set_yscale('log')
ax.set_xscale('log')
plt.show()
```

![pendulum insensitive]({{https://blbadger.github.io}}/misc_images/pendulum_sensitivity.png)

Thus the pendulum map is not sensitive to initial conditions for these values, implying periodicity (which we have already seen in the phase space diagrams above).

In contrast, the semicontinuous [Clifford map](https://blbadger.github.io/clifford-attractor.html) for $a = -1.4, \; b = 1.7, \; c = 1.0, \; d = 0.7$ and $ \Delta t = 1.35$ (iterated for 500000 steps) is extremely sensitive to changes in initial values: going from $(x_0, y_0) = (10.75, 8.2)$ to $(x_0, y_0)= (10.750000001, 8.2)$ results in a significant change in final point location

```python
p1 = (10.98448, 7.96167)
p2 = (11.03945, 8.26257)
```

If we shrink the distance between initial points to 0, the distance between final points does not decrease.

```python
initial = 0.1    final = 3.775e-01
initial = 0.01    final = 3.060e-01
initial = 0.001    final = 3.097e-01
initial = 0.0001    final = 3.042e-01
initial = 1e-05    final = 4.195e-01
initial = 1e-06    final = 1.138e-01
initial = 1e-07    final = 4.368e-02
initial = 1e-08    final = 3.059e-01
initial = 1e-09    final = 3.143e-01
initial = 1e-10    final = 4.002e-01
```

![clifford sensitive]({{https://blbadger.github.io}}/misc_images/clifford_sensitivity.png)

This means that the Clifford attractor is sensitive to initial values, implying that it is aperiodic for these parameters. 

### Pendulum map in the Clifford system

There are a number of similarities between widely different nonlinear systems.  Perhaps the most dramatic example of this is the ubiquitous appearance of self-similar fractals in chaotic nonlinear systems.  This may be most dramatically seen when the constant parameters of certain equation systems are tweaked such that the output produces a near-copy of another equation system, a phenomenon that is surprisingly common to nonlinear systems. For example, take the Clifford attractor:

$$
x_{n+1} = \sin(ay_n) + c \cdot \cos(ax_n) \\
y_{n+1} = \sin(bx_n) + d \cdot \cos(by_n) 
\tag{3} $$

This is clearly a very different system of equations than (1), and for most constant values it produces a variety of maps that look nothing like what is produced by the pendulum system.  But observe what happens when we iterate semicontinuously (see [here](/clifford-attractor.md) for more information), setting

$$
a=-0.3, b=0.2, c=0.5, d=0.3, \Delta t = 0.9 \\
(x_0, y_0) = (90, 90)
$$

We have a (slightly oblong) pendulum map!  

![clifford pendulum image]({{https://blbadger.github.io}}pendulum_map/clifford_pendulum.png)

---

### If using large $\Delta t$ values yields a physically inaccurate map, what do these images mean?

There are some physically relevant reasons to increase a $\Delta t$ value: 

1. The case of periodic forcing, where external energy is applied to a physical system in regular intervals.  The *dt* value may be thought of as a direct measure of this energy, as a large enough $\Delta t$ will send this system towards infinity (ie infinite velocity). 

2. When a field is intermittent: if a particle moves smoothly but only interacts with a field at regular time intervals, the same effect is produced. 

## Roots of polynomial equations II

This page is the second of a two-part series on polynomial root-finding methods.  For a background on Newton's method as well as Halley's and the Secant method, see [part I](https://blbadger.github.io/polynomial-roots.html).

### Periodic attractor basins

Let's look closer at the areas that converge slowly for Newton's method applied to the equation $z^5-z-1$. A little experimentation suggests that these areas may never converge on a root, as increasing the maximum iteration number for Newton's method fails to change them.  Tracking iterations that start in the central (near the origin, that is) sowly-converging area, many are found to converge on the period-3 orbit

$$
-1.000257..., -0.750321..., 0.0833570..., -1.000257...
$$

Certainly there are some points in the plane, such as $\pm \sqrt[4]{1/5}$, which do not converge on anything at all.  But do the other starting points in the slowly-converging region eventually end up in this periodic orbit?  This can be tested for each point plotted for Newton's method (code for this section [here](https://github.com/blbadger/polynomial_roots/blob/main/newton_convergence_test.py)) After 80 iterations, nearly all points head towards a root or else to the periodic orbit above:

![convergence]({{https://blbadger.github.io}}/newton-method/convergence_overlayed.png)

What about the non-converging areas on the periphery?  If we follow the trajectory from the point $4.707 - 4.117i$, 

![convergence]({{https://blbadger.github.io}}/newton-method/newton_trajectory.gif)

the outer region is mapped to the inner region.  This is also appears to the the case for initial points far from the origin that do end up converging on a root.  Such observations suggest that initial points far from the origin may stay arbitrarily close together until entering the central region.  Is this the true for the points plotted previously?

This can be tested by checking how long it takes for two arrays shifted by a small amount (here 0.0000001) to come near each other:

```python
def traveling_together(equation, max_iterations, x_range, y_range):
	"""
	Returns points that stay near points nearby in future iteration.

	Args:
		equation: str, polynomial of interest
		max_iterations: int of iterations
		x_range: int, number of real values per output
		y_range: int, number of imaginary values per output

	Returns:
		iterations_until_together: np.arr[int] (2D) 
		
	"""

	y, x = np.ogrid[2: -2: y_range*1j, -2: 2: x_range*1j]
	z_array = x + y*1j
	z_array_2 = z_array + 0.0000001 # arbitrary change, can be any small amount
	iterations_until_together = max_iterations + np.zeros(z_array.shape)

	# create a boolean table of all 'true'
	not_already_together = iterations_until_together < 10000

	# initialize calculate objects
	nondiff = Calculate(equation, differentiate=False)
	diffed = Calculate(equation, differentiate=True)

	for i in range(max_iterations):
		f_now = nondiff.evaluate(z_array) 
		f_prime_now = diffed.evaluate(z_array)
		z_array = z_array - f_now / f_prime_now

		f_2_now = nondiff.evaluate(z_array_2) 
		f_2_prime_now = diffed.evaluate(z_array_2)		
		z_array_2 = z_array_2 - f_2_now / f_2_prime_now

		# the boolean map is tested for rooted values
		together = (abs(z_array - z_array_2) <= 0.0000001) & not_already_together
		iterations_until_together[together] = i
		not_already_together = np.invert(together) & not_already_together

	return iterations_until_together

```
which yields

![convergence]({{https://blbadger.github.io}}/newton-method/Newton_z^5-z-1_together.png)

All mapped points outside a certain radius from the origin stay near each other for their first iteration.  This is also the case for $z^3-1$, 

![convergence]({{https://blbadger.github.io}}/newton-method/newton_z^3-1_together.png)

And for incremental powers between $z^1-z-1$ and $z^6-z-1$

{% include youtube.html id='AsSwysIDTfg' %}

### Julia and Mandelbrot sets in Newton's map

Let's look closer at the periodic attractor in Newton's map of $z^5-z-1$

![Newton zoomed]({{https://blbadger.github.io}}/newton-method/newton_zoomed.png)

A keen-eyed observer may note that this region resembles a slightly squashed (and filled-in) [Julia set](https://blbadger.github.io/julia-sets.html) defined by 

$$
z_{n+1} = z_n^2 + (0.5 + 0i)
$$

which when mapped in the complex plane appears as

![Julia set]({{https://blbadger.github.io}}/newton-method/julia_0.5.png)

In the case of this Newton map, we are observing initial points in the complex plane that either do or do not converge on a root whereas for classically defined Julia sets, we are interested in initial points in the complex plane that either diverge to infinity or else end up in a periodic orbit.  Thus by assigning points that find roots using Newton's method to be equivalent to points that head towards infinity, we arrive at the interesting conclusion that these Newton fractals are analagous to Julia sets, broadly defined.

Where there are Julia sets, one can often find a [Mandelbrot set](https://blbadger.github.io/mandelbrot-set.html).  This means that Julia sets are defined by fixing a dynamical equation and observing which initial points in the complex plane diverge or are trapped in periodic trajectories, and the generalized Mandelbrot set is defined by allowing the equation to change according to various points in the complex plane whilst holding the intial point constant at the origin.

Similarly, we can see which points find a root (ie which head to 'infinity') given a starting value of $0 + 0i$ and an addition of the given point to Newton's map.  In symbols, we are interested in the map

$$
z_{n+1} = z_{n} + \frac{f(z)}{f'(z)} + a \\
z_0 = 0 + 0i \\
a \in \Bbb C
$$

which can be observed using the following code:

```python
def newton_boundary(equation, max_iterations, x_range, y_range):
	...
	y, x = np.ogrid[0.045: -0.045: y_range*1j, -0.045: 0.045: x_range*1j]
	a_array = x + y*1j
	z_array = np.zeros(a_array.shape)

	iterations_until_rooted = np.zeros(z_array.shape)

	 # create a boolean grid of all 'true'
	not_already_at_root = iterations_until_rooted < 10000

	nondiff = Calculate(equation, differentiate=False)
	diffed = Calculate(equation, differentiate=True)

	for i in range(max_iterations):
		iterations_until_rooted[not_already_at_root] = i
		previous_z_array = z_array
		z = z_array
		f_now = nondiff.evaluate(z)
		f_prime_now = diffed.evaluate(z)
		z_array = z_array - f_now / f_prime_now + a_array

		# the boolean map is tested for rooted values
		found_root = (abs(z_array - previous_z_array) < 0.0000001) & not_already_at_root
		iterations_until_rooted[found_root] = i
		not_already_at_root = np.invert(found_root) & not_already_at_root

	return iterations_until_rooted
```
Looking close to the origin, we find a (slightly distorted) Mandelbrot set has replaced our Julia set.

![convergence]({{https://blbadger.github.io}}/newton-method/Newton_boundaryx5-x-1.png)

More accurately, this is actually a mix of Mandelbrot

$$
z_{n+1} = z_{n}^2 + a \\
$$

and what is sometimes called Multibrot

$$
z_{n+1} = z_{n}^b + a \\
b > 2
$$

sets, which is perhaps not altogether surprising being that our polynomial of interest is a fifth degree entity and thus encompasses both second and fourth dergree polynomials.  Zooming in by a factor of $10^7$ on one of these fourth-degree Multibrots, we have

{% include youtube.html id='-M3ERdfhjaI' %}

A true mandelbrot set may be found by proceeding with the same interation procedure described above for the equation

$$
z^3 - z - 1
$$

when increasing scale by $10^7$ at the point  

$$
0.19379287 + 0.002549i
$$

we have (note that the Mandelbrot set interior fails to converge as for the multibrot above but the color was changed to highlight the border for visual flair)

{% include youtube.html id='vJo4nBMLycI' %}

### Incrementing powers

Maps of convergence using Newton's method display sensitivity to initial conditions: points arbitrarily nearby to each other in the complex plane may have very different times to convergence.  In addition, the exponential powers of the input equation displays analagous behavior: small changes in exponent magnitude lead to large changes for which starting points find roots quickly. As $a$ is increased past 1, $z^5-z^{a}-1$ yields (right click to view in higher resolution)

![still]({{https://blbadger.github.io}}/newton-method/Newton_vanishing_still_083.png)

Upon incrementing $z^5-z-1 \to z^5-z^{1.033}-1$,

{% include youtube.html id='Wi0EQ7WqJtU' %}

For the equation

$$
z^7-z-1
$$

with roots estimated via Newton's method (centered on the origin), we have

![dimension]({{https://blbadger.github.io}}/newton-method/newton_z^7-z-1.png)

at $z^{7.11}-z-1$, 

![dimension]({{https://blbadger.github.io}}/newton-method/newton_z^7.11-z-1.png)

and at $z^{7.14}-z-1$ 

![dimension]({{https://blbadger.github.io}}/newton-method/newton_z^7.14-z-1.png)

From $z^7-z-1$ to $z^{7.397}-z-1$,

{% include youtube.html id='fpKBzto_ZnA' %}

and from $z^7-z-1$ to $z^{7.1438555}-z-1$, incremented slowly, 

{% include youtube.html id='VoxHmL-1Hys' %}

In the last section, we observed some polynomials with complex values,meaning that instead of focusing on real constants and exponents and allowing the unkown to assume complex values, now $a_n, b_n \in \Bbb C$ for polynomials 

$$
a_0x^{b_0} + a_1x^{b_1} + a_2x^{b_2} + \cdots
$$ 

The results for 

$$
z^{5 + 0i}-z-1 \to \\
z^{5 + 0.2802i}-z-1
$$ 

are as follows:

{% include youtube.html id='TyZGJQi0cmM' %}

and for a longer video of 

$$
z^{7.11}-z-1 \to \\
z^{7.11 + 0.002271i}-z^{1+0.002271i}-1
$$

we have

{% include youtube.html id='Q4xXdPizlX0' %}









## Roots of polynomial equations part I

This page is the first of a two-part series on methods to find polynomial roots.  For more exploration on Newton's method, and how it relates to Julia and Mandelbrot sets, see [part II](/polynomial-roots2.html).

### Introduction

Algebraic polynomials are equations of the type 

$$
ax^n + bx^{n-1} + cx^{n-2} + \cdots + z
\tag{1} \label{eq1}
$$

Given any polynomial, the value or values of $x$ such that the polynomial evaluates to zero are called the roots of that equation.  Such value(s) of $x$ are also called the solutions of that equation because once known the polynomial may be split into parts called factors.  Or alternatively if one knows how to factor a polynomial, one can then recover its roots. By the fundamental theorem of algebra, every polynomial has at least one root and therefore \eqref{eq1} contains exactly $n$ (not necessarily distinct) roots in $\Bbb C$.

At first glance, rooting polynomials in terms of their constants seems to be an easy task.  For a degree 1 polynomial (meaning that $n=1$), $y = ax + b$, setting $y$ to $0$ and solving for x yields 

$$
x = -b/a
$$

For a degree 2 polynomial $ax^2 + bx + c$, the roots

$$
ax^2 + bx + c = 0
$$

may be expressed as 

$$
x^2 + (b/a)x = -c/a
$$

Completing the square by adding $b^2/4a^2$ to both sides, 

$$
ax^2 + \frac{b}{a}x + \frac{b^2}{4a^2} = \frac{b^2}{4a^2} - \frac{c}{a} \\
\tag{2} \label{eq2}
$$

the left-hand term is now equal to

$$
\left( x + \frac{b}{2a} \right) ^2
$$

such that when we square root both sides of \eqref{eq2}, 

$$
x + \frac{b}{2a} = \pm \sqrt{\frac{b^2}{4a^2} - \frac{c}{a}} \\
x = \frac{-b \pm \sqrt{b^2-4ac}}{2a}
$$

At this point, there is no clear indication that we cannot find a similar expression for any arbitrary polynomial, but there is some slight indication that finding the expression may be very difficult because completing the square is not a method that is necessarily applicable to higher powers. 

For degree 3 polynomials of the form $ay^3 + by^2 + cy + d$, a change of variables achieved by substituting $y = x - \frac{b}{3a}$ gives $x^3 + ax + b$, and a real root of this resulting equation may be found as follows

$$
x = \sqrt[3]{\frac{-b}{2} + \sqrt D} + \sqrt[3]{\frac{-b}{2} - \sqrt D} \\
D = \frac{a^3}{3^3} + \frac{b^2}{2^2}
$$

Similarly, for polynomials of degree 4 a closed form root expressions in terms of $a, b, c ...$ may be found even though the expression becomes quite long.

It is somewhat surprising then that for a general polynomial of degree 5 or larger, there is no closed equation (with addition, subtraction, multiplication, nth roots, and division) that allows for the finding of all roots.  This is the Abel-Ruffini theorem, and exactly which polynomials can and cannot be rooted is explored in Galois theory.  

The finding that there is no closed expression for any general finite polynomial implies the somewhat counterintuitive idea that some polynomials of degree 5 or more cannot be broken apart into factors (of any smaller degree) using a finite number of the fundamental operations of arithmetic, even though the polynomial itself is defined using these operations.  The inability to find a system's constituent parts given the whole system itself is a general feature of nonlinearity, and is witnessed on many pages of this site.

### Newton's method for estimating roots of polynomial equations

What should one do to find the roots to a polynomial, if most do not have closed form root equations?  If the goal it to approximate a root, rather than express it as a combination arithmetic operations we can borrow the Newton-Raphson method from analysis. The procedure, described [here](https://en.wikipedia.org/wiki/Newton%27s_method), involves first guessing a point near a root, and then finding the x-intercept of the line tangent to the curve at this point.  These steps are then repeated iteratively such that the x-intercept found previously becomes the x-value of the new point.  This can be expressed dynamically as follows:

$$
x_{n + 1} = x_n - \frac{f(x_n)}{f'(x_n)}
$$

After a certain number of iterations, this method usually settles on a root as long as our initial guess is reasonable.  

Let's try it out on the equation $y = x^3 - 1$, which has one real root at $x = 1$ and two complex roots 

$$
\frac{-1+i\sqrt{3}}{2}, \frac{-1-i\sqrt{3}}{2}
$$

These values are called the cube roots of unity, because they are solutions to the equation $x^3 = 1$. The complex valued roots may be found by converting $x^3-1$ to $(x-1)(x^2+x+1)$ and then applying the quadratic formula to the second term.  

How does Newton's method do for finding any of these roots? We can see by tracking each iterative root guess and looking for convergence, which is when subsequent values come closer and closer to each other until they are equivalent.  Dynamically, convergence is described as a period 1 trajectory, because there is only 1 iteration of the equation (in this case Newton's method) before the same value is reached again.  The progress of root finding with Newton's method may be tracked as follows (link to [code for this page](https://github.com/blbadger/polyroots)):

```python
#! python3

def successive_approximations(x_start, iterations):
	'''
	Computes the successive approximations for the equation
	y = x^3 - 1.
	'''
	x_now = x_start
	ls = []
	for i in range(iterations):
		f_now = x_now**3 - 1
		f_prime_now = 3*x_now**2 

		x_next = x_now - f_now / f_prime_now
		ls.append(x_now)
		x_now = x_next
    
	return ls
```

Let's try with an initial guess at $x=-50$, 

```python
print (successive_approximations(-50, 20))
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[-50, -33.333200000000005, -22.221833330933322, -14.813880530329783, -9.874401411977551, -6.579515604587147, -4.378643733243303, -2.9017098282008416, -1.894884560449859, -1.1704210552983418, -0.5369513549799065, 0.7981682581594858, 1.0553388026381803, 1.002851080311187, 1.0000080978680779, 1.0000000000655749, 1.0, 1.0, 1.0, 1.0]
```

There is convergence on the (real) root, in 17 iterations.  What about if we try an initial guess closer to the root, say at $x=2$?
```python
print (successive_approximations(2, 10))
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[2, 1.4166666666666665, 1.1105344098423684, 1.0106367684045563, 1.0001115573039492, 1.0000000124431812, 1.0000000000000002, 1.0, 1.0, 1.0]
```

The method converges quickly on the root of 1. Now from this one might assume that starting from the point near $x=0$ would result in convergence in around 8 iterations as well, but 

```python
print (successive_approximations(0.000001, 20))
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[1e-06, 333333333333.3333, 222222222222.2222, 148148148148.14813, 98765432098.76541, 65843621399.17694, 43895747599.451294, 29263831732.96753, 19509221155.311684, 13006147436.874454, 8670764957.916304, 5780509971.944202, 3853673314.6294684, 2569115543.0863123, 1712743695.3908749, 1141829130.2605832, 761219420.173722, 507479613.44914806, 338319742.29943204, 225546494.86628804]
```

a root is not found in 20 iterations! It takes 72 to find converge on a root if one starts at the origin.   By observing the behavior of Newton's method on three initial points, it is clear that simple distance away from the root does not predict how fast the method will converge. Note that the area near $x=0$ is not the only one that converges slowly on a root: $-0.7937...$ and $-1.4337...$ do as well.

Why does an initial value near 0 lead to a slow convergence using Newton's method?  Notice that the starting value was not exactly 0.  Remembering that $x^3-1$ has a tangent line parallel to the x-axis at point $x=0$, and that parallel lines never meet, it is clear that Newton's method must fail for this point because the tangent line will never meet the x-axis.  Now consider why there must also be one other point that fails to converge using Newton's method: one tangent line of $x^3-1$ intersects the origin precisely, and therefore the second iteration of Newton's method for this point will cause the method to fail. The precise location of this first point may be found by setting $x_{n+1}$ to 0 and solving for $x_n$,

$$
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} \\
0 = x - \frac{x^3-1}{3x^2} \\
-\sqrt[3]{\frac 12} = x
$$

or equivalently, as the tangent line with slope $m = 3x^2$ must pass through both the origin $x_0, y_0$, 

$$
y - y_0 = m(x - x_0) \\
y - 0 = 3x^2(x - 0) \\
y = 3x^3
$$

subject to the condition that the pair of points $(x, y)$ exists on the line $y = x^3-1$, 

$$
y = 3x^3, y = x^3-1 \implies \\
0 = 2x^3 + 1 \\
-\sqrt[3]{\frac12} = x
$$

Which evaluates to around $-0.7937...$.  But because there is one point, there must be an infinite number of other points (not necessarily distinct) along the real line that also fail to find a root because for every initial value $v_i$ that fails with this equation, there is another $v_{i2}$ such that that $v_i$ is the second iteration of Newton's method on $v_{i2}$ and so on.  To illustrate, setting the next iteration to our value just found, 

$$
-\sqrt[3]{\frac12} = x - \frac{x^3-1}{3x^2} \\
0 = 2x^3 + 3 \left( \sqrt[3]{\frac12} x^2 \right) + 1
$$

To find this point exactly, the formidable cubic formula is required. Evaluating this yields $x= -1.4337..$.  It can be appreciated now how difficult it is to find the $n^{th}$ region counting backwards from the origin that does not converge, as each iteration must be calculated in sequence.  

Now consider the equation

$$
y = (x-2)(x-1)(x+3) = x^3-7x+6
$$

with roots at $x=-3, x=1, x=2$.  Again there is more than one point on the real line where Newton's method fails to converge quickly.  We can plot the pattern such points make as follows: darker the point, the faster the convergence. Lines of color corresponding to points on the real line are used for clarity.

![roots]({{https://blbadger.github.io}}/newton-method/real_newton_still.png)

This plot seems reasonable, as the points near the roots converge quickly.  Looking closer at the point $x= - 0.8410713881$, 

{% include youtube.html id='D2fmFNISpco' %}

we find a [Cantor set](fractal-geometry.md).  Not only does this polynomial exhibit many values that are slow to find a root, as was the case for $x^3 - 1$, but these non-converging points form a fractal.  To see which values are in the set of non-converging points, first observe that Newton's method will fail for points where $f'(x) = 0$, and as $f'(x) = 3x^2-7$ this evaluates to $x = \pm \sqrt{7/3}$.  

We have found one non-converging value, but are there more?  Yes, because any point whose next iteration of Newton's method that lands on $\pm \sqrt{7/3}$ will also not converge.  By setting $x_{n+1} = \sqrt{7/3}$ we have

$$
x_{n+1} = x_{n} - \frac{f(x_{n})}{f'(x_{n})} \\
\sqrt{7/3} = x_{n} - \frac{x_{n}^3-7x_{n}+6}{3x_{n}^2-7} \\
x_{n} = 0.8625...
$$

which is the second line passed on the left in the video above.  Setting $x_{n+1} = -0.8625$ and applying Newton's method again and solving for $x_n$ 

$$
-0.8625... = x_{n} - \frac{x_{n}^3-7x_{n}+6}{3x_{n}^2-7} \\
x_{n} = 1.4745...
$$

In other words, if $x_0 = \sqrt{7/3}$ then

$$
x_{-1} = 0.8625...\\
x_{-2} = 1.4745... \\
x_{-3} = -0.8413...\\
x_{-4} = 1.47402...
$$

all of which are algebraic but none of which are rational.

It turns out there are an infinite number of distinct points of $x_n \; n \in \Bbb Z$, but they are contained within $[-3, 2]$ ie the region between two of the roots.

### Newton's method in the complex plane

The method used above for tracking progress made by Newton's method can also be applied for a complex starting number, for example

```python
print (successive_approximations(2 + 5j, 20))
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[(2+5j), (1.325009908838684+3.3254062623860485j), (0.8644548662679195+2.199047743713538j), (0.5325816440348543+1.4253747693717602j), ... (-0.5+0.8660254037844386j), (-0.5+0.8660254037844387j)]
```

where Newton's method finds the complex root 

$$
\frac{-1+i\sqrt{3}}{2}
$$

This means that the complex plane may be explored. Because Newton's method requires differentiation and evaluation of a polynomial, I wrote a module `Calculate` to accomplish these tasks (which may be found [here](https://github.com/blbadger/polynomial_roots/blob/main/Calculate.py)).   Now a map for how long it takes for each point in the complex plane to become rooted using Newton's method may be generated as follows:

```python
# import third party libraries
import numpy as np 
import matplotlib.pyplot as plt 

from Calculate import Calculate # see above


def newton_raphson_map(equation, max_iterations, x_range, y_range, t):
	"""
	Generates a newton-raphson fractal.

	Args:
		equation: str, equation of interest
		max_iterations: int, number of iterations 
		x_range: int, number of real values per output
		y_range: int, number of imaginary values per output
		t: int

	Returns:
		iterations_until_rooted: np.arr (2D) of iterations until a root is found
			at each point in y_range and x_range

	"""

	print (equation)
	y, x = np.ogrid[0.7: -0.7: y_range*1j, -1.1: 1.1: x_range*1j]
	z_array = x + y*1j

	iterations_until_rooted = max_iterations + np.zeros(z_array.shape)

	 # create a boolean gridof all 'true'
	not_already_at_root = iterations_until_rooted < 10000

	nondiff = Calculate(equation, differentiate=False)
	diffed = Calculate(equation, differentiate=True)


	for i in range(max_iterations):
		print (i)
		previous_z_array = z_array
		z = z_array
		f_now = nondiff.evaluate(z)
		f_prime_now = diffed.evaluate(z)
		z_array = z_array - f_now / f_prime_now

		# the boolean map is tested for rooted values
		found_root = (abs(z_array - previous_z_array) < 0.0000001) & not_already_at_root
		iterations_until_rooted[found_root] = i
		not_already_at_root = np.invert(found_root) & not_already_at_root

	return iterations_until_rooted

```

The idea here is to start with a grid of the complex plane points that we are testing (`z_array`), a numerical table `iterations_until_rooted` tracking how many iterations each point takes to find a root and a boolean table `not_already_at_root` corresponding to each point on the plane (set to `True` to start with) to keep track of which points have found a root. An small value (not 0, due to round-off error) which is `0.0000001` then compared to the difference between successive iterations of Newton's method in order to see if the point has moved.  If the point is stationary, the method has found a root and the iteration number is recorded in the `iterations_until_rooted` table.  Note that we are only concerned with whether a root has been found, not which root has been found.

Now we can call this function to see how long it takes to find roots for $x^3 - 1$ as follows

```python
plt.style.use('dark_background')
plt.imshow(newton_raphson_map('x^3-1', 50, 1558, 1558, 30), extent=[-5, 5, -5, 5], cmap='twilight_shifted')
plt.axis('on')
plt.show()
plt.close()
```

which yields

![roots]({{https://blbadger.github.io}}/newton-method/Newton_Raphson.png)

Note that for this color map, purple corresponds to fast convergence and white slow, with no convergence (within the allotted iterations) in black for emphasis.  John Hubbard was the first person to observe the fractal behavior of Newton's method in the complex plane, and the border between the set of non-diverging and diverging points may be considered to be a [Julia set](https://blbadger.github.io/julia-sets.html).  

The roots for $z^3-1$ (dark circles in the plot above) may be plotted along a circle centered around the origin.  This is true not just for that equation but for any root of unity or indeed any root of any complex number by De Moivre's theorem,

$$
z^n = r^n(\cos n\theta + i \sin n\theta )
$$

where $\theta$ is in radians of counterclockwise rotation from the positive real axis.

The above image has symmetry about the real axis (the horizontal line at the center).  Is this a coincidence, or does symmetry around the real axis necessarily occur for Newton's method?  As long as the polynomial in question $a(x)$ contains only real coefficients, the roots are symmetric around the real axis because if $a + bi$ is a root, so is its conjugate $a - bi$.  This is because we can assign a function $f(x)$ to map any point in the complex plane to its conjugate, meaning that $f(x): a - bi \to a + bi, \; a + bi \to a - bi$ for any $a, b \in \Bbb C$.  

This function $f(x)$ is a structure-preserving map, also called a homomorphism, because 

$$
f(ab) = f(a)f(b) \; \forall a, b \in \Bbb C
$$

Furthermore because for any real number $a$, $f(a) = a$, for any polynomial with real coefficients $a(x)$, $f(a(x))$ contains the same constants and only converts $x$ to its conjugate.  This does not change the root values of that function because $f(0) = 0$. 

Moreover, the trajectory taken with Newton's method may be expressed as a series of polynomials, all of which have real coefficients if $a(x)$ has real coefficients.  Because there exists a homomorphism (actually isomorphism) from $a + bi$ to and from $a - bi$, the trajectories given equivalent $b$ are identical apart from an inversion about the real axis.

### Incremental powers

What happens between integer polynomial powers?  Here $z = z^2-1 \to z = z^5-1$, follow the link to see the change (right click the video while playing to loop)

{% include youtube.html id='YIO_w4x1P2k' %}

And taking a closer look at the transition 

$$
z^{3.86} - 1 \to z^{3.886}-1
$$

{% include youtube.html id='qS8g6m0QOik' %}

As the polynomial's largest power is incremented, large area of poorly-converging initial points appears as a light region to the left of the origin.  This is the result of the appearance of a period-2 attractor whose points converge on the root along the negative real line as the polynomial approaches $z^4-1$

The images produced by searching the complex plane for points that converge quickly can be very beautiful.  It is fascinating to explore the exquisite shapes that are made with relatively simple polynomials, so I made a web application to allow one to explore these images freely using Newton's method (and other methods detailed towards the end of this page).  Follow [this link](https://pfinderr.herokuapp.com/) for the app.

### A simple unrootable polynomial

Arguably the simplest polynomial that by Galois theory does not have a closed form rooted expression is

$$
x^5 - x - 1
$$

This does not mean that finding the roots to this equation is impossible, but instead that there is no expression consisting of radicals $ \sqrt x$, addition, subtraction, multiplication, and division that represents the roots of this equation.  How does one know that $x^5-x-1$ cannot be rooted?  As a first step, one can find roots to an equation by attempting to factor it. Factoring here means representing a polynomial by two or more smaller polynomials, so $a(x) = b(x)c(x)$. Can the above expression be factored? 

Perhaps the most direct way to check if a polynomial may be factored is to use Eisenstein's irreducibility criterion, where a polynomial $a(x)$ defined as

$$
a(x) = a_0 + a_1x + \cdots + a_nx^n
$$

is not reducible (not factorable) if there exists a prime number p such that p divides all $a$ except $a_n$, and $p^2$ does not divide $a_0$.  Eisenstein's criterion cannot be applied to our polynomial here because $1$ is not prime.  It can be shown that Eisenstein's criterion also applies if one substitutes $x$ for $x+a$ for any integer $a$, but this is also less than helpful.

Instead, realizing that any polynomial with rational coefficients may be factored into some constant times a new polynomial with integer coefficients, we can determine if our equation has a root in $\Bbb Q$, the rational numbers. This is because for any rational root of $a(x)$, $s/t$, in smallest terms then in the notation above $s \lvert a_0$ and $t \lvert a_n$.

It is an interesting result that any rational root field can be converted to an integer root field, which means that if our equation above has a root, it can be expressed as an integer.  But the only $s/t$ which satisfies these conditions is 1, and by substituting $x=1$ it can clearly be seen that this value is not a root of $x^5-x-1$. 

Therefore $x^5-x-1$ does not have any roots in $\Bbb Q$, and necessarily cannot be factored into a smaller polynomial of integer (or rational) values. But can we make a radical expression for any of these roots? Radicals are generally not elements of $\Bbb Q$, and so ruling out rational roots does not rule out the possibility that any root may be expressable as a finite number of integers and their roots.

The answer to this, too, is no: it turns out that neither can we construct all the roots for this equation with radicals.  This is because the corresponding Galois group to this polynomial is $S_5$, the group of permutations of five elements.  Which is to say that no finite addition of radicals can extend $\Bbb Q$ to the field of roots of our equation, and therefore no finite expression of with the terms desired may be constructed for the roots of this equation.

Let's explore where roots are found with Newton's method in the complex plane, ie for $z^5-z-1$.  With a scale of  $(-1.85, 2.15)$ for the real values on the horizontal axis and $(-1.86i, 2.14i)$ on the vertical (using our original color map),

![still]({{https://blbadger.github.io}}/newton-method/newton_x5_still.png)

Note that the roots are no longer found in a circle centered on the origin, as the equation cannot be expressed as a root of any one complex number.  Looking at the roots more closely, there is one real and four complex roots: this is expected, as complex roots always come in pairs of a number and its conjugate. 

Unlike the case for $z^3-1$, an entire region fails to converge on a root, rather than individual points.  Now there are certainly points analagous to that observed above (for $z^3-1$) in which the tangent line is parallel to the x-axis: these exist at

$$
f'(x) = 0 \\
5x^4 - 1 = 0 \\
x = \pm \sqrt[4]{1/5} \approx \pm 0.6689...
$$

which corresponds to the point in the center of the 5 radial spokes to the right of the largest area of no convergence.  But these do not explain the appearance of large areas of no convergence themselves.

Newton's method yields self-similar fractals, which can be clearly observed by increasing scale. For example, zooming in on the point $0.154047 + 0.135678i$,

{% include youtube.html id='ZTMaKLmLxJM' %}


### Newton's map rotations

So far we have been considering polynomials with only real-valued constants and powers, or in other words polynomials $a_0x^{b_0} + a_1x^{b_1} + \cdots$ where $a_n, b_n \in \Bbb R$.  This is not by necessity, and just as $x$ can be extended to the complex plane so can a polynomial itself be extended.  Programs to accomplish this may be found [here](https://github.com/blbadger/polyroots) (use the non-optimized versions for complex-valued polynomials, as the optimized programs do not currently support complex-valued polynomials).  Skip to the next section to observe what happens upon incrementing a complex power.

There is one particular transformation that is especially striking which can be accomplished by extending a polynomial's constants and powers into the complex plane. Using the identity $e^{\pi i} + 1 = 0$, we can rotate the function in the complex plane about the origin in a radius of $1/4$ as follows:

```python
...
for i in range(max_iterations):
		previous_z_array = z_array
		z = z_array
		f_now = z**5 - z - 1 + np.exp(3.1415j * (t/300))/4 
		f_prime_now = 5*z**4 - 1
		z_array = z_array - f_now / f_prime_now
```

{% include youtube.html id='ejqFKnLRWjM' %}

We can also perform a different rotation as follows:
```python
...
for i in range(max_iterations):
		previous_z_array = z_array
		z = z_array
		f_now = (np.exp(3.1415j * (t/450000))/4) * z**5 - z * np.exp(3.1415j * (t/450000))/4 - 1 + np.exp(3.1415j * (t/450000))/4 
		f_prime_now = 5 * (np.exp(3.1415j * (t/450000))/4)*z**4 - np.exp(3.1415j * (t/450000))/4
		z_array = z_array - f_now / f_prime_now
```

the cover photo for this page found [here](https://blbadger.github.io/) is at t=46, and at t=205 (0.00023 of a full rotation) yields

![rotation]({{https://blbadger.github.io}}/newton-method/Newton_all_205.png)

Follow the link for a video of this rotation:

{% include youtube.html id='NgZZq32in7g' %}

Note that these are the first maps that are not symmetric about the real axis.  This is because the polynomial used to iterate Newton's method no longer has only real-valued coefficients, but instead imaginary values are introduced to accomplish the rotation. Now with $f(x)$ that transforms conjugates $a + bi, a - bi$ into one another, the coefficients are no longer unchanged and there is not an isomorphism that exists between regions of the complex plane.

### Secant method 

There are other methods besides that of Newton for finding roots of an equation.  Some are closely related to Newton's method, for example see the [secant method](https://en.wikipedia.org/wiki/Secant_method) in which is analagous to a non-analytic version of Newton's method, in which two initial guesses then converge on a root. 

There are two initial guesses, $x_0$ and $x_1$, which are then used to guess a third point

$$
x_{n+1} = x_n - f(x_n) \frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}
$$

And so on until the next guess is the same as the current one, meaning that a root has been found.  This is algebraically equivalent to Newton's method if the derivative is treated as a discrete quantity, ie

$$
f'(x) = \frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}
$$

Because there are two initial points rather than one, there is more variety in behavior for any one guess.  To simplify things, here the first guess is half the distance to the origin from the second, and the second guess ($x_1$) is the one that is plotted in the complex plane. This can be accomplished as follows:

```python
def secant_method(equation, max_iterations, x_range, y_range, t):
	"""
	Returns an array of the number of iterations until a root is found
	using the Secant method in the complex plane.

	Args:
		equation: str, polynomial of interest
		max_iterations: int of iterations
		x_range: int, number of real values per output
		y_range: int, number of imaginary values per output
		t: int

	Returns:
		iterations_until_together: np.arr[int] (2D) 
		
	"""

	# top left to bottom right
	y, x = np.ogrid[1: -1: y_range*1j, -1: 1: x_range*1j]
	z_array = x + y*1j
	iterations_until_rooted = max_iterations + np.zeros(z_array.shape)

	 # create a boolean table of all 'true'
	not_already_at_root = iterations_until_rooted < 10000
	zeros = np.zeros(z_array.shape) 

	# set the initial guess to half the distance to the origin from the second guess
	z_0 = (z_array - zeros)/2 

	# initialize calculate object
	nondiff = Calculate(equation, differentiate=False)

	for i in range(max_iterations):
		previous_z_array = z_array
		z = z_array
		f_previous = nondiff.evaluate(z_0)
		f_now = nondiff.evaluate(z)
		z_array = z - f_now * (z - z_0)/(f_now - f_previous)

		# the boolean map is tested for rooted values
		found_root = (abs(z_array - previous_z_array) < 0.0000001) & not_already_at_root
		iterations_until_rooted[found_root] = i
		not_already_at_root = np.invert(found_root) & not_already_at_root

		# set previous array to current values
		z_0 = z 

	return iterations_until_rooted
```

For $z^3-1$, 

![secant]({{https://blbadger.github.io}}/newton-method/secant_z^3-1_half.png)

Remembering that black denotes locations that do not converge within the maximum interations (here 50), it is clear that there are more points that fail to find a root using this method compared to Newton's, which is expected given that the convergence rate is slower than for Newton's.

Rotated at a radius of $1/8$, $z^5-z-1$ yields

{% include youtube.html id='ABkcXst8Jn0' %}

And incrementing from $z^5-z-1$ to $z^6-z-1$ we have

{% include youtube.html id='NpFupXaRvEA' %}

### Halley's method

Edmond Halley (of Halley's comet fame) introduced the following algorithm to find roots for polynomial equations:

$$
x_{n+1} = x_n - \frac{2f(x_n)f'(x_n)}{2(f'(x_n))^2-f(x_n)f''(x_n)}
\;
$$

The general rate of convergence for this method is cubic rather than quadratic as for Newton's method. This method can be implemented using [Calculate](https://github.com/blbadger/polynomial_roots/blob/main/Calculate.py) as follows:

```python

def halley_method(equation, max_iterations, x_range, y_range, t):
	"""
	Returns an array of the number of iterations until a root is found
	using Halley's method in the complex plane.

	Args:
		equation: str, polynomial of interest
		max_iterations: int of iterations
		x_range: int, number of real values per output
		y_range: int, number of imaginary values per output
		t: int

	Returns:
		iterations_until_together: np.arr[int] (2D) 
		
	"""

	# top left to bottom right
	y, x = np.ogrid[1: -1: y_range*1j, -1: 1: x_range*1j]
	z_array = x + y*1j

	iterations_until_rooted = max_iterations + np.zeros(z_array.shape)

	# create a boolean table of all 'true'
	not_already_at_root = iterations_until_rooted < 10000

	# initialize calculate object
	nondiff = Calculate(equation, differentiate=False)
	diffed = Calculate(equation, differentiate=True)

	# second derivative calculation
	diff_string = diffed.to_string()
	double_diffed = Calculate(diff_string, differentiate=True)

	for i in range(max_iterations):
		previous_z_array = z_array
		z = z_array

		f_now = nondiff.evaluate(z)
		f_prime_now = diffed.evaluate(z) # first derivative evaluation

		f_double_prime_now = double_diffed.evaluate(z) # second derivative evaluation

		z_array = z - (2*f_now * f_prime_now / (2*(f_prime_now)**2 - f_now * f_double_prime_now))

		# test the boolean map for rooted values
		found_root = (abs(z_array - previous_z_array) < 0.000000001) & not_already_at_root
		iterations_until_rooted[found_root] = i
		not_already_at_root = np.invert(found_root) & not_already_at_root

	return iterations_until_rooted
```

For $z^3-1$ the pattern in the complex plane of areas of slow convergence is similar but not identical to that observed for Newton's method (see above)

![Halley]({{https://blbadger.github.io}}/newton-method/halley_x^3-1.png)

For larger polynomials, areas of slow convergence are exhibited.  For $z^{13}-z-1$, 

![Halley]({{https://blbadger.github.io}}/newton-method/halley_x^13-x-1.png)

and incrementing $z^{13}-z-1$ to $z^{14}-z-1$, 

{% include youtube.html id='9xl0BWtcc1Y' %}

And similar shapes to that found for Newton's method are also present.  For $z^{9.067}-z-1$, 

![Halley]({{https://blbadger.github.io}}/newton-method/halley_x^9.067-x-1.png)

and incrementing $z^7-z-1$ to $z^8-z-1$,

{% include youtube.html id='-Kbq3EJploo' %}


## Puzzles

### Connect Four winner decider

![connect four]({{https://blbadger.github.io}}/assets/images/connect_4.png)

Let's take a series of moves and determine a winner for the classic turn-based game Connect Four. This was one of my favorite games growing up!  It is played by dropping colored disks into a vertical 7 x 6 board such that each disk played rests on the disks (or base) below.  The winner is the first player to connect four disks of their color in a vertical, horizontal, or diagonal row. 

For the list of moves, a series of moves by each player may be represented by a list of strings as follows:
```python
moves_list = ["F_Yellow", "A_Red", "D_Yellow", "D_Red", ... ]
```
Here each player is denoted by the color of their piece, and each move is denoted by the letter 'A' or 'D' etc. of the row that they put their piece into. 

To determine the winner of any given move list, the strategy is simple: add each move to a matrix representing the game board, and check whether any four pieces are connected at each turn.  To start with, define a function that takes in the move list and returns the winner if there is one and write a docstring that tells us what the funciton will do:

```python
def connect_four_winner(moves_list):
    '''A function that takes a list of connect four moves (each
    move in the format of X_color where X is the row of piece addition
    and color is the color of the piece moved) and returns the winning 
    color, or 'Draw' if there is no winner.
    '''
```
The next step is to initialize a board by using a matrix.  The game board is 7 slots wide by 6 slots high, but it turns out that using a bigger board makes finding connected pieces more simple to program.  Here is a 10 x 9 slot list of lists representing a board.  The reason behind making the board larger will become clear in a moment.

```python
    # initialize the board
    board = [[0,0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0,0],
        [0,0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0,0], 
        [0,0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0,0]]
```
Now let's add each move to the board, one by one.  Abitrarily classifying `'Red'` as player 1 and `'Yellow'` as player 2, each move in the list of moves is positioned by determining who moved.
```python
    for move in moves_list:
        if move[2:] == 'Red':
            piece = 1
        else: 
            piece = 2
```
To determine where each player's piece lands, the letter corresponding to the row played is counted and the piece is added to the first open slot (open slots are denoted `'0'`) in that row.  Our board is upside down, being filled top to bottom!  This does not matter for determining the winner, however.  The code above is omitted using `'...'` for clarity.

```python
     for move in moves_list:
     ...
        for i, letter in enumerate('ABCDEFG'):
            if move[0] == letter:
                for k in range(6):
                    if board[k][i] == 0:
                        board[k][i] = piece
                        break
```
Note that pieces are added to the 7x6 slots in the lower left hand corner of the entire 10x9 board.  Now it is clear why this is: after adding each piece to the board as above, it is easy to check if there are four slots of a color occupied in a row, column, or diagonal by iterating accross every position of the 7x6 board as a starting point.  10x9 comes from adding 3 to both width and height of the board, such that there are not index errors as one traverses each position of the real board.  The slots outside the real board remain empty (`'0'`), and so do not influence whether or not a player wins.

```python
     for move in moves_list:
     ...
     ...
         for i in range(6):
            for j in range(7):
            
                # check for a horizontal four
                count = 0
                for k in range(4):
                    if board[i][j+k] == piece: count += 1
                    else: break
                if count == 4:
                    return 'Red' if piece == 1 else 'Yellow'

                # check for a vertical four
                count = 0
                for n in range(4):
                    if board[i+n][j] == piece: count += 1
                    else: break
                if count == 4:
                    return'Red' if piece == 1 else 'Yellow'

                # check for diagonal four right
                count = 0
                for r in range(4):
                    if board[i+r][j+r] == piece: count += 1
                    else: break
                if count == 4:
                    return 'Red' if piece == 1 else 'Yellow'

                # check for diagonal four left
                count = 0
                for s in range(4):
                    if board[i-s+3][j+s] == piece: count += 1
                    else: break
                if count == 4:
                    return 'Red' if piece == 1 else 'Yellow'

    return 'Draw'
```

The trickiest part here is finding backwards diagonals, ie pieces arranged in a \ pattern.  If we start at [0,0], an index error will be thrown because there is no -1 index in this list of lists! Instead, we offset the starting horizontal value by 3 such that all 4 slots tested are within the matrix size.  Looking at a physical board should make it clear why this is a valid way to test for diagonals, even if not all pieces of the 'real' board are being tested.  If there is no winner after all moves are made, `'Draw'` is returned.
```python
      for move in moves_list:
         ...
         ...
                ...
                # check for diagonal four left
                count = 0
                for s in range(4):
                    if board[i-s+3][j+s] == piece: count += 1
                    else: break
                if count == 4:
                    return 'Red' if piece == 1 else 'Yellow'

    return 'Draw'
```

Let's test it out!  The following move list results in a win by Red
```python
moves_list = [
"F_Yellow", "G_Red", "D_Yellow", "C_Red", "A_Yellow", "A_Red", "E_Yellow", "D_Red", "D_Yellow", "F_Red", 
"B_Yellow", "E_Red", "C_Yellow", "D_Red", "F_Yellow", "D_Red", "D_Yellow", "F_Red", "G_Yellow", "C_Red", 
"F_Yellow", "E_Red"]
```
and if we print the board after adding each piece as a matrix by inserting the following into the moves loop:
```python
import pprint
pprint.pprint (board)
```
we see that the final board indeed has a diagonal four for red, in fact it has two!

```python
[[2, 2, 1, 2, 2, 2, 1, 0, 0, 0],
 [1, 0, 2, 1, 1, 1, 2, 0, 0, 0],
 [0, 0, 1, 2, 1, 2, 0, 0, 0, 0],
 [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],
 [0, 0, 0, 1, 0, 2, 0, 0, 0, 0],
 [0, 0, 0, 2, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
```

The full code is available [here](https://github.com/blbadger/miscellaneous-fun-projects/blob/master/connect_four.py)

### Sudoku solver

![sudoku]({{https://blbadger.github.io}}/assets/images/sudoku.png)

Let's write a program to solve a sudoku puzzle!  Sudoku puzzles involve placing numbers 1-9 in squares on a 9x9 grid with some numbers filled in (above) such that every row or column contains one of each digit, and every smaller 3x3 square in bold also contains only one digit (ie no two 9s or two 1s). 

There are a number of advanced strategies that once can use for this puzzle, but if time is not a limiting factor then we can use a very simple method to solve any given puzzle.  The method is guess and check: for each empty grid space, let's add a number between 1 and 9 and see if the puzzle is OK.  If so, we continue on to the next grid but if not we simply increment the number tried (if 1 did not work, we try 2 etc.).  If none of the numbers work, we go backwards and increment the number in the previous grid, and if none of those work we go backwards until a number does work.  This method is often called backtracking, and there are a number of useful animations online for observing how this works.  

The reason this algorithm works is because it potentially tries every combination of digits in the empty boxes, so if there is a possible solution then the solution will eventually be found!  The program we will write here usually runs in a matter of tens to hundreds of milliseconds on challenging puzzles, but can take up to half a minute or more on extremely difficult ones. The sample problem here is challenging, and takes approximately a second to solve.

Let's represent the Sudoku board as a matrix, which we can specify as a list of lists (1-dimensional arrays, to be precise) without having to resort to a library like numpy to make fancy two-dimensional arrays.  Here is the representation of the puzzle above:

```python
puzzle = [
 [0, 0, 4, 0, 0, 1, 7, 0, 0],
 [0, 8, 0, 0, 2, 0, 0, 0, 0],
 [2, 0, 0, 0, 7, 0, 0, 0, 0],
 [0, 0, 0, 5, 0, 3, 0, 7, 8],
 [0, 0, 5, 0, 0, 0, 4, 0, 0],
 [9, 6, 0, 1, 0, 4, 0, 0, 0],
 [0, 0, 0, 0, 5, 0, 0, 0, 9],
 [0, 0, 0, 0, 1, 0, 0, 6, 0],
 [0, 0, 1, 7, 0, 0, 2, 0, 0]]
```

Now is the time to define a function, remembering a doc string that specifies valid inputs and expected outputs:

```python
def solve(puzzle):
	'''A function that takes a list of lists denoting sudoku
	puzzle clues and places to guess (0s) and returns the completed
	puzzle as a list of lists. Expects a solveable puzzle, and will 
	return only solution if multiple exist.
	'''
```

It is helpful to know which positions on the puzzle we need to try numbers, and which positions are given.  With a nested loop, we can make a list named  `ls`  of all coordinates of positions of to-be-found values as follows:

```python
	# make list of all positions to be determined
	ls = []
	for x in range(9):
		for y in range(9):
			if puzzle[x][y] == 0:
				ls.append([x,y])
	
	# call backtracking function (below)
	return backtrack(puzzle, ls)
```
Now let's apply the backtracking algorithm to the puzzle, but only on the positions of values to be found, by indexing over the list `ls`.  One can define another function within the first to do so, which is not absolutely necessary but provides clarity.

```python
def backtrack(puzzle, ls):
	'''Solves the sudoku puzzle by iterating through the 
	positions in ls and entering possible values into 
	the puzzle array.  Backtracking occurs when no entry
	is possible for a given space.
	'''
	i = 0
	while i in range(len(ls)):
		a, b = ls[i]
```

so now (a, b) is set to the coordinates of the first unknown space.  This is a good time to initialize a variable `count` to be 0, which will change to 1 if a digit cannot be inserted at the unknown space.  If there are no legal moves in the first unknown space, the puzzle is not solveable!  The `count` becomes important in subsequent spaces, and signals the need to backtrack: if no digit can be insterted at a space, then count stays 0 and we will add a clause to initiate backtracking if that is the case.  Let's also initiate the variable `c`, which will store the next number to be tested as follows:

```python
   	count = 0

    	if puzzle[a][b] == 0: 
	    	c = 0

	else: 
	   	c = puzzle[a][b]

	c += 1
```

Now come the tests: first the row (which is equal to `puzzle[a]`), then the column (`ls2`), and finally the 3x3 box (`ls3`) are tested to see if `c` is different than every element of these three lists.

```python
	while c < 10:
		if c not in puzzle[a]:
			ls2 = []
			for q in range(9):
				ls2.append(puzzle[q][b])

			if c not in ls2:
				ls3 = []
				x, y = a // 3, b // 3
				for k in range(3*x, 3*x+3):
					for l in range(3*y, 3*y+3):
						ls3.append(puzzle[k][l])

				if c not in ls3:
```

If `c` is a unique element, it is a possible move!  If so, we add it to the puzzle by assigment, increment our variable 'count', and increment the index of the list of coordinates to be solved (`i`) and break out of the while loop.  If any of these tests fail, `c` cannot be a valid move for the position `ls[i]`, so we increment c and continue the loop to test the next larger digit.

```python
						puzzle[a][b] = c
						count += 1
						i += 1
						break

					else: c += 1
				else: c += 1
			else: c += 1
```
If no digit 1-9 is a legal move at the given position the 'count' variable stays 0 and we use this to test whether a backtrack needs to be made.  If so, we return the current position to 0 and decrement the index of the list of coordinates to be solved (`i`).  If all elements of this list have been iterated, we must have filled in a number at all positions so we can return the solved puzzle. Finally we call the intertior function `backtrack()` in the exterior function `solve()` with the arguments of the puzzle and the list of places to fill in.

```python
		if count == 0:
			puzzle[a][b] = 0
			i -= 1
	return puzzle
	
```

Markdown is not good a tracking spaces between code entries, so make sure to line up each if/else and loop The full sudoku solver code is available [here](https://github.com/blbadger/miscellaneous-fun-projects/blob/master/sudoku_solver1.py).

Now let's test the solver!  Pretty printing (each element of a list is given its own line) is helpful here to make the matrix readable, so let's add the input and pprint it).  

```python
# example of an input
puzzle = [
 [0, 0, 4, 0, 0, 1, 7, 0, 0],
 [0, 8, 0, 0, 2, 0, 0, 0, 0],
 [2, 0, 0, 0, 7, 0, 0, 0, 0],
 [0, 0, 0, 5, 0, 3, 0, 7, 8],
 [0, 0, 5, 0, 0, 0, 4, 0, 0],
 [9, 6, 0, 1, 0, 4, 0, 0, 0],
 [0, 0, 0, 0, 5, 0, 0, 0, 9],
 [0, 0, 0, 0, 1, 0, 0, 6, 0],
 [0, 0, 1, 7, 0, 0, 2, 0, 0]]

# example function call
import pprint
pprint.pprint (solve(puzzle))

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[[5, 9, 4, 8, 3, 1, 7, 2, 6],
 [7, 8, 3, 4, 2, 6, 9, 1, 5],
 [2, 1, 6, 9, 7, 5, 8, 3, 4],
 [1, 4, 2, 5, 9, 3, 6, 7, 8],
 [8, 3, 5, 2, 6, 7, 4, 9, 1],
 [9, 6, 7, 1, 8, 4, 3, 5, 2],
 [3, 7, 8, 6, 5, 2, 1, 4, 9],
 [4, 2, 9, 3, 1, 8, 5, 6, 7],
 [6, 5, 1, 7, 4, 9, 2, 8, 3]]
 
[Finished in 0.9s]
```

The output looks good! No 0s remaining and no obvious errors in digit placement. Running this program on pypy for speed, the time is down to 183 ms.

```bash
(base) bbadger@bbadger:~/Desktop$ time pypy sudoku_solver.py

... (matrix shown here) ...

real	0m0.183s
user	0m0.144s
sys	0m0.021s
```

### Battleship placement validator

![connect four]({{https://blbadger.github.io}}/assets/images/battleship.png)

Say you are playing battleship the old fashioned way: with paper and a drawn 10x10 grid.  You mark the positions of your ships with '1's, but after doing so wonder if you made a mistake.  Is your battleship field valid according to the rules that ships may be touching each other, and ships comprise of one carrier size 4 square, two battleships size 3, three destroyers size 2, and four subs of size 1?

We can represent the battleship grid with a list of lists corresponding to a matrix:

```python
field = [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
         [1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
         [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
         [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
         [1, 1, 0, 0, 0, 0, 0, 0, 1, 0],
         [1, 1, 0, 0, 1, 1, 1, 0, 0, 0],
         [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
```

How do we go about determining if this is a valid field? A simple test that comes to mind is to count the number of positions and determine if this number exceeds or falls short of what it should be (20), which can be accomplished by nesting two loops and incrementing a counter for each position with a ship (denoted `1`).

```python
    # preliminary check for the correct number of spaces
    count = 0
    for lst in field:
        for element in lst:
            if element == 1:
                count += 1
    if count != 20:
        return False
```

But for all the combinations of placements where there are the correct number of positions marked, how do we tell which ones are valid and which are not?

Suppose there was only one ship, a carrier.  Then the problem is easier, and it is a simple task to see if the field is valid: simply loop over each element of the field (left to right and top to bottom is the order for two nested loops) and for the first `1`, see if there are three other `1`s to the right or below.  The trailing `1`s will not be above or behind because then we would not be observing the first `1`!  If there are indeed three `1`s behind or below the first but no more `1`s anywhere else, we have a valid board and vice versa.  

With two ships this becomes more difficult.  To which ship does the first `1` belong to? If we ignore this question and assume that the `1` belongs to one particular type of ship, we may mistakenly classify an invalid board as valid (or the opposite) because ships can be touching. But it is impossible to know which `1` belongs to which ship, so how do we proceed?

Making the problem smaller certainly helped, and this is a sign that recursion is the way forward with this problem.  Using recursion, we can start with a grid with many `1`s and eliminate them as we guess which pieces belong to which ship.  We work around the problem of not knowing which ship is associated with each `1` by simply trying every option possible, and if any are valid then we have a valid board setup!

Let's define a function and attach a docstring.  A list of ships will be useful, so we define a list `ships` using the ship size as the names for each ship.  As we will be checking each position to determine if it is a part of the ship, it is helpful to add `0`s along the right and lower borders of our matrix such that we avoid index errors while checking all `1`s.  Here we include a layer of `0`s along the upper and left borders of the matrix for visual clarity.  Our preliminary check for the correct number of ship spots is added here too.

```python
def validate_battlefield(field):
	'''A function that takes a 10x10 list of lists corresponding to a 
	battleship board as an argument and returns True if the field is
	a legal arrangement of ships or False if not.  Ships are standard
	(1 carrier size 4, 2 battleships size 3, 3 destroyers size 2, 4 subs
	size 1) and ships may be touching each other.  Only the location of 
	hits are known, not the identity of each hit.  
	'''
	ships = [4, 3, 3, 2, 2, 2, 1, 1, 1, 1]
	import pprint

	# make a wrapper for the field for ease of iteration
	for i in field:
		i.insert(0, 0)
		i.append(0)
	field.insert(0, [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
	field.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

	# preliminary check for the correct number of spaces
	count = 0
	for lst in field:
		for element in lst:
			if element == 1:
				count += 1
	if count != 20:
		return False
	# call a validate function on the prepared field
	return validate(field, ships)
```

Now comes the recursion.  To avoid re-wrapping the field with `0`s and re-checking to see if the correct number of spots are filled, we can define another function in this larger one and simply call this one.  The strategy will be iterate along the original field until we see a `1`, and then test if it can be the largest ship in the vertical direction.  If so, we remove all `1`s corresponding to this ship on the field, remove the largest ship from the list `ships`, and copy the ships list (to avoid errors during recursion).  

We then test whether this copied list is valid by calling the `validate` function on this list with the copied ships `ships2`, which causes the function `validate` to be called over and over until either the list `ships` has been exhausted, or until there are no other options for piece placement.  If the former is true, the board is valid and the function returns `True`, otherwise `False` is returned up the stack.

When `False` reaches the first `if validate()` statement, it causes the program to skip to `else`, where we add back the ship we earlier erased from the board.  Otherwise many invalid boards will be classified as valid!  The horizontal direction must also be considered at each stage, so the code above is repeated but for check along each list. At the end, a last check for any remaining ships is made, and this function is called inside `validate_battlefield()`. 

```python
def validate(field, ships):
	'''Assesses the validity of a given field (field), with a 
	list of ships that are yet to be accounted for (ships).  The
	method is to remove ships, largest to smallest, using recursion.
	Returns a boolean corresponding to the possibility of the field
	being legal or not.
	'''
	for i in range(12):
		for j in range(12):
			if field[i][j] == 1:

				k = 0
				while field[i+k][j] == 1:
					k += 1

					if k == ships[0]:
						del ships[0]

						for m in range(k):
							field[i+m][j] = 0
						
						if len(ships) == 0: 
							y = 1
							return True

						ships2 = ships[:]
						if validate(field, ships2):
							return True
						
						else:
							for x in range(k):
								field[i+x][j] = 1
							ships = [k] + ships
						
				w = 0
				while field[i][j+w] == 1:
					w += 1

					if w == ships[0]:
						del ships[0]

						for n in range(w):
							field[i][j+n] = 0
			
						if len(ships)==0:
							y = 1
							return True

						ships3 = ships[:]

						if validate(field, ships3):
							return True

						else:
							for n in range(w):
								field[i][j+n] = 1
							ships = [w] + ships
						
	if len(ships)==0:
		return True
	else:
		return False
```

Let's test this out! With the battlefield shown above, we get

```python
# example function call
print (validate_battlefield(field))
~~~~~~~~~~~~~~~~~~~~~~~~
True
[Finished in 0.0s]
```

Manually trying to put ships in place should convince you that the field is indeed valid! 
## Quantum mechanics 

Delayed choice, nonlocality, and more observations from small objects are considered here.  This commentary should be considered as provisional, to be taken with a grain of salt.

### Wave behavior as nonlinearity

Additivity is necessary for linearity.  If bb-gun is fired through two slits onto a wall, the distribution of probabilities that particular areas on the wall that are hit by bbs that pass through the first slit $P_1$ and the second slit $P_2$ is 

$$
P_{12} = P_1 + P_2
$$

or in other words, the slits are additive.  But now consider waves passing through these same slits. In this case, 

$$
P_{12} \neq P_1 + P_2
$$

meaning that adding a second slit to the first is not additive.  More precisely, the amplitude of a wave traveling through the first slit, denoted as the complex number $h_1$, is the real part of $h_1e^{i \omega t}$.  The intensity distribution along the wall with only slit 1 open is $I_1 = \vert I_1 \vert^2$ and only slit 2 open is $I_2 = \vert h_2 \vert^2$, but the intensity distribution with both slits open is

$$
I_{12} = \vert h_1 + h_2 \vert^2 \neq  \vert h_1 \vert^2 + \vert h_2 \vert^2 \\
I_{12} = I_1 + I_2 + 2\sqrt{I_1I_2}cos \delta
$$

A fundamental feature of quantum mechanics is that when path is known, linear particle behavior is observed and $P_{12} = P_1 + P_2$ but when path is unknown, nonlinear wave behavior is observed and $P_{12} = P_1 + P_2 + 2\sqrt{P_1P_2}cos \delta$.  

Quantum mechanics from first principles extends no further: there is no reason why observation of path leads to particle-like behavior and a lack of path observation leads to wave-like behavior. If it is accepted that small particles are by nature probablistic, this is as far as one can go.  But the similarities of quantum behavior to aperiodic dynamical systems abound (inherently unpredictable future from a finitely precise past, necessity for nonlinear transformations etc.) leading to the possibility that one can better explain first principles of quantum mechanics with the principles of nonlinear dynamics.

### Nonlocality implies a nonlinear space: the observer effect

'Spooky action at a distance', or what is now called quantum entanglement, is the phenomenon why which two particles interact such that in a future time at great distance, what happens to one affects what happens to the other (specifically with regards to quantum spin).  There is no intrinsic reasoning capable of explaining such behavior using vector (linear) space, but if one considers a nonlinear space then the result is implied.  In a space defined by a nonlinear function, a change to one area in the space changes other areas because nonlinear transformations are not additive.  Manipulating a particle (or simply observing it, as by the observer effect these are identical) equivalent to changing a parameter in a nonlinear transformation, which changes all areas in the resulting space simultaneously.

### Renormalization and scale

As noted by Feynmann, quantum particle paths are typically non-differentiable.  Equivalently, quantum paths exist in a fractal dimension greater than 1.  This means that the length of the particle path is infinite, because it increases (apparently indefinitely) as the observation resolution increases.  

This is not the only place where infinite quantities intrude in the study of small objects: the process of calculating field strength for quantum particles, for example, is rife with infinite quantities.  To deal with both issues, the theory of quantum renormalization was introduced to remove such infinities, allowing for calculation to proceed safely.

A necessity for renormalization as well as non-differentiable particle paths imply and are implied by scale invariance.  Scale invariance in turn is typical of nonlinear dynamical systems, and thus it is not a stretch of the imagination to propose that the mechanics of small objects are most accurately defined in terms of nonlinear dynamics. As the Schrodinger equation is a linear partial differential equation, the current formulation is either incomplete or, as is more likely, a linear approximation.

Why would field strength need to be renormalized in the first place?  Consider that in many of the pages on this site, maps between dimensions require explosion to infinity (for example, notice how any plane-filling curve must become infinitely long to cover any finite area).  Now fields are three dimensional, whereas if one takes a particle at face value then it is a one dimensional object.  Generation of the field requires passage between dimensions, specifically from a 0-dimensional point to an 3-dimensional (or 4-dimensional if relativity is taken into account) volume.  Only nonlinear transformations are capable of mapping a 0-dimensional point to a 3-dimensional volume, and 

### Implications 

The above considerations have a number of consequences.  Firstly, they suggest that many questions of particle path and field strength are inherently undecidable for the following reason: aperiodic nonlinear dynamical systems require an infinite number of logical operations to predict an arbitrarily small change.  Such systems are inherently uncomputable and therefore to some extent so are the questions of path and field.  

Some of this has long been recognized (although for different reasons than stated here), and similar concerns prompted the famous comment from Feynman, stated as follows: "Why should it take an infinite amount of logic to figure out what one stinky little bit of space-time is going to do?".  But it should be recognized that the implications continue past this issue.  The presence of an apparent continuum of scale in the physical world (which prompted the above comment) means that physical theories attempt not to predict what will happen to a starting condition, but what approximately will happen to an approximate start.  

But if physical theories at all scales behave according to nonlinear transformations that are aperiodic, then arbitrarily small changes to starting values lead to unpredictable predictions.  What this means is that the accuracy for predicting such attributes as bond angle from orbitals and spin changing etc. rely on a nonlinear system to be periodic. 

To see just how unlikely it is that a nonlinear physical system is periodic for more than two objects, consider the classical [three-body problem](https://blbadger.github.io/3-body-problem.html) for celestial mechanics.  Nearly all possible orientations of three bodies in space yield unpredictable outputs (if bounded) from nonlinear gravitational partial differential equations.  The inability for current quantum mechanical systems to predict events with more than three objects is implied by nonlinearity.  

Furthermore, the similarity between nondifferentiable motion in quantum particles and that observed for Brownian motion (see [here](https://blbadger.github.io/additivity-order.html) for more on Brownian motion) naively suggests a similar physical cause.  This implies that just as the particles undergoing Brownian motion are far larger than the water molecules causing this motion, there may well be particles much smaller than currently observed that buffet about the quantum bodies that are observed.  In this sense, there may be no quantum scale at all, merely a relatively quantum scale for our observations because size continues to shrink indefinitely.

If this seems fantastic, consider photons from a point source traveling through a vacuum to land on a photographic plate.  The resolution limit of a blue photon is on the order of 200nm, meaning that the photons land in a blob of about this diameter called an airy disk.  Why do the photons not simply make a point on the plate?  The current account for such behavior is the wave nature of light, and of all small particles.  But observe that the intensity of the central region is a normal distribution, precisely the same as observed for a collection of particles in a fluid undergoing [Brownian motion](https://en.wikipedia.org/wiki/Brownian_motion).  The longer the wavelength of the light, the larger the blob and equivalently the longer the particles diffuse in fluid.  Flight length of a photon has no effect on distribution width because photons experience infinite time dilation.

### Example: electron orbitals and period doubling

A concrete example for how nonlinearity could be important for quantum mechanical systems is in atomic orbitals.  Consider that larger energies are required to attain each orbital in succession s, p, d, f and that the maximum number of locations (physically speaking, the distinct clouds) an electron can exist in for a given m value is, respectively, 1, 2, 4, 8.  Whether or not the period doubling here follows Feigenbaum's $\delta$ is unclear.

Now note that electron orbitals may be considered as the periodicity of electrons: one cloud corresponds to period one etc. Period doubling is typical of nonlinear systems upon an increase in energy, and therefore observed orbital geometry is consistent with period doubling.  If an electron absorbs enough energy, it is discharged from an orbital and travels freely, and most accurately in a non-differentiable path.  This is an aperiodic path with a fractal dimension greater than one, meaning that the process of energy addition to a bound electron (eventually freeing the particle) is consistent with that for a nonlinear attractor.  

### The uncertainty principle

The uncertainty principle states that it is impossible to know a particle's exact momentum as well as position, as the standard deviation in position $\sigma_x$ and standard deviation in momentum $\sigma_p$ are related to Planck's constant $h$
by 

$$
\sigma_x \sigma_p \geq \frac{h}{4 \pi}
$$

which follows from the wave-like nature of small objects, and also applies to macroscopic waves.  Equivalently, one cannot define a unique wavefunction for a very short waveform.  As precision in location increases as the waveform length decreases, necessarily there is a limit to the precision of the accuracy of location and momentum (which is defined by the wavefunction).

But note that this only applies to predictions of the future, not observations in the past.  A photon passing through a double slit, for example, can be determined precisely in both location and momentum after the fact, that is, after it is no longer a wave.  

The result is that small objects (which have little mass) resemble 3-dimensional probability clouds rather than one-dimensional points.  But this is only true for future predictions rather than events that happened in the past, which implies a transformation from a 0-dimensional

### Aside: Delayed choice

The celebrated models of this study such as Schrodinger's equation imply a particle-wave duality of such objects.  Such a duality was observed upon experimentation with light, which in some experimental apparatus behave as though they were particles and in other experiments behave as though they were waves.  An example of the former case is the excitation of electrons to discrete energy levels, and an example of the latter is the dual slit experiment in which a photon interferes with itself, as a macroscopic wave does, when travelling through two closely spaced slits.  Wave-like or particle-like behavior may be observed from objects such as photons or electrons (or even much larger things) but never both at the same time: this is the complementarity principle.

Wheeler devised a thought experiment, now known as the delayed choice, to test complementarity.  It proceeds as follows: suppose gravitational lensing were used to make a cosmic inferometer, which is a method for telling how far away something is.  With that information, it is possible to know the path a photon took as it travelled between galaxies.  Wheeler's hypothesis was that as a photon is neither a particle nor a wave until observed, but instead something indeterminate, then the act of observing the path choice would preclude a wave-like behavior manifest by

This experiment has been carried out, though not as Wheeler initially proposed.  In various experiments, it has been shown that the method used to observe a photon at the end of its path through a double slit with inferometer determines its wave or particle behavior in the entire apparatus, even through the double slit itself.  

One way to think about this is to try to understand when the photon 'learns' the experimental apparatus.  Now note that the Lorenz time dilation for a photon is infinite because it travels at the speed of light:

$$
\Delta t' = \frac{\Delta t}{1-\frac{v^2}{c^2}} \implies \\
\Delta t' \to \infty \; as \; v \to c 
$$

which means that from the photon's perspective, all events are simultaneous.  

If this is accepted, the question of when the photon learns the apparatus is moot because the ending time is equal to the start time.  A change in the method of recording the photon at the end of a photon flight can change its behavior through a double slit 20ns beforehand (in our reference).  Delayed choice and quantum erasure would be more accurately thought of as a necessary result of simultaneity from a photon's perspective.  This idea also accounts for the finding that a photon can be influenced after detection, ie after it is destroyed.

### Principle of least time as implied by delayed choice

Light travels at different speeds throught different materials.  For example, light travels through water (2.25E8 m/s)  slower than it does in vacuum (3E8 m/s). Fermat noted that light travels along paths in different media of refraction as if it chose the quickest way to get from point A to point B.  Suppose that a new transparent material were invented and light was passed through it. When does light 'learn' what angle it should deflect to in order to minimize the time passing through this object? 










# blbadger.github.io
A collection of pages on diverse projects.  

## Masked Mixer Language Models

For the findings on this page written in more detail as an academic paper, see [here](https://arxiv.org/abs/2409.01482).

### Background

The training of the most effective language models today (3/2024) requires enormous computational resources: a whopping 1720320 hours of 80GB nvidia A100 compute time was required to train the 70 billion parameter version of [Llama 2](https://arxiv.org/pdf/2307.09288.pdf).

This prohibitive amount of compute power required is mostly down to the very large effective size of the models that are currently trained, rather the training dataset itself: most LLMs are trained on between 1 and 5 trillion tokens of text, but this is not actually that much information when one considers that each token is typically expressed as bytecode (ie one byte) and therefore the training dataset is a few terabytes, substantially smaller than the image datasets required to train smaller diffusion models.

Current state-of-the-art transformer models are very large indeed (>100 billion parameters) but with transformers there is another wrinkle: the key, query, and value projection weights form gradients for all of a training input's sequence.  This means that a 7 billion parameter Llama model will actually require much more space than the 7 billion params * 2 bytes per param in fp16, or 14 gibabytes one might think (if training in 16 bit precision, ignoring extra memory incurred by optimizers for the moment), and thus has an 'effective' parameter size much larger than the model's actual size during inference. Together this means that it takes hundreds of gigabytes of vRAM to train a model on a small context window and with a small batch size, even though the same model may require only 14 gigabytes of memory during inference.  This extra memory is due to the necessity of storing gradients on and values of attention projection weight matricies during training, making the space complexity for $n$ sequence elements $O(n^3)$ compared to the $O(n^2$) space required during inference where these values may be cached.

This begs the question: are all these parameters necessary? It is clear that transformer-based models do indeed become substantially more effective with an increase in the number of parameters they contain, but if we were not restricted to one particular architecture is it possible that we could design a model with far fewer parameters for language modeling?

By one estimate the billions of parameters are far more than would be necessary to model the English language. First consider language generation at the level of sentence completion, where the task is to simply finish a sentence. It has been claimed that there are somewhere around $m = 10^{570}$ possible English sentences as an upper bound. Without knowing how to model these sentences, we can view them as unique points in an arbitrarily high-dimension space. This being the case, we can apply a result from the concentration of measure phenomenon to greatly simplify this space.

We will make use of the Johnson-Lindenstrauss lemma, with the result that the same $m$ points may be represented with arbitrary precision in a space that is on the order of $8 \log m = 8 \ln 10^{570} \approx 1312 $ dimensional. More precisely, this lemma states that for some small $\epsilon > 0$ for set $X$ of $m$ points in $\Bbb R^N$, for 

$$
n > 8\ln (m) / \epsilon^2
$$

there is a (linear) representation map $f: \Bbb R^N \to \Bbb R^n$ where for all $u, v \in X$ the following is true:

$$
(1 - \epsilon) ||u - v||^2 \leq ||f(u) - f(v)||^2 \leq (1 + \epsilon)||u - v ||^2
$$

Alternatively, language models today are universally trained to predict every 'next' token in a concatenated sequence of tokens. This means that for a training dataset of $n$ tokens, the model is trained to predict $n$ points (ignoring exactly how the sequences are concatenated and trained upon concatenation) in some presumably very high-dimensional space. The largest dataset any open-source model has been trained on is 15 trillion tokens (for llama-3), and in this case we can represent this dataset arbitrarily well in a space of approximately $8 \ln (15 \times 10^{12}) \approx 8 * 30 = 240$ dimensional. Of course, the goal of language models is to generalize to larger datasets and thus we would hope the model would accurately predict the next tokens of a much larger dataset. But even supposing that this 15 trillion token dataset is only one millionth of the size of this generalized dataset, one would still only require a space of $8 \ln (15 \times 10^{18}) \approx 8 * 44 = 352$ dimensions.

How does this relate to the number of parameters necessary in a model? Assuming that the training algorithm is sufficiently powerful (which is not at all a safe assumption, more on this later) the number of parameters in a model could correspond to one of two things: either the number of parameters in a model is equivalent to the dimensionality of the model with respect to the points it can approximate, or else the model's 'width' or hidden layer dimension is equivalent to the model's dimensionality. Because the vector space of successive layers of practically every modern deep learning model are dependent (activations in layer $n+1$ depend on layer $n$ and perhaps layer $n-1$ etc.) and dimensionality is defined on linear independence, it seems more likely that a model's dimensionality best corresponds with its hidden layer dimension. If this is true then a model of no more than around 1300 hidden layer nodes should be capable of completing any English language sentence, or a model with a width of 350 can accurately predict a next token for a massive dataset. If these hidden widths were used for standard transformer model architectures, the resulting models would are around 100 million (350 hidden width) and 995 million (1300 hidden width) parameters.

It cannot necessarily be assumed that a model with that number of parameters is actually trainable, however: it could be that training requires a large model that must then be converted into a small model.  This is the approach used when performing pruning, where parameters are dropped depending on their importance for some output. Alternatively, instead of removing parameters one could reduce the memory required to store each parameters: this is the approach of quantization methods, which are perhaps the most effective methods currently available for shrinking the effective size of a model. The observation that weight quantization rather than pruning is the most effective method for reducing a transformer model's effective size suggests that this particular architecture may indeed require nearly all the trained parameters in order to function effectively, although whether this is the case or not remains an open questions. 

Here we take the approach of investigating new architectures and training new models, rather than attempting to extract the most information possible from existing models.

### Introduction

The goal of any deep learning architectural choice is fundamentally one of efficiency: as it has long been known that even a simple one-hidden-layer fully connected nerual network is capable of approximating any arbitrary function, although not necessarily capable of *learning* to approximate any function.  Empirical observation over the last decade suggests that indeed if one is given enough compute power, most architectural choices may result in the same quality of output if sufficient numerical stabilization and dataset size are used (and the model follows some general rules, such as exhibiting sufficient depth).  If one were given an unlimited computing budget with unlimited data, then, a model's architecture is unimportant.

But for real-world scenarios where compute is limited, the choice of a model's architecture becomes very important. An example of this is the introduction of exclusively transformer-based architectures to image categorization, where these models were able to achieve performance on par with convolutional models but required vastly more compute power to do so and were thus of dubious practical significance for that task (although they are indeed practically useful for generative image modeling).

With this in mind, what we will seek is a model architecture that is more efficient than the transformer with respect to training: given some fixed language dataset and compute budget, we want a model that is more effective (ie reaches a lower validation loss and generates more cohesive language) than a transformer-based state-of-the-art model.

In scientific endeavors it is often useful to begin with small-scale experiments when testing basic ideas, before moving to larger-scale ones for further experimentation. In this spirit (and because of the author's limited compute) we will test small language models (millions rather than billions of parameters) on small datasets (megabytes rather than terabytes).

The experimental setup will be as follows: for our dataset we will start with TinyStories, which is a collection of simple text that contains a limited vocabulary similar to what a four-year-old would use that allows for effective modeling by transformers in the millions rather than billions of parameter scale. For reference, small versions of state-of-the-art transformer models (based on Meta's Llama model) will be trained and used as a comparison to the new architectures that we will try here.

### Language Mixer Basics

[In other work](https://blbadger.github.io/language-discreteness.html) it was observed that transformers exhibit a somewhat unexpected phenomena: firstly that transformer blocks must be relatively high-dimensional (embedding size $d_{model} > 3000$) in order to have any accurate input representation ability, and secondly that the ability of a transformer to accurately represent a token it has seen previously (ie a 'non-self' token) disappears after training.  On the other hand, a modification of the MLP mixer architecture was found to have accurate self- and nonself- token representation even from very small models with $e < 100$ (nonself representation is only accurate if expansions are not used in the convolutional layers, see below for more information). Thus this may be a good candidate with which to start the process of looking for more effective architectures than the transformer, an architecture that although effective has inefficiencies acknowledged by the authors of the original 'Attention is all you need' paper (GTC 2024).

The MLP mixer architecture is conceptually similar to a transformer if all the multi-head attention layers were replaced with one-dimensional convolutoins over the sequence dimension. The mixer was originally designed for vision tasks, and we will test modifications of this architecture for language.  

The mixer has previously been applied only to vision modeling, where it was found to be not quite as efficient as a transformer of equivalent 'size' for a fixed dataset (the only instance of a mixer applied to language modeling tasks is a nanoscale truncated mixer applied to bloom filtered text that has obvious unsuitibilities as a generative model).  It is important to observe, however, that with language one is typically not bound by a dataset's size but rather the amount of compute one can bring to that dataset, and the efficiency of the models used. 

First, we define the operations on one mixer block, which is a module akin to one transformer block. The 1-dimensional convolutions that replace self-attention may be visualized as follows:

![mixer]({{https://blbadger.github.io}}deep-learning/llm_mixer.png)


The kwarg `expand_conv` allows us to use this expansion or forego it for a single convolution (that must have as many output features as sequence elements). This is initialized as follows:

```python
class MixerBlock(nn.Module):

	def __init__(self, dim, length, mixer_mask=True, expand_conv=True):
		super().__init__()
		self.patch_layernorm = nn.LayerNorm(dim)
		self.seq_layernorm = nn.LayerNorm(dim)
		self.dim = dim
		self.length = length
		self.patch_ff = FeedForward(dim)
		self.expand_conv = expand_conv
		if self.expand_conv:
			self.conv = ConvForward(length)
		else:
			self.conv = nn.Conv1d(length, length, 1)
		
		# for CLM training, apply lower triangular mask to convolution weights during the forward pass
		self.mixer_mask = mixer_mask
```

The last line of the code block above passes the boolean `mixer_mask` to a class variable to be accessed during the forward pass of the `MixerBlock`.  What exactly this mask will look like will depend on whether we expand the inter-token convolution transformations or not, by which is meant mirroring the two-layer linear transformation (with nonlinear activation) that define the `FeedForward` modules. The similarities may be shown as follows:

```python
def FeedForward(dim, expansion_factor=4):
	inner_dim = int(dim * expansion_factor)
	return nn.Sequential(
		nn.Linear(dim, inner_dim),
		nn.GELU(),
		nn.Linear(inner_dim, dim)
	)

def ConvForward(dim, expansion_factor=2):
	inner_dim = int(dim * expansion_factor)
	return nn.Sequential(
		nn.Conv1d(dim, inner_dim, 1),
		nn.GELU(),
		nn.Conv1d(inner_dim, dim, 1)
		)
```

where the `ConvForward` module is the transformation between tokens.  

We will train the architecture using the masking approach that is commonly applied to causal language models in which the objective of training is to predict the next token in a sequence.  This means that we need to prevent the model from using information from tokens to the right some token when learning to predict that token.

```python
class LanguageMixer(nn.Module):

	def __init__(self, n_vocab, dim, depth, tie_weights=False):
		super().__init__()
		self.wte = nn.Embedding(n_vocab, dim)
		self.mixerblocks = nn.ModuleList(
			[MixerBlock(
				dim = dim,
				length = tokenized_length,
				)
			for i in range(depth)]
			).to(device)
		self.lm_head = nn.Linear(dim, n_vocab, bias=False)
		if tie_weights:
			self.lm_head.weight = self.wte.weight
		self.cel = nn.CrossEntropyLoss()
```


For a recursive neural network model, there is pretty much one way to train the model: for each word in a sequence, get the model's prediction, find the loss, and backpropegate. For transformers and other models we could do the same thing but there is a much more efficient way to train: instead of iterating through all words in an input we instead feed the entire input into the model as if we were going to predict the next word, but instead we take the `lm_head` output for each input, find the loss, and backpropegate the total loss.

There are two important modifications necessary for this more parallelizable training. The first is that we need to have the model compare the output of the model for sequence elements $a_0, a_1, ..., a_{n-1}$ to the element $a_n$ during the loss calculation, meaning that we need to shift the target element $a_n$ such that it is accessed when the model is at position $a_{n-1}$. This is accomplished in the forward pass of the model (below) by shifting the labels (the target elements) to start with the input element at index 1 rather than index 0 in `labels[..., 1:].contiguous()`. The model's outputs are clipped such that the last output (which would correspond to the next element after the end of the input and does not exist in the input itself) is omitted. For compatibility with the HuggingFace `trainer`, we compute the cross-entropy loss in the forward pass and supply the value as part of a tuple with the output itself.

The line `labels = rearrange(labels, 'b p t -> b (p t)')` in the forward pass may be unfamiliar with those who have not worked with vision models in the past. For whatever reason, Einstein sum tensor manipulation never became as popular in the language modeling world as for vision models. There are certainly pros (succinct notation) and cons (portability) to using `einsum` notation, but we will use `einsum` mostly for reshaping tensors. For example, `labels = rearrange(labels, 'b p t -> b (p t)')` simply removes an index dimension of our tensor between the `batch` and `token` dimensions and could also be accomplished with `labels = torch.squeeze(labels, dim=1)` but is arguably more expressive.

```python
class LanguageMixer(nn.Module):
	...
	def forward(self, input_ids, labels=None):
		x = input_ids
		x = x.to(device)
		x = self.wte(x)
		for block in self.mixerblocks:
			x = block(x)
		output = self.lm_head(x)
		labels = rearrange(labels, 'b p t -> b (p t)')
		output = rearrange(output, 'b t e -> b e t')
		shift_logits = output[..., :-1].contiguous()
		shift_labels = labels[..., 1:].contiguous()
		loss = self.cel(shift_logits, shift_labels)
		return loss, output
```

Besides shifting the output, we need a second addendum for causal language modeling: only information from previous tokens $a_0, a_1, ..., a_{n-1}$ must reach token $a_n$ and not information from succeeding tokens $a_{n+1}, a_{n+2}, ...$. For the transformer architecture it is customary to mask the softmax values of the $KQ^T$ to only use information from query projections of past tokens, but as we are using a 1-dimensional convolution transformation with no softmax a different approach will be necessary.

Instead we shall mask the weights of the convolution such that the only non-zero weights supplied to $a_n$ will originate from $a_0, a_1, ..., a_{n-1}$. How this may be done is easier to see if we look at only one convolution between token: given an input matrix $X \in \Bbb R^{m, n}$ with $m=3$ tokens and $n=2$ features per token, 

$$
X = 
\begin{bmatrix}
x_{0, 0} & x_{0, 1}\\
x_{1, 0} & x_{1, 1}\\
x_{2, 0} & x_{2, 1}\\
\end{bmatrix}
$$

if we are given convolution weights from a single filter layer

$$
W_0 = 
\begin{bmatrix}
2\\
1\\
0\\
\end{bmatrix}
$$

we get the output (ie one token)

$$
X * W_0 = \\
\\
\begin{bmatrix}
2x_{0, 0}+1x_{1, 0}+0x_{2, 0} & 2x_{0, 1}+1x_{1, 1}+0x_{2, 1}\\
\end{bmatrix}
$$

Likewise, with two convolutional feature weight layers we perform the same operation with the second to recieve a 2x2 output and for three we have a 3x2 output. If we concatenate the weight layers together in a single matrix such that each column weight becomes a matrix column, we want to use an upper triangular mask: in this case, the convolutional weight matrix $W$

$$
W = 
\begin{bmatrix}
2 & 1 & 1\\
1 & 1 & 4\\
1 & 3 & 1\\
\end{bmatrix}
$$

becomes the masked weight matrix $m(W)$ upon Hadamard multiplication to an upper-triangular mask matrix $U$,

$$
m(W) = W \circ U
\\
m(W) = 
\begin{bmatrix}
2 & 1 & 1\\
1 & 1 & 4\\
1 & 3 & 1\\
\end{bmatrix}

\circ

\begin{bmatrix}
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1\\
\end{bmatrix}

= 

\begin{bmatrix}
2 & 1 & 1\\
0 & 1 & 4\\
0 & 0 & 1\\
\end{bmatrix}
$$

such that now for the first token we have the output

$$
X * m(W) =  \\

\begin{bmatrix}
x_{0, 0} & x_{0, 1}\\
x_{1, 0} & x_{1, 1}\\
x_{2, 0} & x_{2, 1}\\
\end{bmatrix}

*

\begin{bmatrix}
2 & 1 & 1\\
0 & 1 & 4\\
0 & 0 & 1\\
\end{bmatrix} 

\\
\\
= 
\\
\\

\begin{bmatrix}
2x_{0, 0}+0x_{1, 0}+0x_{2, 0} & 2x_{0, 1}+0x_{1, 1}+0x_{2, 1}\\
1x_{0, 0}+1x_{1, 0}+0x_{2, 0} & 1x_{0, 1}+1x_{1, 1}+0x_{2, 1}\\
1x_{0, 0}+4x_{1, 0}+1x_{2, 0} & 1x_{0, 1}+4x_{1, 1}+1x_{2, 1}\\
\end{bmatrix}
$$

which is what we want, as each token recieves non-zero weights from preceeding (after shifting) tokens only.

In our implementation we actually use a lower-triangular mask (`tril`) because we must first re-arrange each convolutional weight tensor into a single weight matrix before Hadamard multiplication to the mask, and by default our rearrangement places each convolution weight column as a row in our collected matrix, ie it is transposed.

```python
rearranged_shape = rearrange(self.conv.weight, 'f d p -> f (d p)').shape
mask = torch.tril(torch.ones(rearranged_shape)).to(device)
applied_mask = rearrange(self.conv.weight, 'f d p -> f (d p)') * mask # Hadamard mult to mask
self.conv.weight.data = rearrange(applied_mask, 'f (d p) -> f d p', p=1)
```

Thus we modify each 1D convolution in the mixer such that the convolutional weight is lower-triangular and perform the same operation as before, 

![masked mixer]({{https://blbadger.github.io}}/deep-learning/masked_llm_mixer.png)

We make it optional to use the original mixer architecture (where two 1D convolutions are applied sequentially between each pair of sequence elements) via the kwarg `expand_conv`, and for that we actually need to perform the reshaping and masking for both convolutions.  The architecture with only one convolution between sequence elements we call the 'flat' mixer, as it must have a fixed number of convolutions to sequence length elements.  The mask is applied during the forward pass as follows:

```python
class MixerBlock(nn.Module):

	def __init__(self, dim, length, mixer_mask=True, expand_conv=True):
		super().__init__()
		...

	def forward(self, x: torch.tensor):
		if x.dim() > 3:
			x = rearrange(x, 'b p t f -> (b p) t f')

		# for CLM training, apply lower triangular mask to convolution weights
		if self.mixer_mask:
			# Mask logic here
			...
		residual = x
		x = self.seq_layernorm(x)
		x = self.conv(x) + residual
		residual = x
		x = self.patch_layernorm(x)
		x = self.patch_ff(x) + residual
		return x
```

Note that it is tempting to skip a step and simply apply the triangular mask directly to the convolution weights by re-assigning the parameters of those weights to masked values of the original weights. This leads to a tricky problem after backpropegation: the original weights will not be updated! The optimizer (here AdamW) takes as an argument the model as it is initialized, but the parameters after masking during the forward pass are newly initialized at this time and will not be recognized by the optimizer.

Finally, we use a dataset amenable to small models: the TinyStories dataset, which we truncate to 2M 'stories'. A Llama-style tokenizer with 4096 unique tokens was trained on this dataset and used for both transformer and mixer models during training and inference.

### Training

We make use of the `transformers.trainer()` module, which has a couple very useful features for ease of use: automatic logging checkpointing the model and optimizer states, masking loss on the pad token, 16 bit mixed precision numerical stabilization to name a few. For testing purposes one can also used the following barebones trainer (note that this does not mask pad token loss and should not be used for an actual training run):

```python
def train_model():
	model.train()
	total_loss = 0
	for step, batch in enumerate(train_data):
		batch = rearrange(batch, 'b (p t) -> b p t', p=1)
		optimizer.zero_grad()
		batch = batch.to(device) # discard class labels
		loss, output = model(batch, batch)
		total_loss += loss.item()
		loss.backward()
		optimizer.step()
	print ('Average loss: ', total_loss / len(batch))
```

To make results comparable, we use the same tokenizer, dataset (TinyStories), and batch size (16) and observe the training and evaluation cross entropy loss for the transformer model (Llama, whose source code may be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)) compared to our model. This new model is dubbed the 'masked mixer', both because the model masks convolutional weights during both training and inference and because some versions of this model are not fully-MLP architectures at all but utilize convolutions of size greater than 1 between tokens.  We perform training runs of around 12 hours on an RTX 3060 unless otherwise noted.

It should first be stated that the transformer requires substantially more memory to store the gradients, optimizer, and parameters than the mixer: given a batch size of 16, a llama model with $d_{model}=128$ and $n=8$ exceeds 10 GB vRAM during training compared to the 2.4 GB vRAM required for a mixer of the same $n$ and double the $d_{model}$, both for a context window size of 512.  This is due to inefficient memory usage for that model size (transformers with a width of less than 256 are poorly allocated in memory), but for larger $d_{model}$ values the large amount of memory required stems from the increased number of non-trainable inter-token parameters necessary for backpropegation in the transformer compared to the mixer.

It is also apparent that transformers are much slower to train than mixers. This results from both the number of effective parameters (above) and also from the more efficient use of gradient flow by the mixer, as gradient do not need to pass along non-trainable parameters as is the case for transformers (where attention gradients travel from $K,Q,V$ projections to the $K,Q,V$ values themselves and back as well as softmax transformations etc). Thus we cannot compare these models directly using only $d_{model}$ and $n$, but instead use a ballpark figure for these and compare training and test vRAM.

Because of this, optimizing a mixer of a similar size to a transformer requires much less compute: one typically sees between 10x and 20x the time required for a transformer with $d_{model}=128$ and $n=8$ compared to a mixer with twice the $d_{model}$ and the same number of blocks for the same forward and backpropegations steps. 

Now we train the models using an approximately fixed compute budget (12 GB vRAM and 12 hours on an RTX 3060).  We find that the masked mixer with the parameters detailed above achieves a substantially smaller training (2.169) and validation (2.201) loss (Cross-Entropy loss, equal to $\log_2 \mathrm{perplexity}$) after this twelve hours than the Llama model (2.497 validation and 2.471 training loss).  This is mostly because the mixer is around six times as fast to train as the transformer: there is nearly identical loss if we compare equal steps (2.497 versus 2.505 validation loss at step 160000). Equivalent steps mean little for language models, however, as they are inherently resistant to overfitting (see that there is minimal overfitting after 5.6 epochs at twelve hours of training for the mixer) such that simply increasing the dataset size or number of epochs in this case yeilds lower training and validation loss than training a larger model with an effectively smaller dataset.

### Mixer Inference

Low training and validation loss are useful guides of efficacy, but the goal of this research is to find architectures that are more efficient than the transformer at causal language modeling, which is simply to generate the next word in a sentence.  

It is tempting to simply take the last output of the mixer model as the next token and perform a sliding window on the input to keep a constant number of tokens (which the mixer requires, unlike a transformer model). But there is a problem with this method: the last output is not actually trained at all! Recall that the model used a shifted logits approach, such that the trained output is $[..., :-1]$ which does not include the last element because that is the prediction for the next token after the input ends, information that is absent.

Instead we can have the mixer simply replace token after token, maintaining the causal language mask after the last supplied token as follows:

![masked mixer generation]({{https://blbadger.github.io}}/deep-learning/masked_mixer_gen.png)

For 50 generated tokens at the end of the 512-length context window, this corresponds to the following inference code:

```python
fout = []
for i in range(50, 1, -1):
	loss, output = model(tokens, labels=tokens)
	out_token = torch.topk(output, dim=1, k=1).indices.flatten()[-i]
	tokens[..., -i+1] = out_token
```

while maintaining the `mixer_mask=True` flag on the model.

The outputs are extremely good: for our mixer of size $d_{model}=256$ and $n=8$ blocks (1.8 GB vRAM for 16-bit training on 512 context tokens), trained for 12 hours on TinyStories, we have for the validation dataset input (`<unk>` tokens are newlines)

"""
Once upon a time, there was a little boy named Tim. Tim had a big, orange ball. He loved his ball very much. One day, Tim met a girl named Sue. Sue had a pretty doll. Tim liked Sue's doll, and Sue liked Tim's orange ball.<unk>Tim and Sue thought about a trade. They would trade the ball for the doll. Tim was not sure. He loved his orange ball. Sue said, "I promise to take care of your ball. You can play with it when you
"""

The actual next 50 tokens to this input (generated by ChatGPT) are

"""
want." Tim said, "I promise to take care of your doll too."<unk>They did the trade. Tim played with the doll, and Sue played with the orange ball. But then, Sue's big dog came. The dog saw the orange ball and thought it was a toy. The dog bit the ball and it popped
"""

and our language mixer supplies the following output:

**`
come back."<unk>Tim thought about it and agreed. Sue gave the ball back to Tim. They played together, and Tim was very happy. Sue was happy too. They learned that sharing can make everyone happy. And that is how the orange ball can bring happiness to everyone.
`**

Compare this to the greedy generation output from the Llama transformer model $d_{model}=128$ and $n=8$ blocks (10.7 GB vRAM for 16-bit training on 512 context tokens), trained for 12 hours on TinyStories: 

**`
'll find your ball."<unk>Tim was happy. He said, "Thank you, Sue. You are a good friend. You are a good friend." Sue gave Tim a new ball. Tim gave Sue a new ball. Sush was blue and shiny. Sush, Spot, and Spot. They played with their ball
`**

although this turns out to be a bad prompt for the transformer. A better one is as follows: 

"""
One day, a little boy named Tim went to play with his friend, Sam. They wanted to play a game with a ball. The game was to see who could get the best score.<unk>Tim threw the ball and Sam tried to catch it. Sam missed the ball and it went far away. Sam was angry. He said, "I don't like this game. I want to lie down and rest."<unk>Tim said, "Okay, let's lie down and rest. Then we can try again to get a good score." They lay down under a big tree and looked at the sky. They saw birds flying and clouds moving.<unk>After a while, they got up and
"""

The masked mixer completes this story as follows:

**`
played a game of tag. Tim was fast, but Sam was faster. They ran and ran until they reached the finish line. Tim was tired but happy. Sam said, "Good job, Sam!" Sam smiled and said, "Good job, Tim!" They both smiled and hugged each other.
`**

The transformer completes this as

**`
went home. They sat down on the grass and closed their eyes. They closed their eyes and fell asleep. Tim dreamed of the snow he found his way home. He felt safe and happy in his new score. He knew that he could always count on his score again. And he lived happily ever after.
`**

which is a little less coherent. Thus we see that there is indeed an improvement in output quality, reflective of the lower training and evaluation loss of the trained mixer compared to the transformer.

### Flat Mixers Train Faster

When examining the ability of various model to [represent](https://blbadger.github.io/language-discreteness.html) their inputs, it was found that representation of tokens whose output was masked (indicative of the amount of information transfer between tokens) is accurate for untrained masked mixers if either the model were relatively large ($d_{model}=1024, \; n=24$ or $d_{model}=2048, \; n=8$) or else if the model remained small $d_{model}=512, \; n=8$ but the expanded sequential convolutions were replaced with a single convolution as shown in a previous section on this page.  

This suggests that these 'flat' mixers may be able to be trained more effectively than the expanded convolution mixers or transformers, and experimentally this is indeed the case: a $d_{model}=512, \; n=8$ flat mixer (requiring 3.59 GB vRAM to train on a context window of 512 tokens) achieves a training loss of 1.842 and a validation loss of 1.895 after 12 hours on TinyStories, which is lower than the $d_{model}=256, \; n=8$ transformer that requires more than double the memory (1.99 and 2.03 training and validation, respectively). 

Flat mixers experience superior training characteristics compared to two-layer 'expanded' mixers as well: depending on the expansion factor, the latter's sample throughput is 20-50% smaller. Which expansion factor should one use? If we consider how the causal language weight mask for this particular implementation, it is clear that there is only one expansion that makes sense: the 1x expansion in which the hidden layer is the same dimension (ie the sequence length) as the input and outputs. This is because the lower-triangular mask negates all 'extra' weights for any expansion greater than a factor of one, as is apparent in the following figure.

![masked mixer generation]({{https://blbadger.github.io}}/deep-learning/flat_versus_expanded_mixer.png)

The situation for any mixer with an expansion factor of less than one is even worse: here input components after a certain index are lost due to the causal language mask. It might be wondered if one could simply not mask the first convolution and mask the second to avoid this issue, but that would cause information from later tokens to influence earlier tokens (even with the second convolution being masked) and thus is not a viable approach.

![masked mixer generation]({{https://blbadger.github.io}}/deep-learning/flat_versus_expanded_mixer_decreased.png)

Even disregarding the time and superfluous parameters, however, expanded mixers of the same $d_{model}, n$, context, and batch size achieve larger loss even for a fixed number of samples seen. For example, a $d_{model}=512, \; n=8$ mixer with an expansion factor of 2 achieves a training and validation loss of 2.159 and 2.183 compared to the flat mixer's 2.087 and 2.111 after one epoch. This is counterintuitive given that the expanded mixer has more inter-token trainable parameters than the flat mixer such that one might expect this architecture to achieve lower per-sample loss (even if it trains more slowly), and in the author's opinion is best appreciated from the point of view of the superior nonself-token representation of flat mixers. 

It may be wondered if the performance increase in masked mixers compared to transformers is only due to the relatively small size of the transformer models tested thus far, and if larger transformers would be better learners than larger mixers. The largest 8-layer transformer that will fit in 12GB vRAM (at 512 token context with a 16-size minibatch) has a $d_{model}=256$, and at 12 hours on a 3060 this achieves 1.99 and 2.03 training and validation loss, respectively. This lags behind the training and validation loss of 1.81 and 1.86 of a $d_{model}=1024, \; n=8$ mixer trained with the same total compute budget. 

If one is able to vary the batch size, larger transformers and mixers still may be tested. A transformer of size ($d_{model}=512, \; n = 8$) with a batch size of 8 can just barely fit in the allotted 12 GB vRAM and this model achieves a peak of efficiency for the transformer architecture on this test, reaching training and validation losses of 1.873 and 1.908, respectively. We can consider this to be something near to a peak transformer model performance because if the model is further enlarged to $d_{model}=1024$ (which requires a reduction to a batch size of 4 for our vRAM stipulations) then performance suffers dramatically, reaching only 2.313 and 2.315 training and validation loss at the end of one training run. The larger transformer underperforms even when one focuses on the loss after a particular number of images seen: after 440k images, the $d_{model}=1024$ transformer has a training loss of 2.315 but the $d_{model}=512$ transformer reaches a training loss of $2.078$.

Even for the optimal $d_{model}$ for the transformer at ($512$) the flat masked mixer achieves lower losses (1.842 and 1.895).  When we compare a masked mixer with a smaller effective size ($d_{model}=1024, \; n = 8$) and are free to increase the minibatch size to 32 samples (the largest power of two that fits in the allotted 12GB vRAM), we achieve substantially lower losses once again: 1.784 and 1.837 train and validation, respectively. 

To sum up, flat mixers achieve lower training and validation loss than either llama-style transformers or expanded mixers, which follows from the superior self- and nonself- token representation present in these models.

Testing on inference bears out the cross-entropy loss findings: given the same prompt in the last section ("One day, a little boy..."), the trained $d_{model}=1024, \; n=8$ flat mixer yields

**`
played a game of catch. Tim threw the ball to Sam, and Sam caught it. They laughed and played until the sun went down.<unk>At the end of the day, Tim and Sam were tired but happy. They went home and took a nap. They dreamed of playing catch again tomorrow. And they did.
`**

which is a more coherent and plot-accurate completion than either the transformer or even the expanded mixer presented in the last section of this page, again reflective of a lower training and validation loss than either architecture.

### Parallel rather than Sequential Convolutions

We have seen that the original mixer architecture with two 1-Dimensional convolutions sequentially (and GELU inbetween) learns language much more slowly than a mixer with only one 1-Dimensional convolution between sequence elements. 

This observation presents a certain difficulty in that the 1D convolution must have as many filters as sequence elements squared to map a sequence to itself (all to all), meaning that the inter-sequence weights scale with sequence length but not the $d_{model}$.  This is not necessarily a problem for performance, as it is clear that simply increasing the $d_{model}$ yields increased asymptotic performance. 

What if we do want to increase the number of inter-sequence weights? According to studies on [representation accuracy](https://blbadger.github.io/language-discreteness.html), replacing multiple convolutions in series with sets of convolutions in parallel is the solution: both self- and non-self token representation is superior to either expanded or mixers. In the following diagram, we replace one convolution with two parallel ones,.

![masked mixer generation]({{https://blbadger.github.io}}/deep-learning/parallel_convs.png)

For a given number of training updates, the 2-parallel convolution mixer results in lower loss for a $d_{model}=512, n=8$ mixer: 1.845 versus 1.886 training loss at 354,000 steps (the most this model can finish in our fixed compute training).  However, as the model is slower to train per step it does tends to reach a very similar or perhaps slightly worse fixed-compute loss as the flat mixer of the same $d_{model}$ (1.842). Increasing the number of parallel convolutions to 4 leads to no apparent fixed-compute loss reduction either.

### Mixers do not benefit from positional encoding

By learning values for each combination of two tokens, the masked mixer learns an implicit absolute positional encoding and would not be expected to benefit from more positional encoding information. We can provide positional information in the form of a simple one-element addition to the first block's input vector as follows:


```python
class LanguageMixer(nn.Module):

	def __init__(self, n_vocab, dim, depth, tokenized_length, batch_size, tie_weights=False):
		...
		self.positions = torch.arange(-tokenized_length//2, tokenized_length//2, 1).to(device)

	def forward(self, input_ids, labels=None):
		...
		tokenized_length, batch_size = x.shape[-1], x.shape[0]
		positional_tensor = rearrange(self.positions, '(s b) -> s b', b = tokenized_length).unsqueeze(0).unsqueeze(-1)
		positional_tensor = positional_tensor.repeat(batch_size, 1, 1, 1)
		x = self.wte(x)
		x = torch.cat((x, positional_tensor), dim=-1)
		positional_tensor = positional_tensor.squeeze(1).squeeze(-1)
		x[..., -1] = positional_tensor
		for block in self.mixerblocks:
			x = block(x)
		output = self.lm_head(x[..., :-1])
```
but after doing so we see virtually no change in performance after our one unit of compute has been applied. If we instead apply the positional information to every block via
```	...
		for block in self.mixerblocks:
			x = block(x)
			x[..., -1] = positional_tensor
```
we see detrimental effects on training: our final loss is 1.94 train and 2.10 eval.

### Scaling Properties

The masked mixer architecture gives us an easy way to modify the number of inter-token weights (1D convolution `ConvForward` weights) as well as the number of intra-token weights (the `FeedForward` weights). We can observe the loss achieved by varying the number of each type of parameter independently, a feat which is much more difficult to pull off for the transformer as the number of $K, Q, V$ projection weights are usually tied to the $d_{model}$. 

Which type of weight is likely to be more important: that is, given a fixed number of total weights should we allocate more to intra- or inter-token parameters for the lowest loss given a fixed amount of compute?  When considering the causal language generation process, there are arguments to be made for both types, as clearly complex relationships between words are just as important if not moreso than a nuanced understanding of a word itself.

One argument for the importance of allocating more parameters to intra-token weights is that all information from all previous words must pass through these weights (ignoring residual connections for the moment), whereas inter-token weights may add information from many parts of an input over many layers. 

"""
One day, a little boy named Tim went to play with his friend, Sam. They wanted to play a game with a ball. The game was to see who could get the best score.<unk>Tim threw the ball and Sam tried to catch it. Sam missed the ball and it went far away. Sam was angry. He said, "I don't like this game. I want to lie down and rest."<unk>Tim said, "Okay, let's lie down and rest. Then we can try again to get a good score." They lay down under a big tree and looked at the sky. They saw birds flying and clouds moving.<unk>After a while, they got up and
"""

This can be seen when we compare the completion of a 64-dimension model with 64 layers, which achieves a 2.867 evaluation loss after a training run,

**`
played. They saw a man who had fallen down the street. Tim said, "Time to go home, please." Tim said, "It's okay to be scared, but we have to be careful." Tim nodded and they went home. They played in the big field and had fun. They had a great time playing in the snow.
`**

which is gramatically correct but loses the train of the story (Sam is forgotten, playing outside after going home, snow appearing etc.) which is typical of models of that dimension. On the other hand, a 1024-dimension model with 8 layers reaching a evaluation loss of 1.837, which gives the much more coherent 

**`
played a game. They took turns throwing the ball to each other. Tim was good at catching the ball. Sam was good at catching the ball. They laughed and played until it was time to go home.<unk>The moral of the story is that playing games is fun, but sometimes it's good to try new things and have fun with friends.
`**

The following table provides a summary of the results of transformer, expanded mixer, and flat mixer performance after one unit of compute (12 hours on an RTX 3060) applied to the first 2M examples of TinyStories:

Transformer ($n=8$ layers, $h=16$ attention heads, $b=32$ batch size unless otherwise noted):

|  | $d_{model} = 128$ | $d_{m}=256$ | $d_{m}=512$ | $d_{m}=1024$, b=8 | 
| -------- | ------- | -------- | ------- | -------- | 
| Train | 2.38 | 1.99 | 1.87 | 2.31 | 
| Test  | 2.40 | 2.02 | 1.91 | 2.32 | 


with the flat mixer (n=8, b=16)

|  | $d_{model} = 256$ | $d_{m}=512$ | $d_{m}=1024$ | $d_m=1024$, b=32, n=4 | $d_{m}=2048$ | 
| -------- | ------- | -------- | ------- | -------- | -------- |
| Train | 2.11 | 1.84 | 1.81 | 1.78 | 2.05 | 
| Test  | 2.15 | 1.89 | 1.86 | 1.84 | 2.07 | 


And for the expanded mixer (n=8, b=16),

|  | $d_{model}=256$ | $d_{m}=512$ | $d_{m}=1024$ | 
| -------- | ------- | -------- | ------- |
| Train | 2.17 | 2.05 | 1.83 | 
| Test  | 2.20 | 2.08 | 1.89 | 


We can also consider the memory scaling with respect to the number of tokens in the context length, $n_{context}$. It is apparent that both the transformer and masked mixer have the same computational complexity as length increases because every token is operated on by every other token for both models (resulting in an $O(n^2)$ space complexity). But it is also apparent that the flat masked mixer has a much lower constant factor for this scaling than the transformer, as each token-token operation consists of far fewer mathematical operations. When the memory required (in megabytes of vRAM, beyond 12 is OOM) to make an unbatched forward and backward pass (without a language modeling head) for a $d_m = 1024$ flat masked mixer, 

|  | $n_{context} = 512$ | $n_{c}=1024$ | $n_{c}=2048$ | $n_{c}=4096$ | $n_{c}=8192$
| -------- | ------- | -------- | ------- | -------- | ------- |
| $n_{layers}$ = 4 | 2071 | 2341 | 2637 | 3573 | 6491 |
| $n_l=8$  | 2431 | 2869 | 3425 | 5111 | 10527 |
| $n_l=16$ | 2695 | 3159 | 3811 | 5879 | OOM |

is compared to that for a transformer of the same width, we see that the masked mixer is around four times as memory-efficient with increasing token length.

|  | $n_{context} = 512$ | $n_c=1024$ | $n_c=2048$ | $n_c=4096$ | $n_c=8192$
| -------- | ------- | -------- | ------- | -------- | ------- |
| $n_{layers} = 4$ | 2323 | 3275 | 6809 | OOM | OOM |
| $n_l=8$ | 3176 | 4800 | 10126 | OOM | OOM |
| $n_l=16$ | 4876 | 7750 | OOM | OOM | OOM |


### Transformers with fewer attention heads are more efficient for TinyStories

The observation that the flat mixer performs better than transformers on TinyStories completion given limited compute suggests that perhaps we can get similar performance from transformers if the inter-token information transfer is simplified. We have been using 16-headed attention, which corresponds to 16 parallel linear transformations making key, query, and values for each token. We can simply reduce this number by supplying the desired $n$ number of attention heads as follows:

```python
llama_config_kwargs = {
    ...
    'num_attention_heads': n,
}
```

Conceptually this may be thought of as changing the number of simultaneous combinations of previous tokens' value vectors that the model 'considers' for each next token. A single-headed attention model would be somewhat similar to a flat mixer, as each would have a single value corresponding to the attention between any two tokens. The best-performing 16-head attention transformer was $d_m=512, n_l=8$ and when we reduce the number of attention heads while keeping the compute the same, we have the following for 12 hours of 3060:

|  | $n_{heads} = 16$ | $n_h=8$ | $n_h=4$ | $n_h=2$ |
| -------- | ------- | -------- | ------- | -------- |
| train loss | 1.87 | 1.70 | 1.66 | 1.68 |
| eval loss | 1.91 | 1.77 | 1.71 | 1.73 |

The main reason for this performance increase is that the smaller number of attention heads results in more samples seen during training in a given compute budget: for example, if we compare the loss after a fixed number of inputs seen we find that the 2-headed attention is not as effective as the 16-headed attention, at 1.96 train and 1.98 eval for two heads versus 1.87 train and 1.90 eval loss for 16 heads after half an epoch.

It might be wondered if this more efficient transformer might have better input representation, but we find that the opposite is true: neither at the beginning of training (where 16-headed attemtion transformers have fairly accurate one-block representation) nor at any other step during training does the two- or four-headed attention model exhibit anything remotely resembling accurate representation.

### Fully Convoluted Mixers for optimal TinyStories training

The results in the last section seem to indicate that representational power is more important than representational accuracy, as a transformer model that has poor input representation accuracy and many inter-token parameters outperforms mixer models with extremely accurate input representation but fewer inter-token parameters. This is not necessarily the case, however, as a mixer-derived model with an increased number of inter-token parameters is capable of performing on par even the highly optimized transformer as we shall see in this section.

The hypothesis here is that the mixer model can 'soak up' larger learning rates than the transformer because it is in most cases easier to optimize once training has proceeded a good ways: the difficulty of accurate back-propegation for transformers is documented elsewhere, but mixers typically do not seem to suffer from these problems. Thus we increase the learning rate of the mixer and repeat the experiments for this model (with 8 layers and a $d_m=1024$ and a batch size of 32).

We can observe scaling properties of these models by training them with a much larger and more powerful compute cluster: a [4x V100](https://blbadger.github.io/gpu-server.html) server. We train via Distributed Data Parallel, an algorithm that places replicas of the entire model of interest on each GPU and distributes data to each device, before all-gathering the gradients for backpropegation. This means that the effective batch size increases by a factor of 4 compared to the single-GPU training undertaken above, such that instead of say 32 samples per batch we have 32 samples on each of 4 gpus for 128 samples per gradient update total. Large language models are typically trained using modifications of the distributed data parallel algorithm, as these models cannot typically be held entirely in a single GPU's memory. To make results somewhat comparable with the 12 hours on a 3060, we limit training times to 2.25 hours on the 4x V100.

For clarity, the mixer architecture is detailed in the following figure: 

![conv mixer]({{https://blbadger.github.io}}/deep-learning/masked_conv2_mixer.png)

In contrast to the paralleled convolutions explored earlier, the convolutional kernal acts on the $d_{model}$ index, not the token index. The number of inter-token trainable parameters is equal to the kernel size multiplied by the number of weights in the 1D convolution of kernel size 1 (the number of sequence elements squared divided by two for masked mixers).

With this training protocol, the 4-headed, 8-layered llama model achieves a training (causal language model) loss and validation loss of 1.78 and 1.82, respectively. With the larger learning rate of $\eta=0.005$ this is slightly reduced to 1.76 and 1.79 training and test accuracy. This is far worse than what is achieved for a 8-layer, 4-sized convolution masked mixer (with the increased learning rate) which achieves training and validation losses of 1.62 and 1.74 given the same amount of compute. If we further optimize the transformer by increasing the batch size to 32, the transformer achieves nearly the same accuracies (1.68 and 1.73 respectively). If the mixer is modified slightly to contain an extra linear layer after the word-token embedding, the cross-entropy losses are 1.61 train and 1.71 test.

The main finding here is that a compute- and dataset-optimized mixer is generally slightly more performant than a similarly optimized transformer (notably without Flash Attention 2).

### Multi Headed Mixers

As more than one self-attention head increases the learning efficiency for transformers, it may be wondered if increasing the number of mixer 'heads' would also lead to a concomitant increase in mixer learning efficiency. One may implement masked mixer heads in a number of ways but we choose to use a formulation similar to the original transformer. Each head is projected by a unique weight matrix and then a unique mixer is applied to each head, and the outputs are concatenated and then re-projected back to match the $d_{model}$ dimension. The following diagram portrays this approach for the case where each head's convolutional kernal is of size one.

![conv mixer]({{https://blbadger.github.io}}/deep-learning/multiheaded_convs.png)

This approach yields a slight increase in training efficiency relative to the standard flat masked mixer with convolutional kernel of size 4: training/test losses for 2.25h on 4x V100 are  1.60/1.72 for the two-headed mixer versus 1.62/1.74. 

### Transformer mixer hybrids learn most efficiently

The observation that mixers appear to learn a fundamentally different structure than transformers when applied to the same dataset suggests that they may act orthogonally (with some slight abuse to the linear algebraic phrase). This means that the abstract quantities a mixer learns might be complementary to those that a transformer learns, such that combining these architectures in some way might lead to a model that is a more efficient learner than either mixer or transformer alone.

Can adding a mixer component to a transformer or vice versa increase training efficiency? One way to implement this is as follows:

![conv mixer]({{https://blbadger.github.io}}/deep-learning/mixer_transformer_architecture_original.png)

Recall that the 4-headed, $d_m=512$ transformer model achieved an train/test accuracy of 1.66/1.71 on TinyStories with 12 hours of RTX 3060 compute time, whereas the flat mixer with the same $d_m$ achieved values of 1.84 and 1.89, respectively. If these models learned the same way one would not expect for a composite model to outperform the transformer, but a quick test observes that it does: a transformer-mixer (with a flat mixer applied with a convolution size of one) hybrid achieves an accuracies of 1.62 and 1.67 given the same compute limit. 

Scaling up the compute power via the 4x V100, we find that this transformer-mixer hybrid once again outperforms the transformer: for 2.25 hours on a 4x V100 node the hybrid achieves lower training and test loss than either mixer or transformer (above) with values of 1.53 and 1.59, respectively, which compares to train/test losses of 1.55 and 1.61 without the masked convolution (both with Flash Attention 2).

Another way to add mixer elements to a transformer is as follows: we instead add the mixer in parallel with the self-attention module. 

![conv mixer]({{https://blbadger.github.io}}/deep-learning/mixer_transformer_architecture.png)

This does not yield any benefit from the standard transformer architecture, however, and achieves the same 1.55 and 1.61 loss values on the 4x V100 cluster (with flash attention 2).

### Masked Mixers outperform early CLM Transformers

It should be noted that masked mixers are far more efficient learners than the original causal language modeling-style transformers, that is, before the introduction of optimizations like Rotary Positional Encoding and Flash Attention. This can be shown by comparing an early CLM style transformer's learning on the TinyStories dataset to the mixer: for the OpenAI GPT implemented in the transformers library, the train/test accuracy achieved after 12 hours of 3060 compute is 2.04/1.96 using the optimized learning rate and $d_{model}$ values found for llama-style transformers. This is far worse than the 1.76/1.81 mixer loss, which is lower even than a GPT implementation with an optimized number of heads (1.96/1.89). 

If a community were given seven years to optimize the performance of masked mixers and if the causal language modeling performance of this model were to generalize to larger datasets, it is in the author's opinion very likely that masked mixers would out-perform even the most recent and highly optimized transformers. One could characterize the difference between masked mixer and transformer as being analagous to the x86-68 processor architecture compared with RISC-V: the latter is superior to the former given a constant amount of work in the field, but currently less effective as there have been so many optimizations on the x86 instruction set and architecture.

### Accurate Representation and Retrieval

In some sense it is unsuprising that the transformer has relatively poor input representational accuracy compared to the masked mixer. After all, this architecture was design on the principle that some input words are more important than others for sequence-to-sequence tasks such as machine translation; these more important words receive more *attention*. The softmax-transformed dot product attention serves to effectively limit the number of input tokens that influence an output per head, and with multiple heads more of the input may be attended to for any given output. But for accurate input representation we want something entirely different: a sampling of all input elements with some mixing process such that the information in the hidden layers of the model for token $n$ may recapitulate the prior tokens $0, 1, ..., n-1$. This is exactly what the mixer does.

But if transformers exhibit inaccurate input representation but are effective language task learners, does input representation accuracy really matter for causal language generation? These results seem to imply that transformational power (ie the set of functions that the model maps from input to output) is more important than accurate representation, at least for causal language modeling. And indeed that is equivalent to the hypothesis that attention brings to a model, that not all words are really useful for language tasks.

But transformers are used for many tasks that are a far cry from sequence-to-sequence models: in particular for this work, language retrieval-style tasks via embedding matching is nearly always accomplished today using transformer-based models. But thinking critically, are transformers really the right models for matching a query to a target text passage using some metric on the respective embeddings of each? 

The answer that can be argued from first principles (at least as apparent to this author) is certainly not: the implicit hypothesis present in self-attention is that the model only 'cares' about a subset of input elements for a next element prediction via the attention mechanism, but only paying attention to some inputs is clearly not ideal for mapping one sequence to another. It is better to consider all elements of each input and attempt a match based on more complete information, as otherwise the model is likely to overlook at least some part of the input. More formally, transformers were designed to map sequences of tokens to individual tokens (specifically the next token for causal language modeling tasks or a masked token for sequence-to-sequence tasks) and are an implementation of the assumption that performing this process efficiently requires removal of most input information. But for document retrieval we instead want to map a sequence of tokens to a sequence of tokens directly, preferably not removing information from each sequence unless there is a prior reason to do so. 

On the other hand, the masked mixer was designed to operate in a fundamentally different manner than the transformer: rather than focusing on a few input elements for each next token via an attention mechanism, the masked mixer does indeed mix all elements of the input before transformation via feed-forward layers, and hence exhibits far superior input representation as by design most information is not lost. 

Stated this way, it seems obvious that we should prefer a masked mixer to a transformer for the task of generating embeddings to perform retrieval tasks simply because the former model retains more information about its input than the latter. This can be tested using the same models trained to complete Tiny Stories fairly simply, and we will find that indeed the mixer is far better at retrieval tasks.

The metric normally used for retrieval tasks using transformers is a normalized cosine distance on the last hidden layer of the last token: for model $f$ with parameters $\theta$, the distance between inputs $a, b$ is

$$
d = 1 - \cos (f(a, \theta)^*, f(b, \theta)^*)
$$

where $f()^*$ denotes the flattened (vectorized) version of the potentially multi-dimensional model output for each input. This calculation can be done very quickly for many inputs via matrix multiplication on normalized inputs, using the identity relating inner products to cosine distances,

$$
\cos(x, y) = \frac{x \cdot y}{||x|| \; ||y||}
$$

and as the norms $\vert \vert x \vert \vert$ and $\vert \vert y\vert \vert$ can be set to one, the matrix multiplication yields the set of all pairs of cosine distances.

As explored [elsewhere](https://blbadger.github.io/language-discreteness.html), there is an aligment between this metric on the output and the method by which information passes between tokens in the transformer model, such that one may expect for this metric to be a somewhat accurate reflection of the learned inter-token relationships for that model. No such guarantees are present for the mixer, as there is no dot product and thus effective cosine distance computed between tokens for that model. Some preliminary results find that indeed a trained mixer performs very poorly when a cosine distance metric is used to perform retrieval tasks, which is to be expected.

A different method of matching embeddings must be explored for masked mixers, but it turns out that perhaps the most intuitive one ($L^1$ or $L^2$ distance on the vectorized output layer) is also very inaccurate. This can be understood as showing that the mixer's last layer does not form a manifold resembling a high-dimensional sphere or anything similar, as otherwise this metric (and the last) would accurately reflect similarites assuming the model were sufficiently trained.

Happily we can evaluate retrieval ability by simply training another model to match the embeddings generated by either the mixer or the transformer.  This can be done a number of ways, but the more established methods of training the language model itself on labeled or unlabeled matches (for instance, the [e5 mistral instruct method](https://arxiv.org/abs/2401.00368)) are not particularly well-suited here as they generally modify the embedding model, or else use very large datasets with characters that the tinystories-trained models would not recognize.  

Instead we train an arbitrary model to match embeddings of retrieval pairs, where the embeddings were either generated by transformers or by masked mixers. We choose to use a modified bidirectional mixer as the matching model, but the model architecture should not affect the results here assuming that it is powerful enough to perform the vector-to-vector mapping. The retrieval model architecture is the same as a normal mixer except that there is no word-token embedding (as the inputs are already embeddings) nor language modeling head: instead the 'head' is a map of dimensions $d_{model} \to 1$ such that all outputs for a selection of possible matches may be concatenated as a probability distribution after Softmax transformation. The training proceeds via cross-entropy loss on the softmax-transformed concatenated output, with the inputs being the query (summary) in the first token's position followed by a set number of targets in a random assortment in the following positions. The labels that are used to form the model's loss are one-hots where the one is in the position of the matching tinystory embedding. 

The following depicts the bidirectional mixer model in the case of a hidden dimension of 512:

![retrieval mixer]({{https://blbadger.github.io}}/deep-learning/Retrieval_model.png)

It should be noted that this approach does have practical value: one can perform a B-tree -like algorithm for finding matches from $n$ samples in $n \log n$ forward passes of the sequence matching model.

The retrieval pairs themselves are one-sentence summaries, and were generated for the first 200k tinystories samples by the instruction-tuned version of Llama-3 (8b). We have seen that the embeddings formed by masked mixers contain much more information about the input than those formed by transformers, so the hypothesis is that the retrieval process will be far more successful after identical training runs.

And indeed this is what is found: for 200k summary-story pairs with 32 samples being considered at one time (the 'context'), the mixer's embeddings lead to a train/test loss of 0.05/0.29 whereas the transformer of the same output dimension (1024) peaks at 0.01/0.40. When the number of samples increases the gap widens considerably: for a context size of 128, the mixer achieves train/test losses of 0.01/0.76 where the test loss continues to decrease even after 200 epochs, whereas the transformer only manages peak losses of 0.61/2.27 with severe overfitting (the training run ends at 2.98 test loss).

The mixer has another significant advantage with respect to embedding training: with relatively fewer inter-token parameters for a given $d_{model}$ size, the mixer is able to form a more high-dimensional embedding with the same compute resources as a transformer. The effect of this is that the transformer architecture with the lowest loss after the 12-hour 3060 compute on TinyStories ($d_m=512, n_l=8, h=4, \eta=0.005$) yields a 512-dimensional embedding that results in very poor matching model training even for the limited 32 context window size (virtually no train or test loss after 200 epochs at 3.43, whereas an identically-sized masked mixer achieves cross-entropy losses of 0.01 train and 0.10 test.

With an increase in the number of samples available, the mixer is capable of better performance: for the 512-dimensional masked mixer embeddings given a 32-sized comparison batch the test loss decreases from 0.1 to 0.053.  As the number of samples increases to 550k, and likewise for a 128-sized batch we have a test loss of 0.12.

It is interesting to note that an untrained mixer, while able to accurately represent its input, yields very poor embeddings for retrieval training. For the 200k dataset, an untrained 512-dimensional masked mixer's embeddings lead to practically no learning for batches of size 128 and even for batches of size 32 there is severe overfitting, with loss not below 3.2. Even more unexpected is the finding that the last hidden layer embedding is equally poor for retrieval tasks if the embedding model undergoes autoencoding rather than causal language model training (ie next token prediction). This can be shown by training an encoder-decoder architecture where the encoder mixer's last token's last hidden layer embedding is repeated to a decoder, which is tasked with regenerating the entire input string. This encoder-decoder may be trained remarkably effectively (with <2.0 cross-entropy loss) but the encoder's embedding is no better than that from an untrained model for retrieval learning. This suggests that the causal language model training process itself is important to language retrieval. 

It may be wondered if this is a result of some kind of incompatibility between a transformer's embedding and the ability of a mixer to learn that model's manifold, but this is not the case: if we instead use a bidirectional transformer to learn the retrieval pairs from the transformer's embeddings, we find that this model is far worse than the bidirectional mixer and that practically no learning occurs.

Thus we find that for embedding models of various sizes, embeddings derived from masked mixers lead to far better performance on a retrieval training task when compared to transformers. This supports the hypothesis that the full sequence-to-sequence mapping that is performed during retrieval benefits greatly from the increased informational content in the masked mixer's embedding as compared to the transformer's.  

### Conclusions

Seeking to make a more efficient learning algorithm than a transformer, we used the observation that input representation is far superior for modified MLP-Mixer architectures to craft a model capable of replicating the autoregressive language generation of GPT-style decoder-only transformers. 

We have seen that masked mixers are typically more efficient learners than transformers for the task of causal language modeling on TinyStories with no changes to the default architecture.  Mixers are typically competetive with or slightly wores than hyperparameter-optimized transformers on newer hardware with key, query, and value optimizations like Flash Attention 2, depending on the hardware. Masked mixers are, as implemeted, less efficient for CLM inference because they are highly parallelized and have a fixed-size context window for each forward pass.

On the other hand, masked mixers as they exist today are far more effective at retrieval tasks as evidenced by the observation that a summary-story matching retrieval model is capable of far better training on embeddings from these models compared to embeddings from transformers. For this task, inference would be approximately identical as one typically loads full context for each forward pass for retrieval tasks.

It is worth restating the more noteworthy findings of this work concisely:

1. Depending on the hardware the models are implemented on, given equal compute a masked mixer may reach a much lower training and validation causal language model loss or else a modern, dataset-optimized transformer may out-perform the mixer.
2. The mixer implementations uses no traditional regularization techniques, instead relying on the intrinsic generalization inherent in gradient descent-based optimization of high-dimensional space (see [this paper](https://arxiv.org/pdf/2211.09639.pdf) for more on this subject) combined with the 'inherent' regularization in language datasets. Mixers also have no rotary positional encoding (or any explicit positional encoding at all). Positional encoding instead stems directly from the convolutional filter weights.
3. Compared to the transformer models as originally introduced or adapted for CLM, masked mixers are more efficient learning algorithms. It is only with the optimizations of recent years (RoPE, FA2 etc) that the transformer becomes competitive for causal language modeling.
4. As mixers exhibit far more accurate input representations and thus contain more information on the input than transformers, one expects better sequence-to-sequence retrieval performance from these models. This is experimentally supported when training a retrieval model on mixer or transformer embeddings.

Fundamentally the masked mixer may be thought of as a true feedforward model in which all elements of the input are processed in parallel by sequential models. Transformers on the other hand are conceptually somewhat parallelized (as otherwize the causal language mask training trick would not work) but still retain aspects of recurrent neural networks (backpropegation must unroll the KV values, for example) which makes them faster for inference but slower to train. When one considers that training a language model may require a trillion times more compute than autoregressive inference, this shift does not seem to be entirely uncalled for. For retrieval tasks this does not apply, as the models are typically applied to chunks of data at fixed input sizes.

To conclude, the masked mixer architecture is introduced, motivated by superior input representation accuracy in this model compared to the transformer. This mixer architecture is in many ways much simpler than the transformer, but performs suprisingly well considering the many optimizations introduced since the first transformer architecture in 2017. Transformer-mixer hybrids offer the most efficient models observed for this dataset, outperforming Llama style models with many of the latest optimizations (Flash attention 2, RoPE etc.). These results indicate that a highly optimized transformer with its intrinsic focus on relatively few input elements for each output results in approximately as or slightly more efficient a learning paradigm as the masked mixer's lack of focus if the transformer is properly tuned, depending on the hardware. 

Returning to our original question: how many parameters are necessary for effective language modeling? We hypothesized that a masked mixer with (much) better representational accuracy than the transformer would outperform that model in causal language modeling tasks given a fixed amount of compute, but found that although a naieve implementation of each model did result in much better performance from the masked mixer, a combination of architectural, implementation, and hyperparameter optimizations allows the transformer to learn approximately as efficiently as or slightly better than the mixer depending on the hardware. On the other hand the masked mixer far outperforms the transformer for tasks involving matching one sequence of words to another, suggesting that representational accuracy is indeed important for that task.

How many parameters are required for accurate modeling of the TinyStories, at least the train split of that dataset? We can estimate this as follows: in our (slightly truncated training) dataset there are 2M stories, and each story is composed of perhaps 256 tokens for a total of 512M tokens. Assuming that training proceeds via next token prediction, this means that there are 512M 'next' tokens and thus 512M points that must be approximated in high-dimensional space. From the Johnson-Lindenstrauss lemma we can estimate that this would require an embedding space $n$ such that

$$
n > 8 \ln (512 \times 10^6) \approx 160
$$

If we assume that each model layer's $d_{model}$ corresponds to the model's dimension, this means that even the smaller mixers should be able to effectively memorize the entirety of the training portion of TinyStories. This is somewhat well-supported in our experimental results, as 512 or 1024-dimensional models were apparently capable of reasonably low cross-entropy loss even given very limited compute.

On the other hand, if it is more accurate to assume that each of the model's parameters corresponds to a dimension then even the smallest model we used should have no trouble memorizing TinyStories, which is not observed. This means that the former picture of model dimensionality seems more accurate experimentally, and this is supported by other work which finds that a model's parameters from one layer to the next certainly do not vary independently, and therefore may not be considered to be uniqe dimensions in high-dimensional space.
 








### Computability and periodicity I

[Elsewhere](https://blbadger.github.io/aperiodic-irrationals.html) we have seen that the set of all periodic dynamical systems (albeit systems as defined in a specific manner) is equinumerous with the set of solvable problems.  The set of all aperiodic systems was found to be more numerous, instead equivalent to the size of the set of all unsolvable problems.  This page will investigate a direct equivalence between the the members of the sets of solvable problems and periodic maps to arrive at a stronger conclusion: equality (with substitution) between these sets, rather than equivalence.

'Unsolvable' and 'Undecidable' are here defined in terms of mathematical logic: decidability is the process of determining the validity of a $\mathfrak t, \mathfrak f$ (true or false) statement.  Validity means that the statement is always true, regardless of the inputs to the statement.  Solvable means that there is some computation procedure that applies to the given function (or problem) that is finite in length.  

Decidability and computability (in a number-theoretic setting) are equivalent concepts for this page. The computational problem $x = 2^2$ is identical the the decision problem 'Is 4 the square of 2?'.  For this page, computable implies a 'for-what?' statement and decidable implies a 'is it true?' statement.  The law of the excluded middle is accepted, meaning that a proposed statement must be true or false but not neither. In symbols,

$$
\exists x P(x) \lor \lnot \exists x P(x)
$$

### The Church-Turing thesis

Church and Kleene postulated that all effectively computable functions are $\lambda$-definable, and later shown that these functions are equivalent to the general recursive functions Godel defined to be computable.  

A particularly poignent implication of the Church-Turing thesis is that most functions are uncomputable by any means.  A proof for this is as follows, from Kleene's *Mathematical Logic*:

Theorem (1): Most functions are not effectively computable

Proof: There are an uncountably infinite number of 1-place number theoretic functions on a countably infinite number of inputs.  Suppose that we could enumerate all such functions in a list $f_0(a), f_1(a), f_2(a), ...$.  Each function has the domain of the natural numbers $0, 1, 2, 3, ...$ so we can list them as follows:

$$
f_0(a): f_0(0), \; f_0(1), \; f_0(2), ... \\
f_1(a): f_1(0), \; f_1(1), \; f_1(2), ... \\
f_2(a): f_2(0), \; f_2(1), \; f_2(2), ... \\
...
$$

Now define a function $f_n(a) = f_a(a) + 1$.  This function is an application of Cantor's diagonal method: it differs from $f_0(a)$ because $f_n(0) = f_0(0) + 1$ and no number is its own successor.  Similarly, it differs from the other functions listed because $f_n(1) = f_1(1) + 1$ and $f_n(2) = f_2(2) + 1$.  No matter how many functions we enumerate, $f_n(a)$ will be different from them all.  But as $f_n(a)$ is a one-place number theoretic function, we have a contradiction and conclude that there is no enumeration of all 1-place number theoretic functions. 

Now as there are only countably infinite Turing machines (see below), because each machine is defined by a finite list of instructions and these lists can be enumerated one by one.  Therefore there are many fewer Turing machines than possible number-theoretic functions, and as each Turing machine describes one number-theoretic function then necessarily most functions are uncomputable on Turing machines (or equivalently in general recursive functions or by $\lambda$ calculus).  If we accept the Church-Turing thesis, this means that most functions are not computable by any means (ie they are not effectively computable).  'Most' here is an understatement, as actually only an infinitely small subset of all number theoretic functions are computable.

### Computability with Turing machines

To gain more appreciation for what was presented in the previous section, a closer look at Turing machines is helpful. A [Turing machine](https://en.wikipedia.org/wiki/Turing_machine) is an abstract system whose rules were designed by Turing to model computation.  A tape of infinite length has a sequence (of 0s and 1s for two state Turing machines) with symbols printed on it is fed into this machine and the output is the tape but with the symbols re-written.  Each step of a Turing machine procedure involves performing an action (or not), moving to the left or right (or staying stationary), and then changing state (or staying in the same state).  Turing machines are described by finite tables of instructions that completely specify what the machine should do at any given point.  For example, the following 1-state, 2-symbol machine if starting on a '1', changes the symbol to a '0', moves right, and halts:

$$
0 \; \; \; \; \; \;  1  \\
1\; EC0 \; \; ER1
$$

where 'P' is print '1', 'E' means erase (print 0), 'R' means move right, 'L' means move left, 'C' means stay in the same place, and the final number specifies the state of the machine ('0' or 'H' means halt).  The variable in question is the column head, and the row (here only '1') denotes state.

Now simply changing a symbol does not seem very useful, if one accepts the Church-Turing thesis then Turing machines are capable of performing any computational procedure that we can define. A more complicated one, found in Kleene 1967, takes any integer and adds one.  The instruction table is as follows:

$$
\; \; 0 \; \; \; \; \; 1 \; \\
1 \; C0 \; \; \; R2 \\
2 \; R3 \; \; \; R9 \\
3 \; PL4 \; \; \; R3 \\
4 \; L5 \; \; \; L4 \\
5 \; L5 \; \; \; L6 \\
6 \; R2 \; \; \; R7 \\
7 \; R8 \; \; \; ER7 \\
8 \; R8 \; \; \; R3 \\
9 \; PR9 \; \;  L10 \\
10\;C0 \; \; ER11 \\
11\;PC0 \; \; R11
$$

which may emulated as a Turing machine using c++ as follows:

```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <iomanip>
using namespace std;

vector<int> turing_incrementor(vector<vector<string>> program, int state, int position, vector<int> input, int& counter){
	
	// initialize the result tape as a copy of the input tape
	vector<int> result;
	for (auto u: input) result.push_back(u);
	
	while (true) {
		int square = result[position];
		string step;
		
		if (square == 1) {
			step = program[state-1][1];
		}
		else step = program[state-1][0];
		
		int index = 0;
		if (step[0] == 'E' or step[0] == 'P'){
			index = 1;
			if (step[0] == 'P') {
				result[position] = 1;
			}
			else result[position] = 0;
		}

		if (step[index] == 'R') position += 1;
		else if (step[index] == 'L') position -= 1;
		
		state = 0;
		string num_string = "";

		index++;
		while (index < step.size()) {
			num_string.push_back(step[index]);
			index++;
		}
		state = stoi(num_string);
		counter++;
		if (step[step.size()-2] == 'C' and step[step.size()-1] == '0') {
			return result;
		}
	}

}
	
int main(){
	//initialize program for incrementing integers
	vector<vector<string>> program { {"C0", "R2"}, {"R3", "R9"}, 
		{"PL4", "R3"}, {"L5", "L4"}, {"L5", "L6"}, {"R2", "R7"}, 
		{"R8", "ER7"}, {"R8", "R3"}, {"PR9", "L10"}, {"C0", "ER11"}, 
		{"PC0", "R11"} };
	
	//initialize input tape
	vector<int> input;
	
	//specifiy input number 
	int initial_number = 1000;
	for (int i=0; i<2*initial_number + 10; i++) input.push_back(0);
	for (int j=3; j<initial_number+3; j++) input[j] = 1;
	cout << "Input:   ";
	for (auto u:input) cout << u;
	//state initialization
	int state = 1;
	
	//counter initialization
	int counter = 0;
	
	int position = 3 + initial_number-1;
	vector<int> result;
	
	result = turing_incrementor(program, state, position, input, counter);
	cout << counter;
	cout << endl;
	cout << "Output:  ";
	for (auto u:result) cout << u;
	return 0;
}
```

The input is a series of '1's corresponding to an integer and the output is this followed by '0' and then the incremented integer.  For example, and input of  `00011000000`, corresponding to the integer 1 (as `000100` is 0) becomes `00011011100`, which is 1 followed by 2.  

This program is capable of incrementing any finite number, and so clearly the function of incrementing 1 is computable.  But in the last section, it was claimed that there are number theoretic functions that are uncomputable by Turing machines, or by any other effective method (Church's lambda calculus etc.) if the Church-Turing thesis is accepted.  What are these functions that are not computable, and are any of these uncomputable functions important?  

Let's introduce a useful predicate function: $T(i, a, x)$ signifies a Turing machine, with index integer $i$, input argument integer $a$, that returns $\mathfrak t$ (true) if the computation halts at step $x$ but otherwise $\mathfrak f$.  The input argument $a$ is any integer, which when encoded could be `000100` signifying $a=0$ etc.

The Turing machine index $i$ is defined as the instructions for the Turing machine, encoded as an integer (which is usually very large).  For example, the table of instructions for the Turing machine above that increments integers can be represented as the integer

$$
21136737094333657700134424238322063624969284419 \\
08675647661770484612266829110943192693004390163 \\
75863429484413035036069094311865821860452564289 \\
359749323778362522231196047032053805361
$$

Is $T(i, a, x)$ a decidable predicate function, or in other words is it computable?  It is: given any Turing machine table (in integer form) $i$, and any input $a$, we can simply run the machine until step $x$ to see if the machine stops at that step or not, and return $\mathfrak t$ or $ \mathfrak f$.  

To compare across an infinite number of Turing machines without having to deal with an infinite number of different inputs, we can simply set the input argument $a$ to the machine's index $i$, or $i = a$, which means that the input to the program is the program itself. Now $T(a, a, x)$ is the true or false predicate that the Turing machine with input $a$ and index $a$ halts at step $x$, and is computable for the same reason $T(i, a, x)$ is.  But what about the question of whether there is any number of steps $x$ such that this machine halts, $\exists x \; T(a, a, x)$: is this computable too?  We can no longer simply run the Turing machine to find an answer because we are interested in all possible $a$ values.  

Happily, however, one can list all possibilities of $T(a, a, x)$ as follows:

$$
T(1, 1, 1), \; T(1, 1, 2), \; T(1, 1, 3)  \cdots \\
T(2, 2, 1), \; T(2, 2, 2), \; T(2, 2, 3)  \cdots \\
T(3, 3, 1), \; T(3, 3, 2), \; T(3, 3, 3)  \cdots \\
\vdots
$$

because any Turing machine's instructions may be represented as an integer, and the same for any input to the same machine, it should be apparent that this table includes all possible Turing machine instructions and all possible inputs. Finding whether or not the machine halts at step $x$ is simply a matter of looking up the value on the table.

To restate the question at hand, can we compute the function $\exists x \; T(a, a, x)$, for any input $a$ and index $a$?  If we could do this, then we could create a Turing machine that simply adds one step (move right and halt, perhaps) to each machine $T(a, a, a)$ for all $a$.  Therefore if $T(a, a, a) = \mathfrak t$ then $T(a, a, a+1) = \mathfrak f$ and if $T(a, a, a) = \mathfrak f$ then $T(a, a, a+1) = \mathfrak t$ because there is only one integer number of steps $x$ at which the Turing machine halts.  

Clearly this machine returns a different $\mathfrak t, \mathfrak f$ result than each machine listed above for all values of $a$ where $T(a, a, a)$: if for example a certain encoding gave

$$
T(1, 1, x) = \mathfrak t, \; \mathfrak f, \; \mathfrak f \cdots \\
T(2, 2, x) = \mathfrak f, \; \mathfrak t, \; \mathfrak f \cdots \\
T(3, 3, x) = \mathfrak f, \; \mathfrak t, \; \mathfrak f \cdots \\
$$

then this new Turing machine would be

$$
T'(1, 1, x) = \mathfrak t, \; \mathfrak f, \; \mathfrak f \cdots \\
T'(2, 2, x) = \mathfrak f, \; \mathfrak f, \; \mathfrak t \cdots \\
T'(3, 3, x) = \mathfrak f, \; \mathfrak f, \; \mathfrak t \cdots \\
$$

which is a different table by definition.  Different Turing machine outputs (for the same inputs) can only occur for different machines, so we know that $T'$ differs from all Turing machines listed above whenever $a=i=x$, or at the $\star$ machines in the table:

$$
T\star(1, 1, 1), \; T(1, 1, 2), \; T(1, 1, 3)  \cdots \\
T(2, 2, 1), \; T\star(2, 2, 2), \; T(2, 2, 3)  \cdots \\
T(3, 3, 1), \; T(3, 3, 2), \; T\star(3, 3, 3)  \cdots \\
\vdots
$$

But this means that the new Turing machine is not present on this list, because it differs from each entry for at least one input.  The list by definition included all possible Turing machines, so a contradiction has been reached and the premise that $\exists x \; T(a, a, x)$, assuming $a$ is free, is computable must be false.


### Busy Beaver Turing machines

Code for this section is found [here](https://github.com/blbadger/turing-machines).

In other words, whether or not an arbitrary Turing machine (with an input equal to its instructions) is going to ever halt is not a decidable predicate, even though it is clear for any finite step $x$ whether the machine halts there.  This does not mean that if we are given a value of $a=i$, we cannot determine if the machine halts in some cases.  The first machine presented, which simply changes the initial tape entry and moves to the right and halts, will always halt.  Likewise a machine that only moves to the left without any other instructions clearly will not halt.  But $\exists x \; T(a, a, x)$ contains $a$ as a free variable, meaning that it can take any value.  The surprising conclusion then is that there is no way to determine whether a machine halts given an arbitrary value of $a$, although for certain $a$ the answer is clear.

To better understand just how unclear it is whether an arbitrary program will halt, Rad&oacute; described the busy beaver challenge: to make the longest number of steps to a Turing machine of a given size that does not obviously run forever.  For two state Turing machines (with entries of 1 or 0), the maximum number of steps an eventually-stopping program takes is 6.  This number must be found by trial and error, because as presented in the last section whether or not a program halts for all possible inputs is not computable.  In the language used above, this program is

$$
0 \; \; \; \; \; 1 \; \\
1 \; PR2 \; \; PL2 \\
2 \; PL1 \; \; PRH \\
$$

which given an input of $...0000000000...$ returns $...00001111000...$ (depending exactly where the starting index is).  For three states, the maximum number of steps of an eventually-stopping machine is 21, which also does not seem so unmanageable.  But for five possible states and two symbols, the machine table

$$
\; 0 \; \; \; \; \; 1 \;  \\
1\; PR2\; \; PL3 \\
2\; PR3\; \; PR2 \\
3\; PR4\; \; EL5 \\
4\; PL1\; \; PL4 \\
5\; PRH\; \; EL1 \\
$$

takes 47176870 steps to complete.  As found in the very useful repository [here](http://www.logique.jussieu.fr/~michel/ha.html#tm52), the maximum number of steps for a six-state, two symbol Turing machine is greater than $7.4  10^{36534}$.  The true value is unknown, because this machine (and a number of other similar ones) are still running, with no indication of looping or stopping.  And this is for only 6 possible states and 2 possible symbols: imagine how many steps it would require to find whether all possible 11-state, 2-symbol machines halt!  An yet this number of states was used for something as simple as integer incrementation.  Clearly then, without some kind of external cue as to whether a machine halts or not the answer is unclear.

### Unsolvability and aperiodicity

Solvable problems (ie computable functions) are those for which, by the Church-Turing thesis, there exists an algorithm of finite size for a Turing machine that yields a decision for any of a countably infinite number of decision inputs.  

Take the mapping of Turing machine (or $\lambda$ -recursive function, as in Church's original thesis) inputs to the outputs as a dynamical system, where each element of the set of countably infinite inputs $\{ i_0, i_1, i_2 ... \}$ is mapped to its output $ \{ O_0, O_1, O_2 ... \}$ as 

$$
\{i_0 \to O_0, \; i_1 \to O_1, \; i_2 \to O_2 ...\}
$$

To be solvable (at least on this page), an algorithm of finite length must be able to map all of the countably infinite inputs.  The stipulation of finite length is important here, as it was not included in the original computability work by Turing but become important once one considers [aperiodic sequences](https://blbadger.github.io/uncomputable-aperiodics.html). After a certain number of $n$ inputs, the decision procedure for $i_0$ must be identical to the decision procedure $\mathscr P$ for $i_{0+n}$ or else the algorithm would be of infinite size, meaning that it would be unusable as it would never halt when applied to a Turing machine.  Thus $\mathscr P$ repeats after a finite interval $n$ between inputs.  

As the inputs are countable, they can be arranged in sequence.  Taking the sequence of decision procedures $\mathscr D$ for all inputs, the function mapping an input $\{ i_0, i_1, i_2 ... \}$ to its decision procedure $ \{ \mathscr D_0, \mathscr D_1, \mathscr D_2 \}$ defined as $f(i)$ can be viewed as a discrete dynamical system.  

Theorem (2): For all decidable problems, $f(i)$ is periodic for some ordering of $i_0, i_1, i_2 ...$.

Proof: Suppose that $f(i)$ is computable but is not periodic for any ordering of $i_1, i_2, i_3 ...$.  Then

$$
\forall n, k : n \neq k, \; \mathscr D_n \neq \mathscr D_k
$$

Therefore there are a countably infinite number of elements $\mathscr D_0, \mathscr D_1, \mathscr D_2 ...$ meaning that $f(i)$ is countably infinite.  Thus $f(i)$ would be infinitely long, but this contradicts the definition of computability.  Therefore $f(i)$ is periodic for some ordering of $i_1, i_2, i_3 ...$ (which is equivalent for stating that $f(i)$ is periodic for some ordering of its elements $\mathscr D_0, \mathscr D_1, \mathscr D_2 ...$ ).  

Thus $f(i)$ gives us the function mapping each computable function (or decidable predicate or solvable problem) to one discrete dynamical equation.  

### What does it mean that most problems are unsolvable, or that most $\mathfrak t, \mathfrak f$ predicates are undecidable?

The appearance of undecidable predicates and unsolvable problems (and unprovable but true statements) in arithmetic is disconcerting enough when one considers how intuitively consistent the axioms of this system are as compared to, say, those of analysis.

On the other hand, consider this: in the real world, many problems appear to be so difficult to find a solution to as to be, for all intents and purposes, impossible.  The study of mathematics attempts to understand the behavior of often very abstract ideas, but it is rooted in a study of the natural world: of shapes made by objects here on earth, paths that stars take in the sky, and the like.  Great utility of mathematics in describing the natural world remains to this day.  

Thus mathematics would be expected to reflect something about the natural world.  Now if all mathematical problems were solvable, it would not seem that this study is an accurate description of the world around us.  If a system with axioms as simple as arithmetic could have unsolvable problems, there is hope that even something as historically removed from application as number theory is able to capture something of reality.

### Addition-only or multiplication-only number theories are decidable

Skolem defined an arithemtic with multiplication but not addition, and showed that this is decidable and complete.  Similarly, Presburger defined an arithemtic with addition but not multiplication and found this was decidable and complete.

These results are surprising because addition composes multiplication.  For any arbitrary multiplication function $m(x)$ and addition function $a(x)$, we can compose the multiplication function of repeated additions as follows:

$$
m(x) = nx \implies m(x) = a \circ a \circ a \cdots \circ a = a^n(x)\\
$$

Similarly, division $d(x)$ on the integers can be composed of repeated applications of addition $a(x)$ as follows:

$$
a \circ a \circ a = a(a(a(n))) \\
d(x) = \frac{x}{n} \implies d(x) = card \; \{a \circ a \circ a \cdots \circ a \} :  \\
a \circ a \circ a \cdots \circ a = x \\
a^{d(x)}(n) = x
$$

In other words, division of one number by a second is equivalent to the number of times addition must be composed on the second number to equal the first, or the cardinality of the set of addition compositions required to transform the second number into the first.

As subtraction is defined as the inverse of addition, all four arithmetical operations may be performed on terms of only addition.  How then is addition-only or multiplication-only number theory decidable, but if both operations are allowed then the theory is undecidable?

A direct answer to this question is difficult, but one approach is to consider the nature of prime numbers.  The insight here is to note that prime numbers can only exist in an arithmetic with both addition and multiplication, because without addition every number is divisible by another smaller (non-identity) number and without multiplication there cannot be a prime number because the very definition of a prime number requires division (the inverse operation of multiplication).  

The next step is to understand that in an arithmetic with only multiplication or only addition, every finite element is composed of a known number of identity elements, eg. 101 is composed of 101 1s in an addition-only arithmetic.  But this is not the case for addition with multiplication precisely because primes are non-decomposable: given an arbitrary large number, we cannot know how to divide it into smaller numbers or even if the number in question is prime or not without factoring it. Factoring requires performing an unknown number of operations, contrary to the known number of operations required to compose a number in an arithmetic with both addition and multiplication.

Alternatively, consider Godel's proof of incompleteness and undecidability of arithmetic using the Godel numbers.  These are (usually very large) integers that are formed by raising prime numbers to powers depending on the formulation of any sentence, ie if $\not = 3$ and $p = 5$ then the Godel number for $\not p = 2^3 * 3^5$.  The properties of prime numbers ensure that the resulting Godel number can be converted back into a sentance unambiguously because each integer has a unique factorization.  Now one cannot apply this system to a number system without primes because there is no longer an unambiguous conversion method.

An interesting analogy to decidability between these different number systems exists for dynamical systems.  To restate, number theory with only addition is decidable, as is number theory with only multiplication but with both addition and multiplication, the theory is undecidable.  In dynamics, as seen elsewhere on [this page](https://blbadger.github.io/), transformations involving only addition are linear and solvable, and transformations of only multiplication (if bounded) are nonlinear but also solvable, as they simply expand or compress the function pre-image.  But transformations with both addition and multiplication may be aperiodic and unsolvable.

### Examples of undecidably-undecidable number theoretic statements

Kleene found levels of undecidabity, implying some statements are more undecidable than others.  Some problems, like whether or not a Turing machine will halt given an arbitrary input, are demonstrably undecidable.  But other problems may or may not be undecidable, and these would be expected to occupy a higher level of undecidability with respect to the halting problem.  Some such probably-undecidable statement examples are put forth here.  

As both Presburger and Skolem arithmetics are decidable (and complete) but arithmetic with both addition and multiplication is not, one would expect to find some undecidable number-theoretic statements existing at the intersection of multiplication and addition.  If we consider prime numbers to the the 'atoms' of multiplication, this is indeed the case: there are a number of as-yet-undecidable statements relating primes to addition. 

First and most famous is Goldbach's conjecture, that all integers greater than two are the sum of two primes,

$$
\forall n>2, \; n \in \Bbb N \; \exists a, b \in \{primes\} : n = a + b
$$

There is also the twin primes conjecture, that there is an infinite number of pairs of primes that are two apart from each other,

$$
\lvert \{p, p+2\} \rvert > n \; \forall n\in \Bbb N : p, p+2 \in \{ primes \}
$$

and Legendre's conjecture, that there exists a prime number between the square of any integer and the square of its successor,

$$
\forall n \in \Bbb N, \; \exists p \in \{ primes \} : n^2 < p < (n+1)^2
$$

All are as yet unproved and yet seem true, at least they are true for all inputs observed thus far.  Moreover, they may well turn out to be unprovable in arithmetic itself: like numerous number theoretic proofs that have been found in recent years for difficult problems, the proof may require use of analytic continuation or some other method based on limits, which is not strictly allowed for using only arithmetic. 

In particular, Wiles' proof for Fermat's last theorem, that is, there is no integer solution to $a^n + b^n = c^n$ if $n$ is greater than two and $a, b, c, n$ are all integers and $a, b, c$ are distinct. This proof makes use of algebraic geometry and the theory of elliptic curves in addition to more classical (arithmetical) number theory.  This means that while Wiles' proof is a remarkable achievement indeed, it does not prove what is essentially an arithmetical statement using arithemtical alone..


### Computability and the axiom of choice in Banach-Tarski

Cantor's set theory has become foundational for many areas of mathematics, but in its original form (naive set theory) the theory was observed to harbor contradictions that were unsatisfactory to mathematicians at the turn of the 20th century.  Axiomatized set theory was an attempt to prevent many such contradictions, and did this by replacing intuitive notions of 'collections' with rules for what sets could be composed of.  

One particular axiom of interest is the axiom of choice, which states that the Cartesian product of two aritrary non-empty sets is non-empty. Equivalently, there exists some function to find any value of choice from even an infinitely large set.  This axiom was controversial when first introduced but is not widely accepted. The axiom is similar to the law of the excluded middle detailed at the top of this page, in that one seems to be able to search through an infinite set with a function to find an element just as with the law of the excluded middle, one of $\mathfrak t, \mathfrak f$ is chosen even if the function domain is infinte (meaning that we cannot practically test every element).

The axiom of choice gives rise to an interesting, and somewhat counterintuitive idea: the Banach-Tarski theorem, which states that an arbitrary set-theoretic sphere of infinite points can be decomposed and recomposed into two spheres of the same 'size'.  The idea that one ball can be split up and the re-formed into two of the sme size is also called the pea and the sun paradox, because with repeated applications of Banach-Tarski a small pea could be multiplied to the size of a large object like the sun.

A reconciliation with this paradox may be had if one observes that the function required to perform the dissassembly and reassembly of spheres in Banach-Tarski is uncomputable.  This follows from how the deconstruction and reconstruction is performed: it is an infinite recursive (tree) algorithm, and therefore not computable if we accept the Church-Turing thesis (or if we are reasonable people in general). 

If the Church-Turing thesis is correct and our somewhat undefined notion of computability is captured by Turing machines (or by general recursive functions), then the Banach-Tarski theorem is paradoxical in part because it relies on functions that we cannot compute.  

### Aside: computability and the natural world

Does it matter that most functions are uncomputable (accepting the Church-Turing thesis)?  Sometimes it is argued that no, just as the rationals are dense in the reals so computable functions can approximate any function. This argument relies on assumptions of approximations that do not hold for nonlinear functions and is therefore unhelpful as most functions are nonlinear. 

Another argument is that most of these uncomputable functions would mostly resemble mappings to (white or other) noise, like the static on an old cathode ray tube television.  First suppose this argument were true . To restate, assume that there are many functions out there that are uncomputable which map to noise.  Now consider that most oservations in the natural world are noise.  Any scientific measurement is usually divided into signal (which is rare and what one is usually after) and noise (which is everything else by definition).  Brown noise (which we can define here as $1/f^n, \; n > 0$) is the most common but others are found as well.  Noise is usually discarded but is ever-present.

Therefore if uncomputable functions map to noise, and if noise is by far the most common observation in the natural world, it is implied that uncomputable functions map to most observations of the natural world.  This does not, of course, mean that nothing can be learned about the world using computable functions.  But it does imply that it is a bad idea to disregard uncomputable functions as unimportant in real life.

















## Tesla Coils: high voltage resonant transformers

High voltage arcs.  Beautiful to witness!

Specs: ~4 kVA power input using an ASRG spark gap running at ~500 bps, made from a variable-speed angle grinder with a disk (fashioned from high density polyethylene cutting board) securing four flying electrodes.  The electrodes are brass as iron has high RF losses and get too hot to use with polyethylene, which is a thermoplastic and melts at high temperatures. 

Tesla coils are air-cored resonant transformers, and are a type of LC circuit.  The L stands for inductor, and C for capacitor and when combined with a device that allows for rapid capacitor charging (charged from a circuit power supply) and discharging (the spark gap), the LC circuit is capable of enormous changes in electromagnetic field strength over very short periods of time as it oscillates. 

This rapid oscillation of thousands of volts means that most capacitors are unsuitable for the LC circuit. Here the primary circuit uses a 120nF multi-mini style capacitor made from a few hundred surplus WIMA FKP1 pulse capacitors usually found in industrial lasers.  These are manufactured specifically to maintain good capacitance and not degrade while being sugject to rapid changes in voltage and current.  Cheaper alternatives are glass bottles wrapped in aluminum foil, although these do degrade and do not have nearly the efficiency of industrial capacitors like the WIMA. 

The LC circuit is powered by 4 microwave oven transformers, primaries wired in parallel with secondaries in series (floating cores, and all submerged in mineral oil) for a power ouput of 10 kV at ~400mA.  Not pretty but a very inexpensive and robust power source.  In fact, one of the only components I have used that has not failed catastrophically at one point or another: even when a primary strike led to a vaporized grounding wire (connecting the secondary to the iron core) in one of the transformers, repairs were minimal.

![MOT stack]({{https://blbadger.github.io}}tesla_images/mot_stack.JPG)

One point of caution: the oil used here is plain mineral oil from the grocery store. In terms of flashover and arc resistance, industrial transformer oil is far superior.  But new transformer oil is quite expensive, and old oil can be hazardous. Be extra careful with any oil that has come out of a transformer, as it may contain polychlorinated biphenyls, aka PCBs, which are very toxic.  PCBs were routinely added to transformer oil before the 1970s, and any used transformer or oil purchased and suspected to originate in this era should be checked to be free of PCBs before use.

Power controlled with a variable autotransformer (for low power runs), up to 145V output with 120V input.  This variac allows a maximum current of around 3 1/2 kVA before saturating.

![variac]({{https://blbadger.github.io}}tesla_images/variac.JPG)

The topload is two aluminum dryer ducts forming a double stacked toroid, each stack 8" by 2.5'. Primary coil is ~8 turns of 0.25" refrigerator tubing, secondary coil is 1100 turns of AWG 22 magnet wire on an 8" inner diameter concrete  forming tube coated in polyurethane. The inner turns of the primary gets noticeably warm during operation, not suprising with an estimated 18.9 kA instantaneous current during capacitor discharge.

![tesla coil arcs]({{https://blbadger.github.io}}tesla_images/newtesla.jpg)

![tesla coil arcs]({{https://blbadger.github.io}}tesla_images/tesla_3.jpg)

![tesla coil arcs]({{https://blbadger.github.io}}tesla_images/tesla_4.jpg)

![tesla coil arcs]({{https://blbadger.github.io}}tesla_images/tesla_7.png)

### Low-res videos of the coil above

{% include youtube.html id='gwUA4ATNvRg' %}

![]()

{% include youtube.html id='FyRCdSQW1GY' %}


### Very large topload test

A rubber inner tube was inflated and covered in aluminum tape to make a very large toroidal topload (5' diameter).  This method is not recommended without the use of a forming substance (paper miche or plaster etc) covering the tube because the high voltage arcs will puncture the rubber even when protected via metal tape.  This is the last generation of the coil above, as the secondary base experienced severe flashovers during >4 kVa runs resulting in vaporization of parts of the lower windings.  > 7' arcs!

![tesla coil arcs]({{https://blbadger.github.io}}tesla_images/large_tesla.gif)


### Early generation coil

This coil was based on a secondary coil wound on a 4" diameter PVC with 1300 turns of AWG 26 magnet wire. A ~2.5 kVA input from the microwave oven transformers in oil (see above) was used (an air core inductive current limiter was used to reduce current) along with smaller dryer duct toroids, smaller primary capacitor (~ 50 nF) make from WIMA MKP-10 pulse capacitors These are by design not as durable as FKP-1 but are a good deal cheaper and easier to find.  I did experience a defective batch of these that could not tolerate the >400 discharges per second necessary, but nearly all others I have obtained have performed very well.

![primary capacitor]({{https://blbadger.github.io}}tesla_images/wima_mkp10.JPG)

Note the bleed resistors at the top: I have not found these to be necessary for new capacitors (ie with no dielectric memory) that are then employed for tesla coils.  Both the WIMA FKP-1 and MKP-10 mmcs mentioned on this page have not acquired any appreciable dielectric memory even after hours of total run time.  Nevertheless, it is generally a good idea to include some form of current dissipation for large capacitors like these, as the energy they can release in a small amount of time is considerable.

The primary and secondary coil setup on wood supports, with an angle grinder-based ASRG in the ground to the right and the capacitor bank underneath the primary.

![setup]({{https://blbadger.github.io}}tesla_images/old_tesla.JPG)

The jumper cables are attached to the strike rail protecting the primary coil and the secondary coil base, and lead to an 8' copper grounding rod.

![gen 1 tesla arcs 2]({{https://blbadger.github.io}}tesla_images/tesla_5.JPG)

![gen 1 tesla arcs]({{https://blbadger.github.io}}tesla_images/tesla_6.JPG)



## Transformer and Mixer Features

### Vision Transformer Feature Visualization

Convolutional models consist of layer of convolutional 'filters', also known as feature maps, that tend to learn to recognize specific patterns in the input (for a detailed look at this phenomenon, see [this page](https://blbadger.github.io/feature-visualization.html)).  These filters are linearly independent from one another at each layer, which makes it perhaps unsurprising that they would select for different possible input characteristics.

At first glance, it would appear that transformers do not contain such easily separated components because although each input is separated into a number of patches that are encoded in a linearly separable manner, the attention transformations act to mix this information, moving relevant information from one token (in this case a patch) to another.  For an interesting look at this in the context of attention-only transformers applied to natural language, see [this work](https://transformer-circuits.pub/2021/framework/index.html) by Elhage and colleagues.  

But we can investigate this assumption by performing a feature visualization procedure similar to that undertaken for convolutional image models on [this page](https://blbadger.github.io/feature-visualization.html).  In brief, we seek to understand how each component of a model responds to the input by finding the input which maximizes the activation of that component, subject to certain constraints on that input in order to make it similar to a natural image.

More precisely, we want to find an input $a'$ such that the activation of our chosen model component $z^l$ is maximized, given the model configuration $\theta$ and input $a$, denoted by $z^l(a, \theta)$ 

$$
a' = \underset{a}{\mathrm{arg \; max}} \; z^l(a, \theta)
$$

Finding the exact value of $a'$ can be very difficult for non-convex functions like hidden layer outputs, so instead we perform gradient descent on an initially random input $a_0$ such that after many steps we have an input $a_g$ that approximates $a'$ in that the activation(s) of component $z_l$ is maximized subject to certain constraints. The gradient here is the tensor of partial derivatives of the $L_1$ metric between a tensor with all values equal to some large constant $C$ and the tensor activation of the element $z^l$

$$
g = \nabla_a (C - z^l_f(a, \theta))
$$

At each step of the gradient descent procedure, the input $a_k$ is updated to $a_{k+1}$ as follows

$$
a_{k+1} = \mathscr J \left( \mathcal N_x(a_k - \epsilon * g_k) \right)
$$

where $\mathcal N$ signifies Gaussian blurring  (to reduce high-frequency input features) and $\mathscr J$ is random positional jitter, which is explained in more detail [here](https://blbadger.github.io/input-generation.html#jitter-using-cropped-octaves).  

For clarity, the following figure shows the shape of the tensors we will reference for this page.  We will focus on the generated inputs that result from maximizing the activation of one or more neurons from the output layer of the MLP layers in various components.  

![vit feature activations]({{https://blbadger.github.io}}/deep-learning/transformer_activation_explained.png)

It is important to note that the transformer's MLP is identically applied across all patches of the input, meaning that it has the same weights and biases no matter where in the image it is applied to.  This is similar to a convolutional operation in which one kernal is scanned across an entire image, except that for the vision transformer the feature information is stored in individual MLP neurons, whereas for convolutional models typically there are multiple neurons (3x3 and 5x5 are common convolutional filter sizes) required per feature.

For Vision Transformer (ViT) base with 32x32 pixel patches (with 88M parameters total) we have

![vit feature maps]({{https://blbadger.github.io}}/deep-learning/vit_b_32_feature_map.png)

Maximizing the activation of a subset of neurons in all patches yields the following feature maps:

![vit feature maps]({{https://blbadger.github.io}}/deep-learning/vit_b_32_features_combined.png)

For single neurons in individual patches we have

![vit feature maps]({{https://blbadger.github.io}}/deep-learning/vit_b_32_single_feature.png)

Similarly when we observe features of ViT Large 16 (which contains ~300M parameters in this configuration) which underwent weakly supervised pretraining before ImageNet training, we have

![vit feature maps]({{https://blbadger.github.io}}/deep-learning/vitl16_4_1_16_feature_maps.png)

### MLP Mixer Feature Map

Thus far we have seen that we may recover feature maps from individual neurons of vision transformer encoder block outputs. It may be wondered if the same is true for attentionless transformers, introduced independently by [Melas-Kyriazi](https://arxiv.org/abs/2105.02723) and [Tolstikhin](https://arxiv.org/abs/2105.01601).  The model used on this page is that of Melas-Kyriazi, and may be found [here](https://github.com/lukemelas/do-you-even-need-attention).  

These models do not use the attention operation to mix information between image patches but instead simply transpose the input and add a feed-forward layer (also called MLPs) applied to the embedding of each, followed by a second feed-forward layer across the patches. The order (mixing patches versus non-mixing layers) is therefore swapped compared to the ViT, and may be visualized as follows:

![mixer features]({{https://blbadger.github.io}}/deep-learning/ffonly_activation_explained.png)

As for vision transformers, we find that these models form features in which each neuron of the block output is activated by a stereotypical pattern in early layers, and that this pattern becomes more abstract and complex the deeper the module in question.  It is interesting to note that these neurons correspond to the output of the mixer layer whereas the block output features for the ViT correspond to the output of the non-mixing MLP in that model.

![mixer features]({{https://blbadger.github.io}}/deep-learning/mixer_feature_map.png)

But there begin to be notable differences between vision transformers with attention layers and MLP mixers if we start to consider what a single neuron from a single patch focuses upon: as seen in the last section for ViTs, a neuron from a given patch will generally focus on the input region fed to that patch.  For mlp mixers, however, we find that even in relatively shallow layers a neuron from a given patch typically focuses on the entirety of the input image, or else on a section of the image that does not correspond to the position of the patch in question.

This phenomenon can be most clearly seen in the following figure: observe how even at block 4 we find neurons in which it is impossible to know which patch they belong to without prior knowledge.

![mixer features]({{https://blbadger.github.io}}/deep-learning/mixer_individual_features.png)

This suggests that the 'Mixer' is also an apt name for this architecture, being that the MLP-Mixer is apparently better at mixing information from different patches than the vision transformer is.  We can assess this further by observing the ability of all neurons of certain patches to re-form an input image in mixers compared to ViTs. 

Upon further investigation, it may be appreciated that this mixing occurs fairly thoroughly even in the first block: 

![mixer features]({{https://blbadger.github.io}}/deep-learning/embedding_vs_patch_mixer.png)

and this is reflected in the change from a uniform pattern in the features of the first block's embedding MLP to the composition of patterns present in the first block's patch-mixing MLP.

![mixer features]({{https://blbadger.github.io}}/deep-learning/mixer_sublayer.png)


When the ability of the first 28 patches (approximately the first two rows for a 224x224 image) to re-create an input is tested, it is clearly seen that this subsection in mixers but not vision transformers are capable of representing an input to any degree of accuracy.

![mixer features]({{https://blbadger.github.io}}/deep-learning/mixer_vs_vit.png)

It may be wondered if this is due to a mismatch between the loss function we are minimizing (L1 distance) and the transformations that compose the self-attention layers of the transformer that are responsible for moving information from one patch to another.  To recap, vision transformers typicaly use dot-product attention of some scaled version of the following in vector format

$$
A(q, k, v) = \mathrm{softmax}(q \cdot k) v
$$ 

or in matrix format,

$$
A(Q, K, V) = (QK^T)V
$$

The dot product may be thought of as combining information of relative vector magnitude and angle into one measure, and all information from the query token's vector must pass through the dot product with the other token's key and value vectors (or matricies).  If one assumes that the fully connected layers that come after the self-attention layers in each transformer modules are capable of converting this angle-and-magnitude information into a pure distance (in vector space) information, it does not matter that we are optimizing $L^1$ or $L^2$ distance on this architecture.

But if there is some difficulty in converting between angle-and-magnitude and the difference norm, a more accurate representation may be found by optimizing for angle-and-magnitude instead.  Here we focus on reducing the angle between the output of our generated input $O_l(a_g, \theta)$ and the layer's output of the target input, $O_l(a, \theta)$.  As above we take only the first 24 blocks of the output, so more accurately

Minimization of the angle betwen vectors can be done by finding the gradient of the generated input $a_g$ with respect to the cosine of the vector versions of $O(a_g, \theta)$ and $O(a, \theta)$ as follows:

$$
\cos(O_l(a, \theta), O_l(a_g, \theta)) = \frac{O_l(a, \theta) \cdot O_l(a_g, \theta)}{|| O_l(a, \theta) ||_2 || O_l(a_g, \theta) ||_2} \\
a_{n+1} = a_n + \nabla_{a_n} (1 - \cos(O_l(a, \theta), O_l(a_g, \theta))
$$

where we minimize the value $1 - \cos(\phi)$ because we want to minimize the angle between vectorized versions of the model's output (and $\cos(\phi) = 1$ when $\phi = 0$).

Minimizing angle between target and generated input does lead to mixing of information between the first 24 and the other patches, however. Using the same target input as above, minimizing $\cos (\phi)$ results in poor representations, regardless of whether the output is taken as the MLP or as the dot product attention layer.

![autoencoding]({{https://blbadger.github.io}}/deep-learning/poor_mixing_ViT.png)

The superior mixing in the MLP mixer architecture compared to the vision transformer may also be observed by finding the feature maps of individual patches early in the model, maximizing activations of all elements of patch after an across-patch mixing layer (which occurs second for mixers and first for ViTs) or after the embedding dimension layer (first for mixers and second for ViTs).

![mixer versus vit mixing]({{https://blbadger.github.io}}/deep-learning/vit_vs_mixer_dissected.png)

There are two notable observations when we observe the features corresponding to a single input patch at one particular layer: first, there is very little difference between the patch and feature maps of the vision transformer compared to what is found for mixers, and second that the ViT primarily focuses on the input region corresponding the patch identity (note the bright yellow squares) whereas the mixer attends more broadly to the entire input, regardless of whether we observe mixer or embedding MLP layer activation.

On the other hand, when we observe the feature maps for neurons across all patches, there is less difference between then ViT and MLP-Mixer.  

![mixer versus vit mixing]({{https://blbadger.github.io}}/deep-learning/mlp_mixer_more_sublayers.png)

![mixer features]({{https://blbadger.github.io}}/deep-learning/vit_more_sublayers.png)

### Deep Dream

Given some image, it may be wondered how a computer vision model would modify that image in order to increase the activation of some component of that model.  This is similar to the feature visualization procedure used above but starts with a natural image rather than random noise.

For a trained ViT Base 32 we have the following:

![vit dream]({{https://blbadger.github.io}}/deep-learning/vit_b_32_dream.png)


## Computability and periodicity II

### Uncomputable but definable trajectories

Truly aperiodic (and bounded) trajectories are uncomputable, assuming finite memory and finite time (see [here](https://blbadger.github.io/solvable-periodicity.html) for more with this definition) which is to say that no finite computational procedure is able to accurately account for a bounded aperiodic trajectory.  All 'aperiodic' trajectories displayed in figures (as computed by programs) on this page are actually periodic, as there are a finite number of places any point may be given finite computational memory and so eventually one point must be repeated.  

### Computability and aperiodicity: a look at the logistic map for r=4

Recalling the logistic map, explored [here](https://blbadger.github.io/logistic-map.html)

$$
x_{n+1} = rx_n(1-x_n) \tag{1}
$$

Because of this, one would never know that the period 3, 5, 6, etc. windows in (1) only exist for an infinitely small fraction of starting points for the arbitrarily precise 'true' logistic map, or in other words that this true logistic map is quite different than the approximation presented here.

Given any binary number $\theta$, say $0.1101001$, the number's binary shift map is as follows:

$$
\theta_{n+1} = 2\theta_n \bmod 1
$$

The first few iterations of this map on $1.1101001$ are

$$
\theta_0 = 0.1101001 \\
\theta_1 = 0.1010010 \\
\theta_2 = 0.0100100 \\
\theta_3 = 0.1001000 \\
$$

Now for any rational starting number $\theta_0 \in \Bbb Q$, the bit shift map is periodic or eventually periodic because after a finite number of iterations, the remaining digits are composed of repeating sequences.  On the other hand, the bit shift map is aperiodic for $\theta_0 \in \Bbb R - \Bbb Q$.  This map also exhibits sensitivity to initial conditions because a small change in $\theta_0$ becomes exponentially larger over time, specifically $2^n$ as large after $n$ iterations.

It is surprising that we can find a solution to the logistic map for the special case where r=4 using the following mapping

$$
x_n = \sin^2 (\pi \theta_n)
$$

If $x_{n+1} = 4x_n(1-x_n)$ is implied by $x_n = \sin^2(\pi\theta_n)$ given $\theta_{n+1} = 2 \theta \bmod 1$, the latter is a solution to the logistic map.

because this map is identical to $x_{n+1}$ in the logistic map for $\theta_{n+1}$

$$
x_{n+1} = 4 \sin^2(\pi \theta_n) \left(1-\sin^2(\pi \theta_n) \right), \\
x_{n+1} = \sin^2(\pi2\theta_n \bmod 1 ) \implies \\
4\sin^2(\pi\theta_n)\cos^2(\pi\theta_n) = \sin^2(\pi2\theta_n \bmod 1 ) \\
2 \sin(\pi\theta_n)cos(\pi\theta_n) = \sin(\pi2\theta_n \bmod 1) \\
$$

and as $\sin(2\theta) = 2\sin(\theta)\cos(\theta)$, 

$$
2 \sin(\pi\theta_n)\cos(\pi\theta_n)  = 2 \sin(\pi\theta_n)\cos(\pi\theta_n) 
$$

and therefore these expressions are equivalent regardless of a choice of $\theta_0$.

Expressed another way, the solution to the logistic map with r=4 is 

$$
x_n = \sin^2(\pi 2^n \theta) 
$$

### Other solutions to aperiodic systems

The logistic map is not the only place where one can find the seemingly nonsensical conjunction of an aperiodic system being solved in a cosed-form expression.  Perhaps the two most famous of all irrational numbers, whose digits form aperiodic sequences, may be expressed using periodic procedures as follows:

$$
e = \sum_{n = 0}^\infty \frac{1}{n!} \\
\; \\
\frac{\pi^2}{6} = \sum_{n = 1}^\infty \frac{1}{n^2}
$$

The computational procedure in either case may be expressed as a simple for loop that adds together the resulting number upon substituting the loop number for $n$. But this seems somewhat counterintuitive: aperiodic sequences are inherently unpredictable, so how can they be described with a short computational procedure?  An oblique restatement is as follows: aperiodic systems are for most purposes indistinguishable from 'random' ones.  It is known that no truly random output can be made from any classical computational procedure, so how then could a (very small) classical computational procedure yield a random-like output?

If one is not convinced by the argument that aperiodic outputs are not distinguishable from random ones, consider the memory requirements of storing sequences of integers.  It is considered to be accepted that no irrational number $x \in \Bbb R - \Bbb Q$ can be stored in finite memory, and therefore no program is capable of converting an irrational number into a finite sequence of integers. Because all programs themselves may be mapped to a finite sequence of digits in a bijective manner, no aperiodic sequence can be represented by a finite program and there is no way to represent an aperiodic sequence with a periodic one.  But then how can we express irrationals like $e$ or $\pi$ or even aperiodic sequences like the logistic map for r=4 with such small periodic procedures?

The answer is that these procedures are not finite: to calculate the exact value of $e$, for example, one needs to add an infinite number of $\frac{1}{n!}$ terms together.  A similar statement is true for $\frac{\pi^2}{6}$, and upon close inspection this is also the case for the logistic map where $r=4$.  To see why this is the case, first recall that only irrational values of $\theta$ are aperiodic, and that the logistic map for $r=4$ is sensitive to initial conditions because $\theta$ is transformed by $2^n$, leading to any small change growing exponentially before the value is folded into $(0, 1)$ by $sin^2(x)$.  This means that as $n$ increases in size, the number of digits $\theta$ is known to must also increase to yield a reasonably accurate value of $x_n$.  An arbitrarily large value of $n$ requires an arbitrarily good approximation of $\theta$.

These considerations suggest that it is helpful to define three basic types of computations:

1. Numbers (inputs) grow but the procedure is of fixed size.
2. Numbers (inputs) and procedure size both grow, but the procedure itself is periodic.
3. Numbers and procedure size grow, and the procedure cannot be represented as a periodic sequence.

These classifications correspond to the notions of

1. Rational numbers: $1/2$ etc.
2. Classically computable but irrational numbers: $e,\; \pi,\; \sqrt2$ and so on.
3. Classically uncomputable (irrational) numbers: Chaitlin's constant $\Omega_f$ and others

With this classification, any aperiodic sequence falls in the second or third group. Of note, this group includes the sequence of [prime gaps](https://blbadger.github.io/unpredictable-primes.html), which yields the conclusion that that sequence, and therefore the number of integers one has to pass over to arrive at the subsequent prime number, can never be represented by a finite periodic computational procedure.  In computer science parlance, this means that finding a prime number will never be brought down to $O(1)$, but instead the procedure itself scales with input size.  It is clear that this problem is an example of type 2 computability, because the procedure for finding primes is itself periodic (simply iterate over all numbers less than or equal to $\sqrt n$ and see if any divides $n$).

With this classification, consider the logistic map where $r=4$: the case where an arbitrary aperiodic trajectory of the logistic map for $r=4$.  It is interesting to note that the closed solution presented above is only helpful for a small subset of possible starting points $x_0 \in (0, 1)$.  This is because most real numbers are classically uncomputable (a result of the diagonal argument for procedures) and therefore approximating $\theta$ to an arbitrary degree of accuracy by a (finite) computational procedure is impossible.  

Motivated by these observations, it may be best to consider a more strict definition of computability than is normally used.  For the logistic map where $r=4$, as $n \to \infty$ for an accurate computation the precision of theta must be known to an infinite number of decimal places, which is clearly not possible.  Likewise, the exact values of $e$ and $\pi$ cannot be determined without an infinite number of additions performed, also not possible.






























## Primes are unpredictable

Whilst incrementing through the integers, how long one has to wait until reaching a prime number is often quite a difficult thing to predict.  The question of where the next prime number will be is equivalent to the question of what is the gap between the current prime and the next prime number.  Gaps between prime numbers form a non-repeating sequence, as do the gaps between those gaps.  This makes it difficult to predict how far away the next prime will be without simply computing the primality of each successive integer, either directly or by an interval technique (eg. the sieve of Eratosthenes).  

### Theorem: The sequence of gaps between consecutive prime numbers is not initially periodic

Restated, the sequence between gaps betwen consecutive prime numbers, starting from the first gap $g_1$

$$
g_1 = p_2 - p_1 = 3-2 = 1
$$            

cannot be split into repeated subsequences, such that repetitions of this subsequence cover all prime gaps.  In other words, given a sequence of gaps between consecutive primes $g_1, g_2, g_3 , ... $ there is no way to divide this list into subsequences $ (g_1, g_2, ... g_{n-1}), (g_{n}, g_{n+1}, ... g_{2n-1}), (g_{2n}, g_{2n+1}, ... g_{3n-1}), ... $ such that $g_1 = g_{n} = g_{2n} ..., g_2 = g_{n+1} = g_{2n+1} ...$.  

**Proof:** 

Enumerating the prime numbers, we have

$$
p_1, \; p_2, \; p_3, \; p_4, \; p_5, \; p_6, \; p_7, \; p_8 ...\\
2, \; 3, \; 5, \; 7, \; 11, \; 13, \; 17, \; 19 ...
$$

and the gaps between primes where $g_n = p_{n+1} - p_n$ is listed as

$$
g_1, \; g_2, \; g_3, \; g_4, \; g_5, \; g_6, \; g_7 ...\\
1, \; 2, \; 2, \; 4, \; 2, \; 4, \; 2 ...
$$

Now suppose that this sequence of gaps between primes did repeat for multiples of some finite $n > 0$.  Then

$$
(g_1, g_2, g_3, ..., g_{n-1}) = (g_{n}, g_{n+1}, g_{n+2}, ... , g_{2n-1}) \\
g_1 = g_{n}, \; g_2 = g_{n+1}, \; ... g_{n-1} = g_{2n-1}
$$

Adding the sequence of gaps together, we have a sum $\mathscr S_n$ 

$$
g_1 + g_2 + g_3 + \cdots + g_{n-1} = \mathscr S_n
$$

which is an integer and thus is either even or odd.  If $\mathscr S_n$ is even, prime $p_n$ is necessarily even as well because $p_n = 2 + \mathscr S_n$.  But if $\mathscr S_n$ is odd, then $p_{2n} = 2 + 2(\mathscr S_n) = 2(1 + \mathscr S_n)$ and therefore $p_{2n}$ is even. But this is a contradiction as the only even prime number is $p_1 = 2$ but by definition for any $n > 0$, $p_{2n} > 2$.  Therefore the sequence of gaps between consecutive primes starting from the first prime cannot repeat itself no matter how large $n$ is (remaining finite), or in dynamical terms all subsequences of $g_1, g_2, g_3, ... $ are aperiodic.

### Theorem: Prime gap sequences are not eventually periodic

In symbols, there does not exist finite $n, m$ such that the sequence of prime gaps may at prime $p_n$ and on be split into subsequences of length $m$

$$
\lnot \exists n, m : (g_n, g_{n+1}, g_{n+2}, ... , g_{n + m - 1}) \\
= (g_{n+m}, g_{n+m+1}, g_{n+m+2}, ..., g_{n + 2m - 1}) \\
= (g_{n+2m}, g_{n+2m+1}, g_{n+2m+2}, ..., g_{n + 3m - 1}) \\
\; \; \vdots
$$

**Proof:**

Suppose that there was some finite $n>0$ such that the above statement held.  Then there exists a finite $m>0$ such that after $m$ gaps, the sequence $g_n, g_{n+1}, ...$ is repeated (with period $m$):

$$
(g_n, g_{n+1}, ... , g_{n+m-1}) = \\
(g_{n+m}, g_{n+m+1}, ... , g_{n+2m-1}) = \\
(g_{n+2m}, g_{n+2m+1}, ... , g_{n+3m-1}) = \\
\; \; \vdots
$$

What happens when this pattern repeats $p_n$ times?  The prime number indexed at this position is $p_{n+mp_n}$, because there are $n$ primes before $g_n$, and $mp_n$ primes after: in this enumeration $g_n = p_{n+1} - p_n$ means that $g_{n+mp_n-1} = p_{n + mp_n} - p_{n+mp_n-1}$.   Defining $\mathscr S_n = g_n + g_{n+1} + \cdots + g_{n+m-1}$ as above, the value of $p_{n+mp_n}$ is 

$$
p_{n+mp_n} = p_n + p_n \mathscr S_n \\
p_{n+mp_n} = p_n (1 + \mathscr S_n)
$$

and therefore $p_n \rvert p_{n+mp_n}$ or in words $p_n$ divides $p_{n+mp_n}$.  But then $p_n < p_{n+mp_n}$ for all $n, m>0$, so therefore $p_{n+mp_n}$ is composite, a contradiction as it was ealier stated to be prime. As $n$ and $m$ were chosen arbitrarily, there is no $n$ or $m$ such that the sequence of prime gaps is eventually periodic given that there are infinitely many prime numbers.  

One can appreciate that this is a more general theorem than the first presented on this page, which is the special case where $p_n = 2$.  

### Theorem: Prime gap sequences are aperiodic

**Proof:** As prime gap sequences are neither initially nor eventually periodic for any finite period $m$, they are aperiodic.

### Theorem: The sequence of gaps-of-gaps of primes is aperiodic

Restated, the sequence of gaps $g_{g_1}, g_{g_2}, g_{g_3}, ...$ between prime gaps such that $g_{g_1} = g_2 - g_1$ and $g_{g_2} = g_3 - g_2$ etc. is aperiodic

**Proof:** Suppose that the sequence $g_{g_1}, \; g_{g_2}, \; g_{g_3}, \; ...$ were periodic with finite periodicity $m$.  

$$
p_1,\; p_2,\; p_3,\; p_4,\; p_5,\; \\
\; g_1, \; g_2,\; g_3,\; g_4,...\\
\;  \; g_{g_1},\; g_{g_2},\; g_{g_3} ...
$$

Bearing in mind that $g_1=1$ and $g_2 = 2$, $g_{g_1} = 1 = g_{g_m}$.  It is apparent that all of $g_2, g_3, g_4...$ are even, because all primes after $p_1 = 2$ are odd.  But then $g_{m+1} = 1 + g_m$ would be odd, and a contradiction has been reached.  Therefore there is no periodicity in the sequence of gaps between prime gaps.

**Alternate Proof:** 

$$
g_{g_1} + g_{g_2} + g_{g_3} + \cdots + g_{g_{n-1}} = \mathscr S_n
$$

$\mathscr S$ cannot be negative if there are infinitely many positive prime gaps.  But $\mathscr S$ cannot be 0 either, because then the sequence of prime gaps $g_0, g_1, g_2...$ would be periodic.  And finally $\mathscr S$ cannot be positive, because then there would be 0 or finitely many instances of any gap size, contradicting Zhang's theorem that there is some $k < 70000000$ for which there are infinitely many prime gaps of size $k$, or Maynard's finding of a $k<600$ with the same properties.  As $\mathscr S$ cannot be neither greater than nor less than nor equal to zero, a contradiction has been reached and therefore the gap of prime gaps is aperiodic.


### Theorem: The sequence of all gap levels of primes is aperiodic

In other words, 

$$
g_{g_{g_1}}, g_{g_{g_2}}, g_{g_{g_4}}...
$$ 

(gap level 3) or any other level gap sequence is aperiodic.

**Proof** A direct extension of the previous theorem: the first gap at any level is odd, but subsequent gaps (on that level) are even, which can only occur if the first gap is never repeated.


### Implications for decidability

As presented [here](https://blbadger.github.io/solvable-periodicity.html), one can show that all decidable predicates may be expressed as periodic systems, or in other words decidability implies periodicity (periodicity with respect to the computation procedure itself, that is).  Taking the contrapositive of this, aperiodicity (in the computation procedure) implies undecidability.  On this page it was found that the sequence of integers of prime gaps are aperiodic, but could some computation procedure for finding prime gaps be itself periodic?

Examining such a procedure, note that it would have to map a countably infinite number of inputs (any of the prime numbers) to a countably infinite number of ouputs (prime gaps, which are unbounded).  The set of all functions possible here is equivalent with the set of all subsets of the natural numbers, $2^ \Bbb N \sim \Bbb R$.  Suppose the mapping function were periodic, meaning that the procedure for finding a prime gap given the prime number's index is identical to the procedure for finding the gap for a different prime number...

If indeed the question of prime gap size is undecidable, this would shed light on the findings from Presburger and Skolem that addition-only or multiplication-only arithmetics are decidable, whereas arithmetic with both addition and multiplication is not (more background [here](https://blbadger.github.io/solvable-periodicity.html)).  The reason is as follows: prime number gaps represent intersections between addition and multiplication, but if these intersections are undecidable (meaning that we cannot compute using finite arithmetic how all gaps should be placed) then arithmetic containing this prime gaps is necessarily undecidable. 

More precisely, undecidability in prime gaps would mean that the question 'Will adding any value to a current number yield another number that cannot be divided (properly)' necessitates a longer and longer computational procedure the larger the number and value are.  Assuming the Church-Turing thesis, decidability is only possible if a finite computational procedure gives the right answer for an infinite number of inputs, here the natural numbers.  Accepting theorems (1) and (2) above together with the idea that there is an equivalence between aperiodicity and undecidability, any arithmetic containing in it the prime gaps will be undecidable.  Note that neither Skolem nor Presburger arithmetic do so: one accepts primes but no gaps (because there is no addition and thus no incrementation), and the other gaps but no primes (no multiplication and therefore no unit of multiplication). 

### Computability 

One can convert decidable statements into computable ones and vice versa by assigning each number one of $\mathfrak t, f$ and then performing a decision procedure.  With this idea accepted, it should be noted that the definition for computability on this page is more strict than the more frequently used definition for this term.  

In that parlance, numbers such as $\pi$ and $\sqrt 2$ are termed computable because there is a procedure that approximates them to any desired accuracy, whereas a number like the Turing halting constant is uncomputable as there is no procedure for approximating it.  It should be noted that even by this broad definition, most real numbers are uncomputable.

On this page, however, computability is defined as the ability to map a countably infinite number of inputs to outputs with a procedure of finite size.  In this sense, a number such as $\pi$ is not strictly computable because the procedure for finding any given decimal digit $d$ increases in size as the distance between the decimel point and $d$ increases. The procedure for mapping each decimal place to its correct digit, in any base, cannot be finite over countably infinite input domain.  This is identical to what is observed for aperiodic dynamical trajectories, where every additional step in time requires an extra computation such that the procedure grows without bound as time increases. 

By this definition, computable numbers or sequences may be determined in arbitrary order, whereas uncomputable ones must proceed in one direction.  To see an example of this, imagine trying to find the 100th digit after the decimal point of the fraction $1/7$ without determining earlier values.  

$$
\frac{1}{7} = 0.142857 \; 142857 \; 1...
$$

We only need to find the identity of the first five digits to determine that the 100th digit is $5$ if we know that $1/7$ is periodic with period 5 in this base.  But trying to accomlish this task for $\pi$ is not possible.

$$
\pi = 3.1415926535...
$$

Any method of approximation by definition start farther and ends closer to the desired value.  As each addition to our decimal list increases the accuracy of approximation, each digit must be determined from left to right in sequence.  If this were not so, we could find the nth digit of $\pi$ or any irrational number with a constant computational procedure.  But to determine (correct) digits out of order is impossible for any approximation technique, because knowledge of one digit necessarily implies knowledge of all the precede it because they are larger.  Only for exact numbers (which are equivalent to computable numbers in this strict definition) may arbitrary digits be learned with constant computation.

Note that this idea also applies to algorithms that do not strictly approximations, such as the Bailey-Borwein-Plouffe [formula](https://en.wikipedia.org/wiki/Bailey%E2%80%93Borwein%E2%80%93Plouffe_formula) for finding desired digits of pi.  Although digits prior to the one of interest are not explicitly calculated, they are implicitly, such that the higher the accuracy desired, the more elements in an infinite series must be added together.  This is true for all spigot algorithms, which find digits (from larger to smaller) in sequence rather than by approximation.  








## Representation in Vision Transformers and Attentionless Models

### Introduction

The [convolutional neural network](https://blbadger.github.io/neural-networks.html) has been the mainstay of deep learning vision approaches for decades, dating back to the work of [LeCun and colleagues](https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf) in 1989. In the that work, it was proposed that restrictions on model capacity would be necessary to prevent over-parametrized fully connected models from failing to generalize and that these restrictions (translation invariance, local weight sharing etc.) could be encoded into the model itself.

Convolution-based neural networks have since become the predominant deep learning method for image classification and generation due to their parsimonius weight sharing (allowing for larger inputs to be modeled with fewer parameters than traditional fully connected models), their flexibility (as with proper pooling after convolutional layers a single model may be applied to images of many sizes), and above all their efficacy (nearly every state-of-the-art vision model since the early 90s has been based on convolutions).

It is interesting to note therefore that one of the primary motivations of the use of convolutions, that over-parametrized models must be restricted in order to avoid overfitting, has since been found to not apply to deep learning models.  Over-parametrixed fully connected models do not tend to overfit image data even if they are capable of [doing so](https://arxiv.org/abs/1412.6614), and furthermore convolutional models that are currently applied to classify (quite accurately too) large image dataasets are capable of fitting pure noise ([ref](https://dl.acm.org/doi/abs/10.1145/3446776)).

Therefore it is reasonable to hypothesize that the convolutional architecture, although effective and flexible, is by no means required for accurate image classification or other vision tasks. One particularly effective approach has been translated from the field of natural language processing that has been termed the 'transformer', which makes use of self-attention mechanisms. We also consider mlp-based mixer architectures that do not make use of attention.

### Transformer architecture

We focus on the ViT B 32 model introduced by [Dosovitsky and colleagues](https://arxiv.org/abs/2010.11929#).  This model is based on the original transformer from [Vaswani and colleages](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html), in which self-attention modules previously applied with recurrent neural networks were instead applied to patched and positionally-encoded sequences in series with simple fully connected architectures. 

The transformer architecture was found to be effective for natural language processing tasks and was subsequently employed in vision tasks after convolutional layers.  But the work introducing the ViT went further and applied the transformer architecture directly to patches of images, which has been claimed to occur without explicit convolutions (but does in fact used strided convolution to form patch embeddings as we will later see).  It is an open question of how similar these models are to convolutional neural networks.

The transformer is a feedforward neural network model that adapted a concept called 'self-attention' from recurrent neural networks that were developed previously.  Attention modules attempted to overcome the tendancy of recurrent neural networks to be overly influenced by input elements that directly preceed the current element and 'forget' those that came long before.  The original transformer innovated by applying attention to tokens (usually word embeddings) followed by an MLP, foregoing the time-inefficiencies associated with recurrent neural nets.

In the original self-attention module, each input (usually an embedding of a word) is associated with three vectors $k, q, v$ for Key, Query, and Value that are produced from multiplying learned weight matricies $W^K, W^Q, W^V$ to the input $X$.  Similarity between inputs to the first element (denoted by the vector $\pmb{s_1}$) is calculated by finding the dot product (denoted $\cdot$) of one element's query vector with all element's key vectors as follows:

$$
\pmb{s_1} = (q_1 \cdot k_1, q_1 \cdot k_2, q_1 \cdot k_3,...)
$$

before constant scaling followed by a softmax transformation to the vector $\pmb{s_1}$ to make $\pmb{s_1'}$.  Finally each of the resulting scalar components of $s$ are multiplied by the corresponding value vectors for each input $v_1, v_2, v_3,...$ and the resulting vectors are summed up to make the activation vector $\pmb{z_1}$ (that is the same dimension as the input $X$ for single-headed attention).

$$
\pmb{s_1'} = \mathbf{softmax} \; ((q_1 \cdot k_1)/\sqrt d, (q_1 \cdot k_2)/ \sqrt d, (q_1 \cdot k_3)/ \sqrt d,...) \\
\pmb{s_1'} = (s_{1,1}', s_{1,2}', s_{1,3}',...) \\
\pmb{z_1} = v_1 s_{1,1}' + v_2 s_{1,2}' + v_3 s_{1,3}'+ \cdots + v_n s_{1,n}
$$

The theoretical basis behind the attention module is that certain tokens (originally word embeddings) should 'pay attention' to certain other tokens moreso than average, and that this relationship should be learned directly by the model.  For example, given the sentence 'The dog felt animosity towards the cat, so he behaved poorly towards *it*' it is clear that the word 'it' should be closely associated with the word 'cat', and the attention module's goal is to model such associations.  

When we reflect on the separate mathematical operations of attention, it is clear that they do indeed capture something that may be accurately described by the English word.  In the first step of attention, the production of $q, k, v$ vectors from $W^K, W^Q, W^V$ weight matricies can be thought of as projecting the input embedding $X$ into the relevant vectors such that something useful about the input $X$ is captured, being that these weight matricies are trainable parameters.
The dot product between vectors $q_1$ and $k_2$ may be thought of as a measure of the similarity between embeddings 1 and 2 precisely because the dot product itself may be understood as a measure of vector similarity: the larger the value of $q_1 \cdot k_2$, the more similar these entities are assuming similar norms among all vectors $q, k$.  Softmax then normalizes attention such that all values $s$ are between 0 (least attention) and 1 (most attention).  The process of multiplying these attention values $s$ by the value vectors $v$ serves to 'weight' these value vectors based on that attention amount.  If the value vectors accurately capture information in the input $X$, then the attention module yields an output that is a additive combination of $v$ but with the 'most similar' (ie largest $s$) $v$ having the largest weight.

But this clean theoretical justification breaks down when one considers that models with single attention modules generally do not perform well on their own but require many attention modules in parallel (termed multi-head attention) and in series.  Given a multi-head attention, one might consider each separate attention value to be context-specific, but it is unclear why then attention should be used at all given that an MLP alone may be thought of as providing context-specific attention.  Transformer-based models are furthermore typically many layers deep, and it is unclear what the attention value of an attention value of a token actually means.

Nevertheless, to gain familiarity with this model we note that for multi-head attention, multiple self-attention $z_1$ vectors are obtained (and thus multiple key, value, and query weight matricies $W^K, W^Q, W^V$ are learned) for each input. The multi-head attention is usually followed by a layer normalization and fully connected layer (followed by another layer normalization) to make one transformer encoder. Attention modules are serialized by simply stacking multiple encoder modules sequentially.

A single transformer encoder applied to image data may be depicted as follows:

![vision transformer architecture]({{https://blbadger.github.io}}/deep-learning/transformer_encoder_illustration.png)

For a more thorough introduction to the transformer, see Alammar's [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).  See [here](https://blbadger.github.io/neural-networks3.html#generalization-and-language-model-application) for an example of a transformer encoder architecture applied to a character sequence classification task.

### Input Generation with Vision Transformers

One way to understand how a model yields its outputs is to observe the inputs that one can generate using the information present in the model itself.  This may be accomplished by picking an output class of interest (here the models have been trained on ImageNet so we can choose any of the 1000 classes present there), assigning a large constant to the output of that class and then performing gradient descent on the input, minimizing the difference of the model's output given initial random noise and that large constant for the specified output index.  

We add two modifications to the technique denoted in the last paragraph: a 3x3 Gaussian convolution (starting with $\sigma=2.4$ and ending with $sigma=0.$) is performed on the input after each iteration and after around 200 iterations the input is positionally jittered using cropped regions.  For more information see [this page](https://blbadger.github.io/input-generation.html).  

More precisely, the image generation process is as follows: given a trained model $\theta$ we construct an appropriately sized random input $a_0$

$$
a_0 = \mathcal{N}(a; \mu=0.7, \sigma=1/20)
$$

next we find the gradient of the absolute value of the difference between some large constant $C$ and the output at our desired index $O(a_0, \theta)_i$ with respect to that random input,

$$
g = \nabla_{a_0} |C - O(a_0, \theta)_i|
$$

and finally input is updated by gradient descent followed by Gaussian convolution $\mathcal{N_c}$

$$
a_{n+1} = \mathcal{N_c}(a_n - \epsilon * g)
$$

Positional jitter is applied between updates such that the subset of the input $a_n$ that is fed to the model undergoes gradient descent and Gaussian convolution, while the rest of the input is unchanged.  

$$
a_{n+1[:, \;m:n, \;o:p]} = \mathcal{N_c}(a_{n[:, \; m:n, \; o:p]} - \epsilon * \nabla_{a_{n}[:, \; m:n, \;o:p]} |C - O(a_{n[:, \; m:n, \; o:p]}, \theta)_i|)
$$

One of the first differences of note compared to the inputs generated from convolutional models is the lower resolution of the generated images: this is partly due to the inability of vision transformer base 32 (ViT B 32) to pool outputs before the classification step such that all model inputs must be of dimension $3x224x224$, whereas most convolutional models allow for inputs to extend to $3x299x299$ or even beyond $3x500x500$ due to max pooling layers following convolutions.

When we observe representative images of a subset of ImageNet animal classes with Vit B 32,

![vision transformer input generation]({{https://blbadger.github.io}}/neural_networks/vit_animals.png)

as well as landscapes and inanimate objects with the same model,

![vision transformer input generation]({{https://blbadger.github.io}}/neural_networks/vit_landscapes.png)

it is clear that recognizable images may be formed using only the information present in the vision transformer architecture just as was accomplished for convolutional models.  

### Vision Transformer hidden layer representation overview

Another way to understand a model is to observe the extent to which various hidden layers in that model are able to autoencode an input: the better the autoencoding, the more complete the information in that layer.

First, let's examine an image that is representative of one of the 1000 categories in the ImageNet 1k dataset: a dalmatian.  We can obtain various layer outputs by subclassing the pytorch `nn.Module` class and accessing the original vision transformer model as `self.model`.  For ViT models, the desired transformer encoder layer outputs may be accessed as follows:

```python
class NewVit(nn.Module):

    def __init__(self, model):
        super().__init__()
        self.model = model
    
    def forward(self, x: torch.Tensor, layer: int):
        # Reshape and permute the input tensor
        x = self.model._process_input(x)
        n = x.shape[0]

        # Expand the class token to the full batch
        batch_class_token = self.model.class_token.expand(n, -1, -1)
        x = torch.cat([batch_class_token, x], dim=1)
        for i in range(layer):
            x = self.model.encoder.layers[i](x)
        return x
```
where `layer` indicates the layer of the output desired (`self.model.encoder.layers` are 0-indexed so the numbering is accurate). The `NewVit` class can then be instantiated as

```python
vision_transformer = torchvision.models.vit_b_32(weights='IMAGENET1K_V1')
new_vision = NewVit(vision_transformer).to(device)
new_vision.eval()
```

First let's observe the effect of layer depth (specifically transformer encoder layer depth) on the representation accuracy of an untrained ViT_B_32 and compare this to what was observed for ResNet50 (which appears to be a fairly good stand-in for other convolutional models).

![dalmatian vit]({{https://blbadger.github.io}}/neural_networks/vit_vs_resnet_untrained_representations.png)

The first notable aspect of input representation is that it appears to be much more difficult to approximate a natural image using Gaussian-convolved representations for ViT than for ResNet50, or more precisely it is difficult to find a learning rate $\epsilon$ such that the norm of the difference between the target output $O(a, \theta)$ and the output of the generated input $O(a_g, \theta)$ is smaller than the norm of the difference between the target output and the output of a slightly shifted input $O(a', \theta)$ where $a' = a + \mathcal{N}(a; \mu=0, \sigma=1/18)$, meaning that it is difficult to obtain the following inequality:

$$
||O(a, \theta) - O(a', \theta)||_2 < ||O(a, \theta) - O(a_g, \theta)||_2
$$

Compare the decreasing representation clarity with increased depth to the nearly constant degree of clarity in the ViT: even at the twelth and final encoder, the representation quality is approximately the same as that in the first layer.  The reason as to why this is the case is explored in the next section.

As each encoder is the same size, we can also observe the representation of only that encoder (rather than the given encoder and all those preceeding it).

![dalmatian vit]({{https://blbadger.github.io}}/neural_networks/vit_dalmatian_representations.png)

![tesla coil vit]({{https://blbadger.github.io}}/neural_networks/vit_representations.png)

### ViT input processing convolutions are nonoptimal

When we consider the representation of the first layer of the vision transformer compared to the first layer in ResNet50, it is apparent that the former has a less accurate representation.  Before this first layer, ViT has an input processing step in which the input is encoded as a sequence of tokens, which occurs by forming 768 convolutional filters each 3x32x32 (hence the name ViT B **32**) large, with 32-size strides. In effect, this means that the model takes 32x32 patches (3 colors each) of the original input and encodes 768 different 3x7x7 arrays which act analagously to the word embeddings used in the original transformer.

It may be wondered then if it is the input processing step via strided 32x32 convolutions or the first encoder layer that is responsible for the decrease in representation accuracy.  Generating representation of the outputs of first the input processing convolution and then the input processing followed by the first encoder layer of an initial image of a tesla coil, it is clear that the input processing itself is responsible for the decreased representation clarity, and furthermore that training greatly enhances the processing convolutional layer's representation resolution (although still not to the degree seen in the first convolution of ResNet)

![tesla vision transformer representations]({{https://blbadger.github.io}}/neural_networks/vit_entry_representations.png)

For ResNet50, an increase in representation resolution for the first convolutional layer upon training is observed to coincide with the appearance of Gabor function wavelets in the weights of that layer (see the last supplementary figure of [this paper](https://arxiv.org/abs/2211.06496)).  It may be wondered if the same effect of training is observed for these strided convolutions, and so we plot the normalized (minimum set to 0, maximum set to 1, and all other weights assigned accordingly) weights before and after training to find out.

![tesla vision transformer weights]({{https://blbadger.github.io}}/neural_networks/vit_b_32_conv_representations.png)

In some convolutions we do indeed see wavelets (of various frequencies too) but in other we see something curious: no discernable pattern at all is visible in the weights of around half of the input convolutional filters.  As seen in the paper referenced in the last paragraph, this is not at all what is seen for ResNet50's first convolutional layer, where every convolutional filter plotted has a markedly non-random weight distribution (most are wavelets).

Earlier it was observed that for Vit B 32 the process of training led to the appearance of wavelet patterns in the input convolution layer and a concomitant increase in representational accuracy.  For that model the convolutional operation is not overcomplete, but for the ViT Large 16 model it is.  It can therefore be hypothesized that training is not necessary for accurate input representation for the procesing convolution of ViT L 16, and indeed this is found to be the case.

![tesla coil vit representations]({{https://blbadger.github.io}}/neural_networks/vitl16_input_conv.png)

Note the lack of consistent wavelet weight patterns in the input convolution, even after training (and even after extensive pretraining on weakly [supervised](https://arxiv.org/abs/2201.08371)). This observation may explain why [Xaio and colleagues](https://arxiv.org/pdf/2106.14881.pdf) found that replacing the strided input processing convolutions above with 4 layers of 3x3 convolutions (followed by one 1x1 layer) improves vision transformer training stability and convergence as well as ImageNet test accuracy. 

### Poor Input representation from untrained layer normalization

Being that the input convolutional stem to the smaller ViT models is not capable of accurately representing an input, to understand later layers' representation capability we can substitute a trained input processing convolutional layer from a trained vision transformer, and chain this layer to the rest of a model from an untrained ViT.

```python
class NewVit(nn.Module):

    def __init__(self, model):
        super().__init__()
        self.model = model
    
    def forward(self, x: torch.Tensor):
        # Apply a trained input convolution
        x = trained_vit._process_input(x)
        n = x.shape[0]

        # Expand the class token to the full batch
        batch_class_token = self.model.class_token.expand(n, -1, -1)
        x = torch.cat([batch_class_token, x], dim=1)
        for i in range(1):
            x = self.model.encoder.layers[i](x)
            
vision_transformer = torchvision.models.vit_h_14().to(device) # untrained model
trained_vit = torchvision.models.vit_h_14(weights='DEFAULT').to(device) # weights='IMAGENET1K_V1'
trained_vit.eval()
new_vision = NewVit(vision_transformer).to(device)
new
```

When various layer representations are generated for ViT Base 32, it is clear that although there is a decrease in representation accuracy as depth increases in the encoder stack with fixed ($n=1,500$) iterations, this is mostly due to approximate rather than true non-invertibility as increasing the number of iterations of the generation process to $n=105,000$ yields a representation from the last encoder layer that is more accurate than that obtained with fewer iterations from the first.

![tesla coil vit representations]({{https://blbadger.github.io}}/neural_networks/vit_trainedinput_untrained.png)

It is apparent that all encoder layers have imperfections in certain patches, making them less accurate than the input convolution layer's representation.  It may be wondered why this is, being that the encoder layers have outputs of dimension $50x768$ which is slightly larger than the input convolutional output of $49x768$ due to the inclusion of a 'class token' (which is a broadcasted token that is used for the classification output). 

Vision transformer models apply positional encodings to the tokens after the input convolution in the first transformer encoder block, and notably this positional encoding is itself trained: initialized as a normal distribution `nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))`, this positional encoding tensor (`torch.nn.Parameter` objects are `torch.Tensor` subclasses) is back-propegated through such that the tensor elements are modified during training. It may be wondered if a change in positional encoding parameters is responsible for the change in first encoder layer representational accuracy.  This can be easily tested: re-assigning an untrained vision transformer's positional embedding parameters to that of a trained model's positional embedding parameters may be accomplished as follows:

```python
vision_transformer = torchvision.models.vit_b_32(weights='DEFAULT').to(device) # 'IMAGENET1K_V1'
vision_transformer.eval()
untrained_vision = torchvision.models.vit_b_32().to(device)
untrained_vision.encoder.pos_embedding = vision_transformer.encoder.pos_embedding
```

When this is done, however, there is no noticeable difference in the representation quality.  Repeating the above procedure for other components of the first transformer encoder, we find that substituting the first layer norm (the one that is applied before the self-attention module) of the first encoder for a trained version is capable of increasing the representational quality substantially.

![tesla coil vit representations]({{https://blbadger.github.io}}/neural_networks/vitb32_encoder1_ln.png)

### Layer Normalization considered

In the last section it was observed that changing the parameters (specifically swapping untrained parameters for trained ones) of a layer normalization operation led to an increase in representational accuracy.  Later on this page we will see that layer normalization tends to decrease representational accuracy, and so we will stop to consider what exactly this transformation entails.

Given a layer's features indexed by $n$ with the layer's activations denoted $x_n$, the output of layer normalization $y$ is defined as

$$
y = \frac{x - \mathrm{E}(x_n)}{\sqrt{\mathrm{Var}(x_n) + \epsilon}} * \gamma + \beta
$$

where $\gamma$ and $\beta$ are trainable parameters.  Here the expectation $\mathrm{E}(x_n)$ refers to the mean of certain dimensions of $x$, termed features, and variance $\mathrm{Var}(x)$ is the sum-of-squares variance on those same dimensions.  For transformer models, features are typically the activations of each neuron in the MLP of each block, meaning that layer normalization as it is applied to vision transformers most accurately normalizes values in each image patch separately.

Short consideration of the above formula should be enough to convince one that layer normalization is in general non-invertible such that many different possible inputs $x$ may yield one identical output $y$ for any $\gamma, \beta$ values.  For example, observe that $x = [0, 2, 4]^T, t = [-1, 0, 1]^T, u = [2.1, 2.2, 2.3]^T$ are all mapped to the same $y$ despite having very different $\mathrm{Var}(x)$ and $\mathrm{E}(x)$ and elementwise values in $x$.  

Thus it should come as no surprise that the vision transformer's representations of the input often form noticeably patchwork-like images when layer normalization is applied (to each patch separately).  The values $\gamma, \beta$ are all initialized to $1$ and $0$, respectively, but the values $x$ per input patch may be widely different such that some patches may have only one feasible $x$ per given $y$, whereas other may have many $x_1,x_2,...,x_n$ that equivalently give some $y$.  The latter would be expected to have worse input representation due to non-uniqueness.

### Decreased Representational Accuracy with Increased Vision Transformer Depth

We will now switch to larger vision transformers, mostly because these are the ones that performed well on ImageNet and other similar benchmarks.  We can use a different image of a Tesla coil and apply this input to a ViT Large 16 model.  This model accepts inputs of size 512x512 rather than the 224x224 used above and makes patches of that input of size 16x16 such that there are $32^2 + 1 = 1024 + 1$ features per input, and the model stipulates an embedding dimension of 1024.  All together, this means that all layers from the input procesing convolution on contain $1025* 1024=1049600$ elements, which is larger than the $512x512x3 = 786432$ elements in the input.

Transformer encoders contain a number of operations: layer normalization, self-attention, feedforward fully connected neural networks, and residual addition connections.  With the observation that removing layer normalization yields more accurate input representations from encoders before training in small vision transformers, it may be wondered what exactly in the transformer encoder module is necessary for representing an input, or equivalently what exactly in this module is capable of storing useful information about the input.

Recall the architecture of the vision transformer encoder module:

![vision transformer architecture]({{https://blbadger.github.io}}/deep-learning/transformer_encoder_illustration.png)

We are now going to focus on ViT Large 16, where the 'trained' modules are pretrained on weakly supervised datasets before being trained on ImageNet 1K, and images are all 3x512x512.  

The first thing to note is that this model behaves similarly to ViT Base 32 with respect to the input convolution: applying a trained input convolution without switching to a trained first layernorm leads to patches of high-frequency signal in the input generation, which can be ameliorated by swapping to a trained layernorm.

![tesla coil vit representations]({{https://blbadger.github.io}}/neural_networks/vitl16_layernorm_trained.png)

The approach we will follow is an ablation survey: each component will be removed one after the other in order to observe which ones are required for input representation from the module output.  Change are made sub-classing the `EncoderBlock` module of ViT and then simply removing the relevant portions.

This class is originally as follows:

```python
class EncoderBlock(nn.Module):
    """Transformer encoder block."""
    ...
    def forward(self, input: torch.Tensor):
        torch._assert(input.dim() == 3, f"Expected (batch_size, seq_length, hidden_dim) got {input.shape}")
        x = self.ln_1(input)
        x, _ = self.self_attention(x, x, x, need_weights=False)
        x = self.dropout(x)
        x = x + input

        y = self.ln_2(x)
        y = self.mlp(y)
        return x + y
```

to remove residual connections, we remove the tensor addition steps as follows:

```python
class EncoderBlock(nn.Module):
    """Transformer encoder block."""
    ...
    def forward(self, input: torch.Tensor):
        torch._assert(input.dim() == 3, f"Expected (batch_size, seq_length, hidden_dim) got {input.shape}")
        x = self.ln_1(input)
        x, _ = self.self_attention(query=x, key=x, value=x, need_weights=False)
        x = self.dropout(x)

        y = self.ln_2(x)
        y = self.mlp(input)
        return y
```

Then we can replace the `EncoderBlock` modules in the vision transformer with our new class containing no residual connections as follows:

```python
# 24 encoder modules per vision transformer
for i in range(24): 
    vision_transformer.encoder.layers[i] = EncoderBlock(16, 1024, 4096, 0., 0.)
```

Similar changes can be made to remove layer normalizations, MLPs, or self-attention modules. One problem remains, and that is how to load trained model weights into our modified transformer encoders.  As we have re-built the encoders to match the architecture of the original, however, and as the residual connections contain no trainable parameters we can simply load the original trained model and replace each layer with the trained version of that layer.  For example, modifying the ViT L 16 to discard residual connections before adding weighted MLP layers we have

```python
for i in range(24): 
    vision_transformer.encoder.layers[i] = EncoderBlock(16, 1024, 4096, 0., 0.)
    
original_vision_transformer = torchvision.models.vit_l_16(weights='IMAGENET1K_SWAG_E2E_V1').to(device)
original_vision_transformer.eval()

for i in range(24):
    vision_transformer.encoder.layers[i].mlp = original_vision_transformer.encoder.layers[i].mlp
```

For some choice encoder representations from an untrained ViT L 16 (with a trained input convolutional stem to allow for 512x512 inputs), we have the following input representations $a_g$ given a constant $n=1000$ iterations:

![tesla coil vit representations]({{https://blbadger.github.io}}/neural_networks/transformer_dissection.png)

For the last layer, the same number of iterations yields

![tesla coil vit representations]({{https://blbadger.github.io}}/neural_networks/transformer_dissection_24.png)

It is clear that removal of either self-attention or (both) layer normalizations in each encoder module, but not the MLPs from each encoder module is sufficient to prevent the vast majority of the decrease in representation quality with increased depth up to encodrer layer 12, whereas at the deepest layer (24) of this vision transformer we can see that removal of attention modules from each encoder (but not MLPs or LayerNorms alone) prevent most of the decline in input representation accuracy.

This is somewhat surprising given that the MLP used in the transformer encoder architecture is itself typically non-invertible: standard practice implemented in vision transformers is to have the MLP implemented as a one-hidden-layer (with input and output dimensions equal to the dimension of the self-attention hidden layer $d_{model}$) with the hidden layer three or four times as large as $d_{model}$.  This MLP is identically applied to all self-attention outputs such that each embedding of the input patch after self-attention receives the same MLP.  But being that the transformation from hidden to output layer of the MLP is non-invertible as there are fewer output elements than elements in theinput, there is in general not a single unique input for this layer.

Likewise, self-attention and layernorm transformations are both non-invertible and yet removal of only one or the other appears sufficient for substantially improving input representation accuracy in early layers.  From the rate of minimization of the embedding distance

$$
||O(a_n, \theta) - O(a, \theta)||
$$ 

it is clear that self-attention layers are extremely poorly conditioned: including these makes satisfying

$$
||O(a_g, \theta) - O(a, \theta)|| < ||O(a', \theta) - O(a, \theta)||
$$

(where $a' = a + \mathcal{N}(a; \mu=0, \sigma=1/20$ is a slightly shifted input that is visually very similar to $a$ and $a_g$ is the final generated input representation after $n$ steps) extremely difficult for a reasonable amount of steps $n$ regardless of the update size $\epsilon$. Why this is the case will be considered below.

Removal of layer normalization transformations does not yield as much of an increase in input representation accuracy for the last layer (24) of the ViT large 16.  This is because without normalization, at that depth the gradient begins to explode for certain patches: observe the high-frequency signal originating from two patches near the center of the layernormless example above. A very small gradient update rate $\epsilon$ must be used in the gradient descent procedure $a_{n+1} = a_n + \epsilon * \nabla_{a_n}O(a_n, \theta)$ to avoid sending those patch values to infinity.  In turn, the finding that a vision transformer (with attention modules intact) results in exploding gradients $\nabla_{a_n}O(a_n, \theta)$ suggests that this model is poorly conditioned.

### Attention does not transmit most input information

Now we will examine the effects of residual connections on input representation in the context of vision transformers.  After removing all residual connections from each transformer encoder layer, we have

![tesla coil vit representations]({{https://blbadger.github.io}}/neural_networks/vitl16_no_residuals_dissection.png)

It is apparent from these results that self-attention transformations are quite incapable of transmitting most input information, which is why removing all attention modules from encoders 1 through 4 results in a recovery of the ability of the output of encoder 4 to accurately represent the input.  

It may be observed that even a single self-attention layer requires an enormous number of gradient descent iterations to achieve a marginally accurate representation for a trained model, and that even this is insufficient for an untrained one.  This is evidence for approximate non-invertibility, which may equivalently be viewed as poor conditioning in the forward transformation.

![vision transformer representations]({{https://blbadger.github.io}}/neural_networks/vitl16_no_residuals_or_mlp.png)

It is interesting to observe that training leads to a somewhat more informative (with respect to the input) self-attention module considering that attention layers from trained models carry very little input information with respect to fully connected layers.  There may be some benefit for the transformer encoder to transmit more information during the learning procedure, but it is unclear why this is the case.

What about true non-invertibility?  If our gradient descent procedure on the input is effective, then an unbounded increase in the number of iterations of gradient descent would be expected to result in an asymptotically zero distance between the target layer output $O(a, \theta)$ and the layer output of the generated input in question $O(a_g, \theta)$, or more precisely 

$$
a_{n+1} = a_n - g * \epsilon, \; n \to \infty \implies ||O(a, \theta) - O(a_g, \theta) ||_2 \to 0
$$

If this is the case, we can easily find evidence that points to non-invertibility as a cause for the poor input representation for attention layers.  For an invertible transformation $O$ such that each input $a$ yields a unique output, or for a non-invertible $O$ such that only one input $a_g$ such that $O(a_g, \theta) = O(a, \theta), a_g \neq a$ subject to the restriction that $a_g$ is sufficiently near $a$, or

$$
||O(a, \theta) - O(a_g, \theta)||_2 \to 0 \implies ||a - a_g||_2 \to 0
$$

On the other hand, if we find that the embedding distance heads towards the origin

$$
||O(a, \theta) - O(a_g, \theta)||_2 \to 0
$$

but at the input distance does not head towards the origin

$$
||a - a_g||_2 \not \to 0
$$

then our representation procedure cannot distinguish between the multiple inputs that may yield one output.  And indeed, this is found to be the case for the representation of the tesla coil above for one encoder module with layer normalization followed by multi-head attention alone.

![vision transformer representations]({{https://blbadger.github.io}}/neural_networks/noninvertible_encoder_1.png)

Thus it is apparent that self-attention layers are generally incapable of accurately representing an input due to non-invertibility as well as poor conditioning if residual connections are removed. 

It is not particiularly surprising that self-attention should be non-invertible first and foremost because the transformations present in the self-attention layer (more specifically the multi-head attention layer) are together non-invertible in the general case.  Recall that the first step of attention (after forming $q, k, v$ vectors) is to compute the dot product of $q, k$ vectors.  The dot product, like any inner product operation, is in general non-invertible.  For the case of multi-head attention where the weight matricies $W^K, W^Q, W^V$ multiplied to the input $X$ to form $q, k, v$ are non-square, this projection operation is also non-invertible.  In the standard implementation of self-attention, the embedding dimension is split among the heads such that each $q, k, v$ has dimension $d=e_X/n$ where $e_X$ is the input embedding dimension and $n$ is the number of attention heads. None of these weight matricies are typically square for this reason.

With residual connections included and assuming constraints on the self-attention transformation's Lipschitz constants (and assuming the presence of residual connections) as observed by [Zha and colleages](https://arxiv.org/pdf/2106.09003.pdf).  That said, it is apparent from the experiments above that the vision transformer's attention modules are indeed invertible to at least some degree without modification if residuals are allowed (especially if layernorms are removed).

It may be wondered how much attention layers contribute to a trained model's input representation ability to transform the input to match the manifolds learned during training. Removing self-attention layers or MLPs from each transformer encoder, we see that there is some small highlighting of certain areas of the Tesla coil that exist in the deeper representations without attention layers but not without MLPs.

![tesla coil vit representations]({{https://blbadger.github.io}}/neural_networks/vitl16_trained_dissection.png)

The same observations are made for an input image of a Dalmatian, which is one of the 1000 classes that the vision transformer is trained upon.

![dalmatian vit representations]({{https://blbadger.github.io}}/deep-learning/vitl16_trained_dalmatian_dissection.png)

It may be appreciated that training results in a substantial increase in the ability of attention modules (in the context of ViT Large 16 without MLP layers) to represent an input, but at the same time it is apparent that each attention layer severely limits the information that passes through to the output.  

As vision transformers are effective regardless of this severely limited information pass, it may be wondered whether this matters to the general goal of image classification.  For the training process, there certainly is a significant effect of such information restriction: if one removes the residual connections from ViTs, the resulting model is very difficult to train and fails to learn even modest datasets such as CIFAR-10.

On the other hand, if remove residual connections from MLP-mixers (which are more or less identical to transformers except that the self-attention layer has been swapped for a transposed feed-forward one) the resulting model is not difficult to optimize, and indeed has only a limited decrease in accuracy.


### Attention transformations present challenges for Gradient Descent Optimization

The process of layer representation visualization relies on gradient descent to modify an initially random input such that the layer in question cannot 'distinguish' between the modified input $a_g$ and some target input $a$.  We have seen already that self-attention layers and layer normalization transformations result in non-uniqueness in the forward pass such that many values of $a_g$ yield one identical $O(a_g, \theta)$.

This is not necessarily a problem for the problem of classification: far from it, we typically want many inputs $a_1, a_2, ..., a_n$ to map to one class output $O(a_n, \theta) = y$ for a successful classification.  If the layer in question has separated the classes such that the layer's output $O_l(a_n, \theta)$ provides a simple mapping for subsequent layers, classification is likely to be successful.

On the other hand, poor convergence of the gradient descent procedure is likely to indicate difficulty training.  Suppose that it takes a very large number of iterations $n$ for the input representation gradient descent

$$
a_{n+1} = a_n - \epsilon * \nabla_{a_n}J(O_l(a_n, \theta))
$$

(where $J(O(a_n, \theta))$ is typically a norm of the difference between the target output and the current output, $\vert \vert O_l(a_n, \theta) - O_l(a, \theta) \vert \vert$) such that for some sufficiently small $\delta$ we have

$$
|| O_l(a, \theta) - O_l(a_n, \theta) || < \delta
$$

Now consider the learning procedure of stochastic gradient descent in which the model's parameters $\theta$ are modified to minimize some objective function on the output $J(O(a, \theta))$.

$$
\theta_{n+1} = \theta_n - \epsilon * \nabla_{\theta}J(O(a, \theta))
$$

It can be recognized that these are closely related optimization problems.  In particular, observe how the ability to minimize $\vert \vert O_l(a_n, \theta) - O_l(a, \theta) \vert \vert$ via changes in $a_n$ as $n$ increases is related to the problem of minimizing $J(O(a, \theta)$ via changes in the parameters of the first layer of our model $\theta_1$.  To make things simpler, we can assume that the first layer is not fully connected by is composed of $m$ linear functions acting on $m$ input variables. 

### Attentionless Patch Model Representations

After the successes of vision transformers, [Tolstikhin and colleagues](https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html) and independently [Melas-Kyriazi](https://arxiv.org/abs/2105.02723) investigated whether or not self-attention is necessary for the efficacy of vision transformers. Somewhat surprisingly, the answer from both groups is no: replacing the attention layer with a fully connected layer leads to a minimal decline in model performance, but requires significantly less compute than the tranditional transformer model.  When compute is constant, Tolstikhin and colleagues find that there is little difference or even a slight advantage to the attentionless models, and Melas-Kyriazi finds that conversely using only attention results in very poor performance.

The models investigated have the same encoder stacks present in the Vision transformers, but each encoder stack contains two fully connected layers.  The first fully connected layer is applied to the features of each patch, and for example if the hidden dimension of each patch were 512 then that is the dimension of each parallel layer's input.  The second layer is applied over the patch tokens (such that the dimension of each MLP in that layer's input is the number of tokens the model has).These models were referred to as 'MLP-Mixers' by the Tolstikhin group, which included this helpful graphical sumary of the architecture: 

![mlp mixer architecture]({{https://blbadger.github.io}}/neural_networks/mlp_mixer_architecture.jpeg)

There is a notable difference between the mixer architecture and the Vision Transformer: each encoder block in the ViT places the attention layer first and follows this by the MLP layer, whereas each block in the attentionless mixer architecture has the feature MLP first and mixer second.

We investigate the ability of various layers of an untrained MLP mixer to represent an input image.  We employ an architecture with a patch size of 16x16 to a 224x224x3 input image (such that there are $14*14=196$ patch tokens in all) with a hidden dimension per patch of 1024.  Each layer therefore has a little over 200k elements, which should be capable of autoencoding an input of ~150k elements.

Somewhat surprisingly, this is not found to be the case: the first encoder layer for the above model is not particularly accurate at autoencoding its input, and the autencoding's accuracy declines the deeper the layer in question.  Specifying a smaller patch size of 8  does reduce the representation error.

It may be wondered why the representation of each encoder layer for the 16-sized patch model is poor, being that each transformer encoder in the model is overcomplete with respect to the input.  

This poor representation is must therefore be (mostly) due to approximate non-invertibility (due to poor conditioning), and this is bourne out in practice as the distance of the model output with generated input $O(a_g, \theta)$ to the output of the target input $O(a, \theta)$ which we are attempting to minimize, ie 

$$
m = || O(a, \theta) - O(a_g, \theta) ||_2
$$

is empirically difficult to reduce beyond a certain amount. By tinkering with the mlp encoder modules, we find that this is mostly due to the presence of layer normalization: removing this transformation (from every MLP) removes the empirical difficulty of minimizing $m$ via gradient descent on the input, and visually provides a large increase in representation clarity.  For the Tolstikhin implementation, the effect of removing layer normalization is somewhat more dramatic

![mlp mixer representations]({{https://blbadger.github.io}}/neural_networks/mlp_mixer_representations.png)

than for the Melas-Kryiazi implementation, which is shown in the following figure.

![mlp mixer representations]({{https://blbadger.github.io}}/deep-learning/transformer_mlp_untrained_representation.png)

After training on ImageNet, we find that the input representations are broadly similar to those found for ViT base models, with perhaps some modest increase in clarity (ie accuracy compared to the original) in the deeper layers.

![mlp mixer representations]({{https://blbadger.github.io}}/deep-learning/mixer_input_representation.png)

For further investigation into the features of MLP-mixers and vision transformers, see [this page](https://blbadger.github.io/transformer-features.html).
 

### Counting and numbers

We can construct the natural numbers as sets of the empty set as follows:

$$
\varnothing \to 0 \\
\{\varnothing \} \to 1 \\
\{ \varnothing, \{\varnothing \} \} \to 2 \\
\{ \varnothing, \{\varnothing \}, \{ \varnothing, \{\varnothing \} \}  \} \to 3 \\
...
$$

With this definition, $\varnothing \in A$ for any given set $A$.  In fact, if any element of $A$ is examined, it is composed of elements that are composed of other elements that eventually are composed of the empty set.

Because of this, it is accurate to say that nothing composes everything. 

### Rational numbers as sets of equivalent fractions

Each rational number may be thought of as being the set of all equivalent fractions

$$
\frac{1}{2} = \{ \frac{-1}{-2}, \frac{5}{10}, \frac{15}{30}, \frac{2}{4}, \ldots \}
$$

with the rational number being the expression in lowest terms (positive if possible).  

There are therefore a countably infinite number of names for each rational number, as we can map

$$
\frac{1}{2} = \{ \frac{1}{2}, \frac{2}{4}, \frac{3}{6}, \frac{4}{8}, \ldots \}
$$

to $\Bbb N$ by taking the numerator as our value in $\Bbb N$.  

Irrational numbers, however, cannot be represented as fractions and therefore only have one member in the set of elements:

$$
e = \{ e \}
$$

because expressions like 

$$
\frac{2e}{2}, \frac{3e}{3} ...
$$

are not accurate representations of $e$. This is because these numbers by definition are rational (expressed by fractions


